---
title: "LARGE LANGUAGE MODELS (LLM) APLICADOS AL ENTORNO MÉDICO"
date: "2025-02-12"
categories: [Presentación]
image: img/portada_llm.png
author: "[Antonio Otal Palacín](antonio.otal@udl.cat)"
institute: Hospital Universitari Arnau de Vilanova (Lleida)
lang: es
format: 
    revealjs:
        logo: 'https://cdn0.iconfinder.com/data/icons/modern-ui-glyph-1/64/modern-ui-glyph-1-03-512.png'
        footer-logo-link: "/"
        footer: "[IAMED](https://cv.udl.cat/portal/site/100794-2324/tool/3d5c2c81-a50b-43ce-a0bf-ecbfd152350b)"
        css: styles.css
        number-sections: false
        slide-number: false
        center: false
        preview-links: true
        smaller: true
filters:
  - reveal-header        
jupyter: python3   
---

## Índice

-   Introducción breve a los LLM
-   Embedding Systems
-   Knowledge Graphs
-   Bases de datos
-   Fine-Tuning y Retrieval Augmentation Generation (RAG)
-   Límites de los LLM

::: notes
Notes
:::

# Introducción breve a los LLM

::: notes
Notes
:::

## LLM

![](img/llm.gif){fig-align="center"}

::: notes
Notes
:::

## LLM Pretraining

A una velocidad de 1000 millones de operaciones por segundo entrenar a GPT3 nos costaría 100 millones de años.

Se hubiese tenido que empezar a entrenar el modelo en el cretácico.

::: notes
Notes
:::

## LLM Reinforcement Learning with Human Feedback

![](img/pretraining.gif){fig-align="center"}

::: notes
Notes
:::

## LLM GPU

![](img/gpu.gif){fig-align="center"}

::: notes
Notes
:::

## TRANSFORMERS

::::: columns
::: {.column width="20%"}
![](img/transformer.jpg){.absolute top="200left=0" width="200" height="200"} ![](img/qrcode_transformers.png){.absolute top="350" left="0" width="200" height="200"} ![](img/deeplearningai.png){.absolute top="550" left="0" width="200"}
:::

::: {.column width="80%"}

**Codificaciones posicionales**: Los transformers agregan un número a cada palabra para indicar su posición en la oración. Esto ayuda al modelo a comprender el orden de las palabras, lo cual es crucial para el significado.

**Atención**: Permite al modelo considerar todas las palabras de la oración al traducir o analizar una palabra específica. Esto ayuda a capturar relaciones complejas entre palabras y mejora la precisión de la traducción y la comprensión.

**Autoatención**: Permite al modelo comprender una palabra en el contexto de las palabras que la rodean, lo que ayuda a desambiguar palabras con múltiples significados y captar matices del lenguaje.

:::
:::::

::: notes
Notes
:::

# Vector Embedding

## EMBEDDING

<blockquote cite>

*Vector embeddings* son representaciones numéricas de información, como texto, documentos, imágenes o audio. Capturan el significado semántico de la información

</blockquote>

::: notes
Notes
:::

## VECTORES

![](img/vectores.gif)

<a href="https://colab.research.google.com/github/aotal/ImagenMedica/blob/master/posts/code/Embedding.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>

::: notes
Notes
:::

## EMBEDDING MACHINE

![](img/embeddings.png)

::: notes
Notes
:::

## CHUNKS

<blockquote cite>

Un *chunk* es una unidad discreta de información extraída de un cuerpo de texto más grande. Puede ser una frase, un párrafo, una sección de un documento o incluso un documento completo, dependiendo del sistema y la aplicación. La idea principal es dividir la información en partes más pequeñas que sean más fáciles de procesar y analizar para la IA.

</blockquote>


::: notes
Notes
:::

## Métodos de División en Chunks

- **Por encabezados**: Se usan los encabezados y subtítulos del documento para crear chunks.
- **Por párrafos**: Se divide el texto en chunks según los párrafos.
- **Ventanas deslizantes**: Una ventana captura texto al desplazarse, creando chunks que se superponen.
- **Segmentación semántica**: Se usan algoritmos para identificar unidades de significado y dividir el texto en chunks.

::: notes
Notes
:::

## ¿Cómo los LLMs Utilizan los Chunks?

Los LLMs utilizan los chunks para comprender el contexto y generar texto coherente. Al procesar un chunk, el LLM puede:

- **Identificar las ideas principales**: Extraer la información más relevante del chunk.

- **Establecer conexiones**: Relacionar la información del chunk con otros chunks o con el conocimiento previo del LLM. Para lograr esto, los LLMs utilizan mecanismos como la atención y la codificación posicional. La atención permite al modelo enfocarse en las partes más relevantes de la información dentro de un chunk y entre diferentes chunks. La codificación posicional proporciona información sobre la ubicación de las palabras dentro de un chunk y en relación con otros chunks, lo que ayuda al modelo a comprender el orden y la secuencia de la información.

- **Generar resúmenes**: Condensar la información del chunk en un resumen conciso.

- **Responder preguntas**: Proporcionar respuestas precisas y relevantes a las preguntas basadas en la información del chunk.

::: notes
Notes
:::

## Desafíos de los *Chunks* en LLMs

- Pérdida de contexto: Dividir el texto en *chunks* puede dificultar la comprensión de relaciones entre datos, afectando la precisión de las respuestas.

- Sesgo: La selección de *chunks* puede introducir sesgos, llevando a resultados incompletos o inexactos.

- Información compleja: Cierta información (imágenes, datos, etc.) es difícil de representar en *chunks*.

- Tamaño del *chunk*: Encontrar el tamaño óptimo es crucial para el rendimiento del modelo.


::: notes
Notes
:::

# KNOWLEDGE GRAPH (KG)

## ¿QUÉ SON LOS KG?

<blockquote cite>
Los grafos de conocimiento son bases de datos inteligentes que representan el conocimiento médico de manera estructurada, permitiendo a la IA comprender y razonar sobre la información de forma más efectiva.
</blockquote>

[![Lord of the rings](img/thering.jpg){width=300px}](https://alon-cohen-gordon.wixsite.com/lotr-graph){preview-links="true" .h}

::: notes
Notes
:::

## Componentes principales de un grafo de conocimiento

::::: columns
::: {.column width="30%" }
 ![](img/esquema_kg.jpg)
:::

::: {.column width="70%" .absolute top="220" left="340"}


- **Nodos**: Representan entidades, conceptos u objetos, como enfermedades, síntomas, genes o medicamentos.
- **Aristas**: Representan las relaciones entre los nodos, como "causa", "trata", "se asocia con" o "es un tipo de".
- **Etiquetas**: Proporcionan información adicional sobre los nodos y las aristas, como nombres, descripciones o atributos.

:::
:::::

::: notes
Notes
:::

## Beneficios de los Grafos de Conocimiento
::::: columns
::: {.column width="30%" }
 ![](img/beneficios_KG.jpg)
:::

::: {.column width="70%" .absolute top="230" left="340"}


- **Precisión en el diagnóstico**: Integración de información de diversas fuentes.
- **Personalización de tratamientos**: Uso de información genómica, clínica y de estilo de vida.
- **Descubrimiento de fármacos**: Identificación de nuevas relaciones entre genes, proteínas y enfermedades.

:::
:::::

::: notes
Notes
:::

## Limitaciones de los Grafos de Conocimiento
::::: columns
::: {.column width="30%" }
 ![](img/limitations_kg.jpg)
:::

::: {.column width="70%" .absolute top="230" left="340"}


- **Calidad de los datos**: La precisión y completitud son cruciales.
- **Escalabilidad**: Construcción y mantenimiento a gran escala pueden ser complejos.
- **Interoperabilidad**: Integración con otros sistemas y bases de datos puede ser un desafío.

:::
:::::

::: notes
Notes
:::



# FINE-TUNNING y RETRIEVAL AUGMENTATION GENERATION (RAG)

## FINE-TUNNING

<blockquote cite>

El ajuste fino de un LLM es una técnica de aprendizaje por transferencia en la que se toma un modelo pre-entrenado con un gran conjunto de datos para una tarea general, y se realizan pequeños ajustes a sus parámetros internos para optimizar su rendimiento en una nueva tarea específica

</blockquote>

::: notes
Notes
:::

## TRANSFER LEARNING

![](img/transferlearning.png)

::: notes
Notes
:::

## FINE-TUNNING (ESQUEMA)

![](img/esquemafinetunning.png)

::: notes
Notes
:::

## FINE-TUNNING (ESQUEMA)

![](img/esquemafinetunning.png)

::: notes
Notes
:::

## FINE-TUNING (DEMOSTRACIÓN)

::::: columns
::: {.column width="30%"}
 ![](img/qr_demo_bio.png){.absolute top="200" left="0" width="300" height="300"}
:::

::: {.column width="70%" .absolute top="240" left="340"}


- BioMistral/BioMistral-7B
- mistralai/Mistral-7B-Instruct-v0.1

[Ejemplo](https://aotal.github.io/CursoAI2024/)
:::
:::::

![](img/huggingface_logo.png){.absolute bottom="100" right="50" width="300"}

::: notes
Notes
:::

## RAG (2020)

- **Vectorización del contenido**: Cada fragmento de texto se convierte en un vector numérico utilizando un modelo de embeddings. Estos vectores representan el significado semántico del texto en un espacio multidimensional.

- **Búsqueda de similitud**: Cuando se realiza una consulta, esta también se vectoriza, y se busca en la base de datos vectorial los fragmentos cuyos vectores sean más cercanos al de la consulta (similaridad coseno, distancia euclidiana, etc.).

::: notes
Notes
:::

## ESQUEMA RAG

![](img/esquemarag.png)

::: notes
Notes
:::

## FINE-TUNNING VS RAG


<div style="display: flex; justify-content: center; align-items: center; height: 75%; margin-top: -60px" >
  <table style="transform: scale(0.75);">
    <tr>
      <th>Característica</th>
      <th>Fine-tuning</th>
      <th>RAG</th>
    </tr>
    <tr>
      <td>Enfoque principal</td>
      <td>Adaptación del modelo</td>
      <td>Aumento de la información</td>
    </tr>
    <tr>
      <td>Método</td>
      <td>Ajuste de parámetros</td>
      <td>Recuperación de información externa</td>
    </tr>
    <tr>
      <td>Ventajas</td>
      <td>Personalización del modelo, eficiencia</td>
      <td>Respuestas contextualmente relevantes, precisión</td>
    </tr>
    <tr>
      <td>Limitaciones</td>
      <td>Dificultad con datos cambiantes, adaptación de estilo limitada</td>
      <td>Adaptación de estilo limitada, enfoque en recuperación</td>
    </tr>
  </table>
</div>

::: notes
Notes
:::

## FINE-TUNNING VS RAG

::: {layout="[15,-5,12]"}
![Finne-tunning](img/medico_fine_tunning.jpg)

![RAG](img/lector_rag.jpg)
:::

::: notes
Notes
:::





## prueba2

[JSON](https://g.co/gemini/share/1a71601dec07)

