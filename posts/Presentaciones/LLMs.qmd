---
title: "LARGE LANGUAGE MODELS (LLM) APLICADOS AL ENTORNO MÉDICO"
date: "2025-02-12"
categories: [Presentación]
image: img/portada_llm.png
author: "[Antonio Otal Palacín](antonio.otal@udl.cat)"
institute: Hospital Universitari Arnau de Vilanova (Lleida)
lang: es
format: 
    revealjs:
        logo: 'https://cdn0.iconfinder.com/data/icons/modern-ui-glyph-1/64/modern-ui-glyph-1-03-512.png'
        footer-logo-link: "https://aotal.github.io/ImagenMedica/"
        footer: "[IAMED](https://cv.udl.cat/portal/site/100794-2324/tool/3d5c2c81-a50b-43ce-a0bf-ecbfd152350b)"
        css: styles.css
        number-sections: false
        slide-number: false
        center: false
        preview-links: true
        smaller: true
        navigation-mode: vertical
filters:
  - reveal-header        
jupyter: python3   
---

## Índice

-   Introducción breve a los LLM
-   Embedding Systems
-   Knowledge Graphs
-   Fine-Tuning y Retrieval Augmentation Generation (RAG)
-   Límites de los LLM

::: notes
Notes
:::

# Introducción breve a los LLM

::: notes
Notes
:::

## LLM

![](img/llm.gif){fig-align="center"}

::: notes
Notes
:::

## LLM Pretraining

A una velocidad de 1000 millones de operaciones por segundo entrenar a GPT3 nos costaría 100 millones de años.
Se hubiese tenido que empezar a entrenar el modelo en el cretácico.

![](img/cretacico_a_hoy.jpg){fig-align="right" width="300" height="300"}

::: notes
Notes
:::

## LLM Reinforcement Learning with Human Feedback

![](img/pretraining.gif){fig-align="center"}

::: notes
Notes
:::

## LLM GPU

![](img/gpu.gif){fig-align="center"}

::: notes
Notes
:::

## TRANSFORMERS

::::: columns
::: {.column width="20%"}
![](img/transformer.jpg){.absolute top="200left=0" width="200" height="200"} ![](img/qrcode_transformers.png){.absolute top="350" left="0" width="200" height="200"} ![](img/deeplearningai.png){.absolute top="550" left="0" width="200"}
:::

::: {.column width="80%"}

**Codificaciones posicionales**: Los transformers agregan un número a cada palabra para indicar su posición en la oración. Esto ayuda al modelo a comprender el orden de las palabras, lo cual es crucial para el significado.

**Atención**: Permite al modelo considerar todas las palabras de la oración al traducir o analizar una palabra específica. Esto ayuda a capturar relaciones complejas entre palabras y mejora la precisión de la traducción y la comprensión.

**Autoatención**: Permite al modelo comprender una palabra en el contexto de las palabras que la rodean, lo que ayuda a desambiguar palabras con múltiples significados y captar matices del lenguaje.

:::
:::::

::: notes
Notes
:::

## Codificación Posicional

- **Problema**: A diferencia de las RNNs, los Transformers procesan palabras en paralelo, perdiendo información sobre el orden de las palabras.

- **Solución**: La codificación posicional añade información sobre la posición de cada palabra mediante vectores que se suman a los embeddings de las palabras.

- **Beneficio**: Permite al modelo distinguir oraciones con las mismas palabras en diferente orden, crucial para la interpretación del significado.

::: notes
Notes
:::

## Mecanismo de Atención (Attention)

- **Propósito**: Permite al modelo enfocarse en las partes más relevantes de la secuencia de entrada al generar la salida.

- **Funcionamiento**: Asigna pesos a cada palabra de la entrada, indicando cuánto debe "prestar atención" el modelo.

- **Cálculo**: Se computan matrices Query (Q), Key (K) y Value (V). La atención se calcula como una función de Q y K, ponderando los valores en V para generar un vector de contexto.

- **Beneficio**: Mejora el rendimiento en tareas de secuencia a secuencia al proporcionar información contextualmente relevante.

::: notes
Notes
:::

## Autoatención (Self-Attention)

- **Propósito**: Permite a cada palabra prestar atención a todas las demás palabras dentro de la misma secuencia.

- **Funcionamiento**: Las matrices Q, K y V se calculan a partir de la misma secuencia de entrada.

- **Beneficio**: Permite el procesamiento paralelo y la captura de dependencias a largo alcance, superando las limitaciones de las RNNs.

- **Relación con Atención**: La autoatención es un caso particular del mecanismo de atención.

::: notes
Notes
:::

# Vector Embedding

## EMBEDDING

<blockquote cite>

*Vector embeddings* son representaciones numéricas de información, como texto, documentos, imágenes o audio. Capturan el significado semántico de la información

</blockquote>

![](img/vector_embeddings.jpg){fig-align="right" width="300" height="300"}

::: notes
Notes
:::

## VECTORES

![](img/vectores.gif)

<a href="https://colab.research.google.com/github/aotal/ImagenMedica/blob/master/posts/code/Embedding.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>

::: notes
Notes
:::

## EMBEDDING MACHINE

![](img/embeddings.png)

::: notes
Notes
:::

## CHUNKS

<blockquote cite>

Un *chunk* es una unidad discreta de información extraída de un cuerpo de texto más grande. Puede ser una frase, un párrafo, una sección de un documento o incluso un documento completo, dependiendo del sistema y la aplicación. La idea principal es dividir la información en partes más pequeñas que sean más fáciles de procesar y analizar para la IA.

</blockquote>

![](img/chunk_image.jpg){fig-align="right" width="300" height="300"}


::: notes
Notes
:::

## Métodos de División en Chunks

- **Por encabezados**: Se usan los encabezados y subtítulos del documento para crear chunks.
- **Por párrafos**: Se divide el texto en chunks según los párrafos.
- **Ventanas deslizantes**: Una ventana captura texto al desplazarse, creando chunks que se superponen.
- **Segmentación semántica**: Se usan algoritmos para identificar unidades de significado y dividir el texto en chunks.

::: notes
Notes
:::

## ¿Cómo los LLMs Utilizan los Chunks?

Los LLMs utilizan los chunks para comprender el contexto y generar texto coherente. Al procesar un chunk, el LLM puede:

- **Identificar las ideas principales**: Extraer la información más relevante del chunk.

- **Establecer conexiones**: Relacionar la información del chunk con otros chunks o con el conocimiento previo del LLM. Para lograr esto, los LLMs utilizan mecanismos como la atención y la codificación posicional. La atención permite al modelo enfocarse en las partes más relevantes de la información dentro de un chunk y entre diferentes chunks. La codificación posicional proporciona información sobre la ubicación de las palabras dentro de un chunk y en relación con otros chunks, lo que ayuda al modelo a comprender el orden y la secuencia de la información.

- **Generar resúmenes**: Condensar la información del chunk en un resumen conciso.

- **Responder preguntas**: Proporcionar respuestas precisas y relevantes a las preguntas basadas en la información del chunk.

::: notes
Notes
:::

## Desafíos de los *Chunks* en LLMs

- **Pérdida de contexto**: Dividir el texto en *chunks* puede dificultar la comprensión de relaciones entre datos, afectando la precisión de las respuestas.

- **Sesgo**: La selección de *chunks* puede introducir sesgos, llevando a resultados incompletos o inexactos.

- **Información compleja**: Cierta información (imágenes, datos, etc.) es difícil de representar en *chunks*.

- **Tamaño del *chunk***: Encontrar el tamaño óptimo es crucial para el rendimiento del modelo.

::: notes
Notes
:::


## **Word Embeddings**: Métodos Basados en Frecuencia

- **Bag-of-Words (BoW)**: Conjunto no ordenado de palabras y sus frecuencias. Simple pero ignora el orden y la semántica.

- **TF-IDF**: Asigna pesos a las palabras según su frecuencia en un documento y en el corpus. Mejora BoW pero ignora el contexto.

- **N-gramas**: Considera secuencias de n palabras. Captura contexto local pero basado en frecuencia.

- **Matrices de Co-ocurrencia**: Capturan la frecuencia con la que las palabras aparecen juntas. Base para generar embeddings.

- **One-Hot Encoding**: Vector binario para cada palabra. Sencillo pero de alta dimensionalidad y sin relaciones semánticas.


::: notes
Notes
:::

## **Word Embeddings**: Embeddings Estáticos

Generan un vector único por palabra, independientemente del contexto.

- **Word2Vec (CBOW y Skip-gram)**: Redes neuronales para predecir palabras a partir de su contexto.

- **GloVe**: Combina métodos de conteo y predicción, capturando relaciones semánticas locales y globales.

- **FastText**: Mejora Word2Vec considerando la estructura interna de las palabras.


::: notes
Notes
:::

## **Word Embeddings**: Embeddings Contextuales

Generan embeddings que varían según el contexto, capturando la polisemia.

- **Self-Attention**: Permite a cada palabra relacionarse con todas las demás. Fundamental en Transformers.

- **ELMo**: Considera todo el contexto de la oración para embeddings dinámicos.

- **BERT**: Arquitectura Transformer para embeddings contextuales, capturando contexto izquierdo y derecho.


::: notes
Notes
:::

## **Word Embeddings**: Embeddings Contextuales

Generan embeddings que varían según el contexto, capturando la polisemia.

- **Self-Attention**: Permite a cada palabra relacionarse con todas las demás. Fundamental en Transformers.

- **ELMo**: Considera todo el contexto de la oración para embeddings dinámicos.

- **BERT**: Arquitectura Transformer para embeddings contextuales, capturando contexto izquierdo y derecho.


::: notes
Notes
:::

# KNOWLEDGE GRAPH (KG)

## ¿QUÉ SON LOS KG?

<blockquote cite>
Los grafos de conocimiento son bases de datos inteligentes que representan el conocimiento médico de manera estructurada, permitiendo a la IA comprender y razonar sobre la información de forma más efectiva.
</blockquote>

[![Lord of the rings](img/thering.jpg){width=300px}](https://alon-cohen-gordon.wixsite.com/lotr-graph){preview-links="true"}

::: notes
Notes
:::

## Componentes principales de un grafo de conocimiento

::::: columns
::: {.column width="30%" }
 ![](img/esquema_kg.jpg)
:::

::: {.column width="70%" .absolute top="220" left="340"}


- **Nodos**: Representan entidades, conceptos u objetos, como enfermedades, síntomas, genes o medicamentos.
- **Aristas**: Representan las relaciones entre los nodos, como "causa", "trata", "se asocia con" o "es un tipo de".
- **Etiquetas**: Proporcionan información adicional sobre los nodos y las aristas, como nombres, descripciones o atributos.

:::
:::::

::: notes
Notes
:::

## Beneficios de los Grafos de Conocimiento
::::: columns
::: {.column width="30%" }
 ![](img/beneficios_KG.jpg)
:::

::: {.column width="70%" .absolute top="230" left="340"}


- **Precisión en el diagnóstico**: Integración de información de diversas fuentes.
- **Personalización de tratamientos**: Uso de información genómica, clínica y de estilo de vida.
- **Descubrimiento de fármacos**: Identificación de nuevas relaciones entre genes, proteínas y enfermedades.

:::
:::::

::: notes
Notes
:::

## Limitaciones de los Grafos de Conocimiento
::::: columns
::: {.column width="30%" }
 ![](img/limitations_kg.jpg)
:::

::: {.column width="70%" .absolute top="230" left="340"}


- **Calidad de los datos**: La precisión y completitud son cruciales.
- **Escalabilidad**: Construcción y mantenimiento a gran escala pueden ser complejos.
- **Interoperabilidad**: Integración con otros sistemas y bases de datos puede ser un desafío.

:::
:::::

::: notes
Notes
:::


# FINE-TUNNING y RETRIEVAL AUGMENTATION GENERATION (RAG)

## FINE-TUNNING

<blockquote cite>

El ajuste fino de un LLM es una técnica de aprendizaje por transferencia en la que se toma un modelo pre-entrenado con un gran conjunto de datos para una tarea general, y se realizan pequeños ajustes a sus parámetros internos para optimizar su rendimiento en una nueva tarea específica

</blockquote>

![](img/fine_tunning_presentacion.jpg){fig-align="right" width="350" height="350"}

::: notes
Notes
:::

## TRANSFER LEARNING

![](img/transferlearning.png)

::: notes
Notes
:::

## FINE-TUNNING (ESQUEMA)

![](img/esquemafinetunning.png)

::: notes
Notes
:::

## FINE-TUNNING (ESQUEMA)

![](img/esquemafinetunning.png)

::: notes
Notes
:::

## FINE-TUNING (DEMOSTRACIÓN)

::::: columns
::: {.column width="30%"}
 ![](img/qr_demo_bio.png){.absolute top="200" left="0" width="300" height="300"}
:::

::: {.column width="70%" .absolute top="240" left="340"}


- BioMistral/BioMistral-7B
- mistralai/Mistral-7B-Instruct-v0.1

[Ejemplo](https://aotal.github.io/CursoAI2024/)
:::
:::::

![](img/huggingface_logo.png){.absolute bottom="100" right="50" width="300"}

::: notes
Notes
:::

## RAG (2020)

- **Vectorización del contenido**: Cada fragmento de texto se convierte en un vector numérico utilizando un modelo de embeddings. Estos vectores representan el significado semántico del texto en un espacio multidimensional.

- **Búsqueda de similitud**: Cuando se realiza una consulta, esta también se vectoriza, y se busca en la base de datos vectorial los fragmentos cuyos vectores sean más cercanos al de la consulta (similaridad coseno, distancia euclidiana, etc.).

::: notes
Notes
:::

## ESQUEMA RAG

![](img/esquemarag.png)

::: notes
Notes
:::

## FINE-TUNNING VS RAG


https://notebooklm.google.com/notebook/d40fef92-ed62-4659-9b3e-0e5cde949f45
https://notebooklm.google.com/notebook/35c43bd2-d012-45fb-a514-aa2958b1fdda


<div style="display: flex; justify-content: center; align-items: center; height: 75%; margin-top: -60px" >
  <table style="transform: scale(0.75);">
    <tr>
      <th>Característica</th>
      <th>Fine-tuning</th>
      <th>RAG</th>
    </tr>
    <tr>
      <td>Enfoque principal</td>
      <td>Adaptación del modelo</td>
      <td>Aumento de la información</td>
    </tr>
    <tr>
      <td>Método</td>
      <td>Ajuste de parámetros</td>
      <td>Recuperación de información externa</td>
    </tr>
    <tr>
      <td>Ventajas</td>
      <td>Personalización del modelo, eficiencia</td>
      <td>Respuestas contextualmente relevantes, precisión</td>
    </tr>
    <tr>
      <td>Limitaciones</td>
      <td>Dificultad con datos cambiantes, adaptación de estilo limitada</td>
      <td>Adaptación de estilo limitada, enfoque en recuperación</td>
    </tr>
  </table>
</div>

::: notes
Notes
:::

## FINE-TUNNING VS RAG

::: {layout="[15,-5,12]"}
![Finne-tunning](img/medico_fine_tunning.jpg)

![RAG](img/lector_rag.jpg)
:::

::: notes
Notes
:::

## Ejemplo RAG

**Pregunta**:Basándote en la historia clínica proporcionada, ¿cuáles son los tres factores de riesgo cardiovascular más relevantes para la paciente María Pérez y por qué son importantes en este caso específico?

[Historia mal estructurada](https://notebooklm.google.com/notebook/d40fef92-ed62-4659-9b3e-0e5cde949f45)

[Historia bien estructurada](https://notebooklm.google.com/notebook/35c43bd2-d012-45fb-a514-aa2958b1fdda)

![](img/notebooklm_logo.png)

# Límites de los LLM

::: notes
Notes
:::

## Límites de los LLM (Clásicos)

- **Falta de comprensión del mundo real**: Un LLM podría tener dificultades para responder preguntas sobre eventos actuales, lugares geográficos o conceptos abstractos.

- **Sesgo**: Los LLMs pueden perpetuar y amplificar los sesgos presentes en los datos de entrenamiento. Esto puede resultar en respuestas discriminatorias, ofensivas o estereotipadas. Es importante abordar el sesgo en los LLMs para garantizar la equidad y la inclusión en las aplicaciones de IA.

**Incapacidad para razonar**: Un LLM podría tener dificultades para resolver un problema matemático o para comprender las relaciones causales entre diferentes eventos.

::: notes
Notes
:::

## Límites de los LLM (Clásicos) II

**Dificultad para manejar información nueva o inesperada**: Los LLMs se basan en los datos con los que fueron entrenados y pueden tener dificultades para procesar información nueva o inesperada. Esto puede llevar a respuestas inexactas o irrelevantes cuando se enfrentan a situaciones novedosas.

**Dificultad con el lenguaje complejo**: Los LLMs pueden tener dificultades para comprender y procesar el lenguaje complejo o matizado, como el sarcasmo, las metáforas y otras figuras retóricas..

**Limitaciones en la creatividad**: Los LLMs pueden generar diferentes tipos de contenido creativo de formato textual, como poemas, guiones, piezas musicales y correos electrónicos. Sin embargo, su capacidad para producir contenido verdaderamente original y creativo es limitada.

::: notes
Notes
:::

# GRACIAS POR VUESTRA ATENCIÓN


