{
  "hash": "4625ce706c3638c0efbc1b5729f54531",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"LARGE LANGUAGE MODELS (LLM) APLICADOS AL ENTORNO MÉDICO\"\ndate: \"2025-02-10\"\ncategories: [Presentación]\nimage: img/portada_llm.png\nauthor: \"[Antonio Otal Palacín](antonio.otal@udl.cat)\"\ninstitute: Hospital Universitari Arnau de Vilanova (Lleida)\nlang: es\nformat: \n    revealjs:\n        logo: 'https://cdn0.iconfinder.com/data/icons/modern-ui-glyph-1/64/modern-ui-glyph-1-03-512.png'\n        footer-logo-link: \"/\"\n        footer: \"[IAMED](https://cv.udl.cat/portal/site/100794-2324/tool/3d5c2c81-a50b-43ce-a0bf-ecbfd152350b)\"\n        css: styles.css\n        number-sections: false\n        slide-number: false\n        center: false\nfilters:\n  - reveal-header        \njupyter: python3   \n---\n\n\n## Índice\n\n-   Introducción breve a los LLM\n-   Embedding Systems\n-   Fine-Tuning y Retrieval Augmentation Generation (RAG)\n-   Ejemplo de RAG según la estructuración de los datos\n-   Knowledge Graphs\n-   Límites de los LLM\n\n::: notes\nNotes\n:::\n\n# Introducción breve a los LLM\n\n::: notes\nNotes\n:::\n\n## LLM\n\n![](img/llm.gif){fig-align=\"center\"}\n\n::: notes\nNotes\n:::\n\n## LLM Pretraining\n\nA una velocidad de 1000 millones de operaciones por segundo entrenar a GPT3 nos costaría 100 millones de años.\n\nSe hubiese tenido que empezar a entrenar el modelo en el cretácico.\n\n::: notes\nNotes\n:::\n\n## LLM Reinforcement Learning with Human Feedback\n\n![](img/pretraining.gif){fig-align=\"center\"}\n\n::: notes\nNotes\n:::\n\n## LLM GPU\n\n![](img/gpu.gif){fig-align=\"center\"}\n\n::: notes\nNotes\n:::\n\n## TRANSFORMERS\n\n::::: columns\n::: {.column width=\"20%\"}\n![](img/transformer.jpg){.absolute top=\"200left=0\" width=\"200\" height=\"200\"} ![](img/qrcode_transformers.png){.absolute top=\"350\" left=\"0\" width=\"200\" height=\"200\"} ![](img/deeplearningai.png){.absolute top=\"550\" left=\"0\" width=\"200\"}\n:::\n\n::: {.column width=\"80%\"}\n<p style=\"font-size: 30px;\">\n\n**Codificaciones posicionales**: Los transformers agregan un número a cada palabra para indicar su posición en la oración. Esto ayuda al modelo a comprender el orden de las palabras, lo cual es crucial para el significado.\n\n</p>\n\n<p style=\"font-size: 30px;\">\n\n**Atención**: Permite al modelo considerar todas las palabras de la oración al traducir o analizar una palabra específica. Esto ayuda a capturar relaciones complejas entre palabras y mejora la precisión de la traducción y la comprensión.\n\n</p>\n\n<p style=\"font-size: 30px;\">\n\n**Autoatención**: Permite al modelo comprender una palabra en el contexto de las palabras que la rodean, lo que ayuda a desambiguar palabras con múltiples significados y captar matices del lenguaje.\n\n</p>\n:::\n:::::\n\n::: notes\nNotes\n:::\n\n# Vector Embedding\n\n## EMBEDDING\n\n<blockquote cite>\n\n*Vector embeddings* son representaciones numéricas de información, como texto, documentos, imágenes o audio. Capturan el significado semántico de la información\n\n</blockquote>\n\n::: notes\nNotes\n:::\n\n## VECTORES\n\n![](img/vectores.gif)\n\n::: notes\nNotes\n:::\n\n## EMBEDDING MACHINE\n\n![](img/embeddings.png)\n\n::: notes\nNotes\n:::\n\n# FINE-TUNNING y RETRIEVAS AUGMENTATION GENERATION (RAG)\n\n## FINE-TUNNING\n\n<blockquote cite>\n\nEl ajuste fino de un LLM es una técnica de aprendizaje por transferencia en la que se toma un modelo pre-entrenado con un gran conjunto de datos para una tarea general, y se realizan pequeños ajustes a sus parámetros internos para optimizar su rendimiento en una nueva tarea específica\n\n</blockquote>\n\n::: notes\nNotes\n:::\n\n## TRANSFER LEARNING\n\n![](img/transferlearning.png)\n\n::: notes\nNotes\n:::\n\n## FINE-TUNNING (ESQUEMA)\n\n![](img/esquemafinetunning.png)\n\n::: notes\nNotes\n:::\n\n## FINE-TUNNING (ESQUEMA)\n\n![](img/esquemafinetunning.png)\n\n::: notes\nNotes\n:::\n\n## FINE-TUNING (DEMOSTRACIÓN)\n\n::::: columns\n::: {.column width=\"30%\"}\n ![](img/qr_demo_bio.png){.absolute top=\"200\" left=\"0\" width=\"300\" height=\"300\"}\n:::\n\n::: {.column width=\"70%\" .absolute top=\"240\" left=\"340\"}\n\n\n- BioMistral/BioMistral-7B\n- mistralai/Mistral-7B-Instruct-v0.1\n\n[Ejemplo](https://aotal.github.io/CursoAI2024/)\n:::\n:::::\n\n![](img/huggingface_logo.png){.absolute bottom=\"100\" right=\"50\" width=\"300\"}\n\n::: notes\nNotes\n:::\n\n## RAG (2020)\n\n- **Vectorización del contenido**: Cada fragmento de texto se convierte en un vector numérico utilizando un modelo de embeddings. Estos vectores representan el significado semántico del texto en un espacio multidimensional.\n\n- **Búsqueda de similitud**: Cuando se realiza una consulta, esta también se vectoriza, y se busca en la base de datos vectorial los fragmentos cuyos vectores sean más cercanos al de la consulta (similaridad coseno, distancia euclidiana, etc.).\n\n::: notes\nNotes\n:::\n\n## ESQUEMA RAG\n\n![](img/esquemarag.png)\n\n::: notes\nNotes\n:::\n\n## FINE-TUNNING VS RAG\n\n\n<div style=\"display: flex; justify-content: center; align-items: center; height: 75%; margin-top: -60px\" >\n  <table style=\"transform: scale(0.75);\">\n    <tr>\n      <th>Característica</th>\n      <th>Fine-tuning</th>\n      <th>RAG</th>\n    </tr>\n    <tr>\n      <td>Enfoque principal</td>\n      <td>Adaptación del modelo</td>\n      <td>Aumento de la información</td>\n    </tr>\n    <tr>\n      <td>Método</td>\n      <td>Ajuste de parámetros</td>\n      <td>Recuperación de información externa</td>\n    </tr>\n    <tr>\n      <td>Ventajas</td>\n      <td>Personalización del modelo, eficiencia</td>\n      <td>Respuestas contextualmente relevantes, precisión</td>\n    </tr>\n    <tr>\n      <td>Limitaciones</td>\n      <td>Dificultad con datos cambiantes, adaptación de estilo limitada</td>\n      <td>Adaptación de estilo limitada, enfoque en recuperación</td>\n    </tr>\n  </table>\n</div>\n\n::: notes\nNotes\n:::\n\n## FINE-TUNNING VS RAG\n\n::: {layout=\"[15,-5,12]\"}\n![Finne-tunning](img/medico_fine_tunning.jpg)\n\n![RAG](img/lector_rag.jpg)\n:::\n\n::: notes\nNotes\n:::\n\n# KNOWLEDGE GRAPH\n\n## Lord os the rings\n\n\n```{=html}\n<iframe width=\"780\" height=\"500\" src=\"https://alon-cohen-gordon.wixsite.com/lotr-graph\" title=\"Lord of the rings\"></iframe>\n```\n\n\n\n## Prueba\n\n\n```{=html}\n<iframe width=\"1000\" height=\"800\" src=\"html/1_CrearJSON_medicamentos/1_CrearJSON_medicamentos.html\" title=\"CrearJSON\"></iframe>\n```\n\n",
    "supporting": [
      "LLMs_files"
    ],
    "filters": [],
    "includes": {}
  }
}