{
  "hash": "9930bf2e07f45361ff290d526da92cf0",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: LARGE LANGUAGE MODELS (LLM) APLICADOS AL ENTORNO MÉDICO\ndate: '2025-02-12'\ncategories:\n  - Presentación\nimage: img/portada_llm.png\nauthor: '[Antonio Otal Palacín](antonio.otal@udl.cat)'\ninstitute: Hospital Universitari Arnau de Vilanova (Lleida)\nlang: es\nformat:\n  revealjs:\n    logo: 'https://cdn0.iconfinder.com/data/icons/modern-ui-glyph-1/64/modern-ui-glyph-1-03-512.png'\n    footer-logo-link: /\n    footer: '[IAMED](https://cv.udl.cat/portal/site/100794-2324/tool/3d5c2c81-a50b-43ce-a0bf-ecbfd152350b)'\n    css: styles.css\n    number-sections: false\n    slide-number: false\n    center: false\n    preview-links: true\n    smaller: true\nfilters:\n  - reveal-header\n---\n\n## Índice\n\n-   Introducción breve a los LLM\n-   Embedding Systems\n-   Knowledge Graphs\n-   Bases de datos\n-   Fine-Tuning y Retrieval Augmentation Generation (RAG)\n-   Límites de los LLM\n\n::: notes\nNotes\n:::\n\n# Introducción breve a los LLM\n\n::: notes\nNotes\n:::\n\n## LLM\n\n![](img/llm.gif){fig-align=\"center\"}\n\n::: notes\nNotes\n:::\n\n## LLM Pretraining\n\nA una velocidad de 1000 millones de operaciones por segundo entrenar a GPT3 nos costaría 100 millones de años.\n\nSe hubiese tenido que empezar a entrenar el modelo en el cretácico.\n\n::: notes\nNotes\n:::\n\n## LLM Reinforcement Learning with Human Feedback\n\n![](img/pretraining.gif){fig-align=\"center\"}\n\n::: notes\nNotes\n:::\n\n## LLM GPU\n\n![](img/gpu.gif){fig-align=\"center\"}\n\n::: notes\nNotes\n:::\n\n## TRANSFORMERS\n\n::::: columns\n::: {.column width=\"20%\"}\n![](img/transformer.jpg){.absolute top=\"200left=0\" width=\"200\" height=\"200\"} ![](img/qrcode_transformers.png){.absolute top=\"350\" left=\"0\" width=\"200\" height=\"200\"} ![](img/deeplearningai.png){.absolute top=\"550\" left=\"0\" width=\"200\"}\n:::\n\n::: {.column width=\"80%\"}\n\n**Codificaciones posicionales**: Los transformers agregan un número a cada palabra para indicar su posición en la oración. Esto ayuda al modelo a comprender el orden de las palabras, lo cual es crucial para el significado.\n\n**Atención**: Permite al modelo considerar todas las palabras de la oración al traducir o analizar una palabra específica. Esto ayuda a capturar relaciones complejas entre palabras y mejora la precisión de la traducción y la comprensión.\n\n**Autoatención**: Permite al modelo comprender una palabra en el contexto de las palabras que la rodean, lo que ayuda a desambiguar palabras con múltiples significados y captar matices del lenguaje.\n\n:::\n:::::\n\n::: notes\nNotes\n:::\n\n# Vector Embedding\n\n## EMBEDDING\n\n<blockquote cite>\n\n*Vector embeddings* son representaciones numéricas de información, como texto, documentos, imágenes o audio. Capturan el significado semántico de la información\n\n</blockquote>\n\n::: notes\nNotes\n:::\n\n## VECTORES\n\n![](img/vectores.gif)\n\n<a href=\"https://colab.research.google.com/github/aotal/ImagenMedica/blob/master/posts/code/Embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n\n::: notes\nNotes\n:::\n\n## EMBEDDING MACHINE\n\n![](img/embeddings.png)\n\n::: notes\nNotes\n:::\n\n## CHUNKS\n\n<blockquote cite>\n\nUn *chunk* es una unidad discreta de información extraída de un cuerpo de texto más grande. Puede ser una frase, un párrafo, una sección de un documento o incluso un documento completo, dependiendo del sistema y la aplicación. La idea principal es dividir la información en partes más pequeñas que sean más fáciles de procesar y analizar para la IA.\n\n</blockquote>\n\n\n::: notes\nNotes\n:::\n\n## Métodos de División en Chunks\n\n- **Por encabezados**: Se usan los encabezados y subtítulos del documento para crear chunks.\n- **Por párrafos**: Se divide el texto en chunks según los párrafos.\n- **Ventanas deslizantes**: Una ventana captura texto al desplazarse, creando chunks que se superponen.\n- **Segmentación semántica**: Se usan algoritmos para identificar unidades de significado y dividir el texto en chunks.\n\n::: notes\nNotes\n:::\n\n## ¿Cómo los LLMs Utilizan los Chunks?\n\nLos LLMs utilizan los chunks para comprender el contexto y generar texto coherente. Al procesar un chunk, el LLM puede:\n\n- **Identificar las ideas principales**: Extraer la información más relevante del chunk.\n\n- **Establecer conexiones**: Relacionar la información del chunk con otros chunks o con el conocimiento previo del LLM. Para lograr esto, los LLMs utilizan mecanismos como la atención y la codificación posicional. La atención permite al modelo enfocarse en las partes más relevantes de la información dentro de un chunk y entre diferentes chunks. La codificación posicional proporciona información sobre la ubicación de las palabras dentro de un chunk y en relación con otros chunks, lo que ayuda al modelo a comprender el orden y la secuencia de la información.\n\n- **Generar resúmenes**: Condensar la información del chunk en un resumen conciso.\n\n- **Responder preguntas**: Proporcionar respuestas precisas y relevantes a las preguntas basadas en la información del chunk.\n\n::: notes\nNotes\n:::\n\n## Desafíos de los *Chunks* en LLMs\n\n- Pérdida de contexto: Dividir el texto en *chunks* puede dificultar la comprensión de relaciones entre datos, afectando la precisión de las respuestas.\n\n- Sesgo: La selección de *chunks* puede introducir sesgos, llevando a resultados incompletos o inexactos.\n\n- Información compleja: Cierta información (imágenes, datos, etc.) es difícil de representar en *chunks*.\n\n- Tamaño del *chunk*: Encontrar el tamaño óptimo es crucial para el rendimiento del modelo.\n\n\n::: notes\nNotes\n:::\n\n# KNOWLEDGE GRAPH (KG)\n\n## ¿QUÉ SON LOS KG?\n\n<blockquote cite>\nLos grafos de conocimiento son bases de datos inteligentes que representan el conocimiento médico de manera estructurada, permitiendo a la IA comprender y razonar sobre la información de forma más efectiva.\n</blockquote>\n\n[![Lord of the rings](img/thering.jpg){width=300px}](https://alon-cohen-gordon.wixsite.com/lotr-graph){preview-links=\"true\" .h}\n\n::: notes\nNotes\n:::\n\n## Componentes principales de un grafo de conocimiento\n\n::::: columns\n::: {.column width=\"30%\" }\n ![](img/esquema_kg.jpg)\n:::\n\n::: {.column width=\"70%\" .absolute top=\"220\" left=\"340\"}\n\n\n- **Nodos**: Representan entidades, conceptos u objetos, como enfermedades, síntomas, genes o medicamentos.\n- **Aristas**: Representan las relaciones entre los nodos, como \"causa\", \"trata\", \"se asocia con\" o \"es un tipo de\".\n- **Etiquetas**: Proporcionan información adicional sobre los nodos y las aristas, como nombres, descripciones o atributos.\n\n:::\n:::::\n\n::: notes\nNotes\n:::\n\n## Beneficios de los Grafos de Conocimiento\n::::: columns\n::: {.column width=\"30%\" }\n ![](img/beneficios_KG.jpg)\n:::\n\n::: {.column width=\"70%\" .absolute top=\"230\" left=\"340\"}\n\n\n- **Precisión en el diagnóstico**: Integración de información de diversas fuentes.\n- **Personalización de tratamientos**: Uso de información genómica, clínica y de estilo de vida.\n- **Descubrimiento de fármacos**: Identificación de nuevas relaciones entre genes, proteínas y enfermedades.\n\n:::\n:::::\n\n::: notes\nNotes\n:::\n\n## Limitaciones de los Grafos de Conocimiento\n::::: columns\n::: {.column width=\"30%\" }\n ![](img/limitations_kg.jpg)\n:::\n\n::: {.column width=\"70%\" .absolute top=\"230\" left=\"340\"}\n\n\n- **Calidad de los datos**: La precisión y completitud son cruciales.\n- **Escalabilidad**: Construcción y mantenimiento a gran escala pueden ser complejos.\n- **Interoperabilidad**: Integración con otros sistemas y bases de datos puede ser un desafío.\n\n:::\n:::::\n\n::: notes\nNotes\n:::\n\n\n\n# FINE-TUNNING y RETRIEVAL AUGMENTATION GENERATION (RAG)\n\n## FINE-TUNNING\n\n<blockquote cite>\n\nEl ajuste fino de un LLM es una técnica de aprendizaje por transferencia en la que se toma un modelo pre-entrenado con un gran conjunto de datos para una tarea general, y se realizan pequeños ajustes a sus parámetros internos para optimizar su rendimiento en una nueva tarea específica\n\n</blockquote>\n\n::: notes\nNotes\n:::\n\n## TRANSFER LEARNING\n\n![](img/transferlearning.png)\n\n::: notes\nNotes\n:::\n\n## FINE-TUNNING (ESQUEMA)\n\n![](img/esquemafinetunning.png)\n\n::: notes\nNotes\n:::\n\n## FINE-TUNNING (ESQUEMA)\n\n![](img/esquemafinetunning.png)\n\n::: notes\nNotes\n:::\n\n## FINE-TUNING (DEMOSTRACIÓN)\n\n::::: columns\n::: {.column width=\"30%\"}\n ![](img/qr_demo_bio.png){.absolute top=\"200\" left=\"0\" width=\"300\" height=\"300\"}\n:::\n\n::: {.column width=\"70%\" .absolute top=\"240\" left=\"340\"}\n\n\n- BioMistral/BioMistral-7B\n- mistralai/Mistral-7B-Instruct-v0.1\n\n[Ejemplo](https://aotal.github.io/CursoAI2024/)\n:::\n:::::\n\n![](img/huggingface_logo.png){.absolute bottom=\"100\" right=\"50\" width=\"300\"}\n\n::: notes\nNotes\n:::\n\n## RAG (2020)\n\n- **Vectorización del contenido**: Cada fragmento de texto se convierte en un vector numérico utilizando un modelo de embeddings. Estos vectores representan el significado semántico del texto en un espacio multidimensional.\n\n- **Búsqueda de similitud**: Cuando se realiza una consulta, esta también se vectoriza, y se busca en la base de datos vectorial los fragmentos cuyos vectores sean más cercanos al de la consulta (similaridad coseno, distancia euclidiana, etc.).\n\n::: notes\nNotes\n:::\n\n## ESQUEMA RAG\n\n![](img/esquemarag.png)\n\n::: notes\nNotes\n:::\n\n## FINE-TUNNING VS RAG\n\n\n<div style=\"display: flex; justify-content: center; align-items: center; height: 75%; margin-top: -60px\" >\n  <table style=\"transform: scale(0.75);\">\n    <tr>\n      <th>Característica</th>\n      <th>Fine-tuning</th>\n      <th>RAG</th>\n    </tr>\n    <tr>\n      <td>Enfoque principal</td>\n      <td>Adaptación del modelo</td>\n      <td>Aumento de la información</td>\n    </tr>\n    <tr>\n      <td>Método</td>\n      <td>Ajuste de parámetros</td>\n      <td>Recuperación de información externa</td>\n    </tr>\n    <tr>\n      <td>Ventajas</td>\n      <td>Personalización del modelo, eficiencia</td>\n      <td>Respuestas contextualmente relevantes, precisión</td>\n    </tr>\n    <tr>\n      <td>Limitaciones</td>\n      <td>Dificultad con datos cambiantes, adaptación de estilo limitada</td>\n      <td>Adaptación de estilo limitada, enfoque en recuperación</td>\n    </tr>\n  </table>\n</div>\n\n::: notes\nNotes\n:::\n\n## FINE-TUNNING VS RAG\n\n::: {layout=\"[15,-5,12]\"}\n![Finne-tunning](img/medico_fine_tunning.jpg)\n\n![RAG](img/lector_rag.jpg)\n:::\n\n::: notes\nNotes\n:::\n\n\n\n\n\n## prueba2\n\n[JSON](https://g.co/gemini/share/1a71601dec07)\n\n",
    "supporting": [
      "LLMs_files"
    ],
    "filters": [],
    "includes": {}
  }
}