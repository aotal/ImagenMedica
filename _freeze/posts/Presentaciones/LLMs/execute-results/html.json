{
  "hash": "5ad1bd0b4672f1755de9f9a50660c09e",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"LARGE LANGUAGE MODELS (LLM) APLICADOS AL ENTORNO MÉDICO\"\ndate: \"2025-02-12\"\ncategories: [Presentación]\nimage: img/portada_llm.png\nauthor: \"[Antonio Otal Palacín](antonio.otal@udl.cat)\"\ninstitute: Hospital Universitari Arnau de Vilanova (Lleida)\nlang: es\nformat: \n    revealjs:\n        logo: 'https://cdn0.iconfinder.com/data/icons/modern-ui-glyph-1/64/modern-ui-glyph-1-03-512.png'\n        footer-logo-link: \"/\"\n        footer: \"[IAMED](https://cv.udl.cat/portal/site/100794-2324/tool/3d5c2c81-a50b-43ce-a0bf-ecbfd152350b)\"\n        css: styles.css\n        number-sections: false\n        slide-number: false\n        center: false\n        preview-links: true\n        smaller: true\n        navigation-mode: vertical\nfilters:\n  - reveal-header        \njupyter: python3   \n---\n\n\n## Índice\n\n-   Introducción breve a los LLM\n-   Embedding Systems\n-   Knowledge Graphs\n-   Fine-Tuning y Retrieval Augmentation Generation (RAG)\n-   Límites de los LLM\n\n::: notes\nNotes\n:::\n\n# Introducción breve a los LLM\n\n::: notes\nNotes\n:::\n\n## LLM\n\n![](img/llm.gif){fig-align=\"center\"}\n\n::: notes\nNotes\n:::\n\n## LLM Pretraining\n\nA una velocidad de 1000 millones de operaciones por segundo entrenar a GPT3 nos costaría 100 millones de años.\nSe hubiese tenido que empezar a entrenar el modelo en el cretácico.\n\n![](img/cretacico_a_hoy.jpg){fig-align=\"right\" width=\"300\" height=\"300\"}\n\n::: notes\nNotes\n:::\n\n## LLM Reinforcement Learning with Human Feedback\n\n![](img/pretraining.gif){fig-align=\"center\"}\n\n::: notes\nNotes\n:::\n\n## LLM GPU\n\n![](img/gpu.gif){fig-align=\"center\"}\n\n::: notes\nNotes\n:::\n\n## TRANSFORMERS\n\n::::: columns\n::: {.column width=\"20%\"}\n![](img/transformer.jpg){.absolute top=\"200left=0\" width=\"200\" height=\"200\"} ![](img/qrcode_transformers.png){.absolute top=\"350\" left=\"0\" width=\"200\" height=\"200\"} ![](img/deeplearningai.png){.absolute top=\"550\" left=\"0\" width=\"200\"}\n:::\n\n::: {.column width=\"80%\"}\n\n**Codificaciones posicionales**: Los transformers agregan un número a cada palabra para indicar su posición en la oración. Esto ayuda al modelo a comprender el orden de las palabras, lo cual es crucial para el significado.\n\n**Atención**: Permite al modelo considerar todas las palabras de la oración al traducir o analizar una palabra específica. Esto ayuda a capturar relaciones complejas entre palabras y mejora la precisión de la traducción y la comprensión.\n\n**Autoatención**: Permite al modelo comprender una palabra en el contexto de las palabras que la rodean, lo que ayuda a desambiguar palabras con múltiples significados y captar matices del lenguaje.\n\n:::\n:::::\n\n::: notes\nNotes\n:::\n\n## Codificación Posicional\n\n- **Problema**: A diferencia de las RNNs, los Transformers procesan palabras en paralelo, perdiendo información sobre el orden de las palabras.\n\n- **Solución**: La codificación posicional añade información sobre la posición de cada palabra mediante vectores que se suman a los embeddings de las palabras.\n\n- **Beneficio**: Permite al modelo distinguir oraciones con las mismas palabras en diferente orden, crucial para la interpretación del significado.\n\n::: notes\nNotes\n:::\n\n## Mecanismo de Atención (Attention)\n\n- **Propósito**: Permite al modelo enfocarse en las partes más relevantes de la secuencia de entrada al generar la salida.\n\n- **Funcionamiento**: Asigna pesos a cada palabra de la entrada, indicando cuánto debe \"prestar atención\" el modelo.\n\n- **Cálculo**: Se computan matrices Query (Q), Key (K) y Value (V). La atención se calcula como una función de Q y K, ponderando los valores en V para generar un vector de contexto.\n\n- **Beneficio**: Mejora el rendimiento en tareas de secuencia a secuencia al proporcionar información contextualmente relevante.\n\n::: notes\nNotes\n:::\n\n## Autoatención (Self-Attention)\n\n- **Propósito**: Permite a cada palabra prestar atención a todas las demás palabras dentro de la misma secuencia.\n\n- **Funcionamiento**: Las matrices Q, K y V se calculan a partir de la misma secuencia de entrada.\n\n- **Beneficio**: Permite el procesamiento paralelo y la captura de dependencias a largo alcance, superando las limitaciones de las RNNs.\n\n- **Relación con Atención**: La autoatención es un caso particular del mecanismo de atención.\n\n::: notes\nNotes\n:::\n\n# Vector Embedding\n\n## EMBEDDING\n\n<blockquote cite>\n\n*Vector embeddings* son representaciones numéricas de información, como texto, documentos, imágenes o audio. Capturan el significado semántico de la información\n\n</blockquote>\n\n![](img/vector_embeddings.jpg){fig-align=\"right\" width=\"300\" height=\"300\"}\n\n::: notes\nNotes\n:::\n\n## VECTORES\n\n![](img/vectores.gif)\n\n<a href=\"https://colab.research.google.com/github/aotal/ImagenMedica/blob/master/posts/code/Embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n\n::: notes\nNotes\n:::\n\n## EMBEDDING MACHINE\n\n![](img/embeddings.png)\n\n::: notes\nNotes\n:::\n\n## CHUNKS\n\n<blockquote cite>\n\nUn *chunk* es una unidad discreta de información extraída de un cuerpo de texto más grande. Puede ser una frase, un párrafo, una sección de un documento o incluso un documento completo, dependiendo del sistema y la aplicación. La idea principal es dividir la información en partes más pequeñas que sean más fáciles de procesar y analizar para la IA.\n\n</blockquote>\n\n![](img/chunk_image.jpg){fig-align=\"right\" width=\"300\" height=\"300\"}\n\n\n::: notes\nNotes\n:::\n\n## Métodos de División en Chunks\n\n- **Por encabezados**: Se usan los encabezados y subtítulos del documento para crear chunks.\n- **Por párrafos**: Se divide el texto en chunks según los párrafos.\n- **Ventanas deslizantes**: Una ventana captura texto al desplazarse, creando chunks que se superponen.\n- **Segmentación semántica**: Se usan algoritmos para identificar unidades de significado y dividir el texto en chunks.\n\n::: notes\nNotes\n:::\n\n## ¿Cómo los LLMs Utilizan los Chunks?\n\nLos LLMs utilizan los chunks para comprender el contexto y generar texto coherente. Al procesar un chunk, el LLM puede:\n\n- **Identificar las ideas principales**: Extraer la información más relevante del chunk.\n\n- **Establecer conexiones**: Relacionar la información del chunk con otros chunks o con el conocimiento previo del LLM. Para lograr esto, los LLMs utilizan mecanismos como la atención y la codificación posicional. La atención permite al modelo enfocarse en las partes más relevantes de la información dentro de un chunk y entre diferentes chunks. La codificación posicional proporciona información sobre la ubicación de las palabras dentro de un chunk y en relación con otros chunks, lo que ayuda al modelo a comprender el orden y la secuencia de la información.\n\n- **Generar resúmenes**: Condensar la información del chunk en un resumen conciso.\n\n- **Responder preguntas**: Proporcionar respuestas precisas y relevantes a las preguntas basadas en la información del chunk.\n\n::: notes\nNotes\n:::\n\n## Desafíos de los *Chunks* en LLMs\n\n- **Pérdida de contexto**: Dividir el texto en *chunks* puede dificultar la comprensión de relaciones entre datos, afectando la precisión de las respuestas.\n\n- **Sesgo**: La selección de *chunks* puede introducir sesgos, llevando a resultados incompletos o inexactos.\n\n- **Información compleja**: Cierta información (imágenes, datos, etc.) es difícil de representar en *chunks*.\n\n- **Tamaño del *chunk***: Encontrar el tamaño óptimo es crucial para el rendimiento del modelo.\n\n::: notes\nNotes\n:::\n\n\n## **Word Embeddings**: Métodos Basados en Frecuencia\n\n- **Bag-of-Words (BoW)**: Conjunto no ordenado de palabras y sus frecuencias. Simple pero ignora el orden y la semántica.\n\n- **TF-IDF**: Asigna pesos a las palabras según su frecuencia en un documento y en el corpus. Mejora BoW pero ignora el contexto.\n\n- **N-gramas**: Considera secuencias de n palabras. Captura contexto local pero basado en frecuencia.\n\n- **Matrices de Co-ocurrencia**: Capturan la frecuencia con la que las palabras aparecen juntas. Base para generar embeddings.\n\n- **One-Hot Encoding**: Vector binario para cada palabra. Sencillo pero de alta dimensionalidad y sin relaciones semánticas.\n\n\n::: notes\nNotes\n:::\n\n## **Word Embeddings**: Embeddings Estáticos\n\nGeneran un vector único por palabra, independientemente del contexto.\n\n- **Word2Vec (CBOW y Skip-gram)**: Redes neuronales para predecir palabras a partir de su contexto.\n\n- **GloVe**: Combina métodos de conteo y predicción, capturando relaciones semánticas locales y globales.\n\n- **FastText**: Mejora Word2Vec considerando la estructura interna de las palabras.\n\n\n::: notes\nNotes\n:::\n\n## **Word Embeddings**: Embeddings Contextuales\n\nGeneran embeddings que varían según el contexto, capturando la polisemia.\n\n- **Self-Attention**: Permite a cada palabra relacionarse con todas las demás. Fundamental en Transformers.\n\n- **ELMo**: Considera todo el contexto de la oración para embeddings dinámicos.\n\n- **BERT**: Arquitectura Transformer para embeddings contextuales, capturando contexto izquierdo y derecho.\n\n\n::: notes\nNotes\n:::\n\n## **Word Embeddings**: Embeddings Contextuales\n\nGeneran embeddings que varían según el contexto, capturando la polisemia.\n\n- **Self-Attention**: Permite a cada palabra relacionarse con todas las demás. Fundamental en Transformers.\n\n- **ELMo**: Considera todo el contexto de la oración para embeddings dinámicos.\n\n- **BERT**: Arquitectura Transformer para embeddings contextuales, capturando contexto izquierdo y derecho.\n\n\n::: notes\nNotes\n:::\n\n# KNOWLEDGE GRAPH (KG)\n\n## ¿QUÉ SON LOS KG?\n\n<blockquote cite>\nLos grafos de conocimiento son bases de datos inteligentes que representan el conocimiento médico de manera estructurada, permitiendo a la IA comprender y razonar sobre la información de forma más efectiva.\n</blockquote>\n\n[![Lord of the rings](img/thering.jpg){width=300px}](https://alon-cohen-gordon.wixsite.com/lotr-graph){preview-links=\"true\"}\n\n::: notes\nNotes\n:::\n\n## Componentes principales de un grafo de conocimiento\n\n::::: columns\n::: {.column width=\"30%\" }\n ![](img/esquema_kg.jpg)\n:::\n\n::: {.column width=\"70%\" .absolute top=\"220\" left=\"340\"}\n\n\n- **Nodos**: Representan entidades, conceptos u objetos, como enfermedades, síntomas, genes o medicamentos.\n- **Aristas**: Representan las relaciones entre los nodos, como \"causa\", \"trata\", \"se asocia con\" o \"es un tipo de\".\n- **Etiquetas**: Proporcionan información adicional sobre los nodos y las aristas, como nombres, descripciones o atributos.\n\n:::\n:::::\n\n::: notes\nNotes\n:::\n\n## Beneficios de los Grafos de Conocimiento\n::::: columns\n::: {.column width=\"30%\" }\n ![](img/beneficios_KG.jpg)\n:::\n\n::: {.column width=\"70%\" .absolute top=\"230\" left=\"340\"}\n\n\n- **Precisión en el diagnóstico**: Integración de información de diversas fuentes.\n- **Personalización de tratamientos**: Uso de información genómica, clínica y de estilo de vida.\n- **Descubrimiento de fármacos**: Identificación de nuevas relaciones entre genes, proteínas y enfermedades.\n\n:::\n:::::\n\n::: notes\nNotes\n:::\n\n## Limitaciones de los Grafos de Conocimiento\n::::: columns\n::: {.column width=\"30%\" }\n ![](img/limitations_kg.jpg)\n:::\n\n::: {.column width=\"70%\" .absolute top=\"230\" left=\"340\"}\n\n\n- **Calidad de los datos**: La precisión y completitud son cruciales.\n- **Escalabilidad**: Construcción y mantenimiento a gran escala pueden ser complejos.\n- **Interoperabilidad**: Integración con otros sistemas y bases de datos puede ser un desafío.\n\n:::\n:::::\n\n::: notes\nNotes\n:::\n\n\n# FINE-TUNNING y RETRIEVAL AUGMENTATION GENERATION (RAG)\n\n## FINE-TUNNING\n\n<blockquote cite>\n\nEl ajuste fino de un LLM es una técnica de aprendizaje por transferencia en la que se toma un modelo pre-entrenado con un gran conjunto de datos para una tarea general, y se realizan pequeños ajustes a sus parámetros internos para optimizar su rendimiento en una nueva tarea específica\n\n</blockquote>\n\n![](img/fine_tunning_presentacion.jpg){fig-align=\"right\" width=\"350\" height=\"350\"}\n\n::: notes\nNotes\n:::\n\n## TRANSFER LEARNING\n\n![](img/transferlearning.png)\n\n::: notes\nNotes\n:::\n\n## FINE-TUNNING (ESQUEMA)\n\n![](img/esquemafinetunning.png)\n\n::: notes\nNotes\n:::\n\n## FINE-TUNNING (ESQUEMA)\n\n![](img/esquemafinetunning.png)\n\n::: notes\nNotes\n:::\n\n## FINE-TUNING (DEMOSTRACIÓN)\n\n::::: columns\n::: {.column width=\"30%\"}\n ![](img/qr_demo_bio.png){.absolute top=\"200\" left=\"0\" width=\"300\" height=\"300\"}\n:::\n\n::: {.column width=\"70%\" .absolute top=\"240\" left=\"340\"}\n\n\n- BioMistral/BioMistral-7B\n- mistralai/Mistral-7B-Instruct-v0.1\n\n[Ejemplo](https://aotal.github.io/CursoAI2024/)\n:::\n:::::\n\n![](img/huggingface_logo.png){.absolute bottom=\"100\" right=\"50\" width=\"300\"}\n\n::: notes\nNotes\n:::\n\n## RAG (2020)\n\n- **Vectorización del contenido**: Cada fragmento de texto se convierte en un vector numérico utilizando un modelo de embeddings. Estos vectores representan el significado semántico del texto en un espacio multidimensional.\n\n- **Búsqueda de similitud**: Cuando se realiza una consulta, esta también se vectoriza, y se busca en la base de datos vectorial los fragmentos cuyos vectores sean más cercanos al de la consulta (similaridad coseno, distancia euclidiana, etc.).\n\n::: notes\nNotes\n:::\n\n## ESQUEMA RAG\n\n![](img/esquemarag.png)\n\n::: notes\nNotes\n:::\n\n## FINE-TUNNING VS RAG\n\n\nhttps://notebooklm.google.com/notebook/d40fef92-ed62-4659-9b3e-0e5cde949f45\nhttps://notebooklm.google.com/notebook/35c43bd2-d012-45fb-a514-aa2958b1fdda\n\n\n<div style=\"display: flex; justify-content: center; align-items: center; height: 75%; margin-top: -60px\" >\n  <table style=\"transform: scale(0.75);\">\n    <tr>\n      <th>Característica</th>\n      <th>Fine-tuning</th>\n      <th>RAG</th>\n    </tr>\n    <tr>\n      <td>Enfoque principal</td>\n      <td>Adaptación del modelo</td>\n      <td>Aumento de la información</td>\n    </tr>\n    <tr>\n      <td>Método</td>\n      <td>Ajuste de parámetros</td>\n      <td>Recuperación de información externa</td>\n    </tr>\n    <tr>\n      <td>Ventajas</td>\n      <td>Personalización del modelo, eficiencia</td>\n      <td>Respuestas contextualmente relevantes, precisión</td>\n    </tr>\n    <tr>\n      <td>Limitaciones</td>\n      <td>Dificultad con datos cambiantes, adaptación de estilo limitada</td>\n      <td>Adaptación de estilo limitada, enfoque en recuperación</td>\n    </tr>\n  </table>\n</div>\n\n::: notes\nNotes\n:::\n\n## FINE-TUNNING VS RAG\n\n::: {layout=\"[15,-5,12]\"}\n![Finne-tunning](img/medico_fine_tunning.jpg)\n\n![RAG](img/lector_rag.jpg)\n:::\n\n::: notes\nNotes\n:::\n\n## Ejemplo RAG\n\n**Pregunta**:Basándote en la historia clínica proporcionada, ¿cuáles son los tres factores de riesgo cardiovascular más relevantes para la paciente María Pérez y por qué son importantes en este caso específico?\n\n[Historia mal estructurada](https://notebooklm.google.com/notebook/d40fef92-ed62-4659-9b3e-0e5cde949f45)\n\n[Historia bien estructurada](https://notebooklm.google.com/notebook/35c43bd2-d012-45fb-a514-aa2958b1fdda)\n\n![](img/notebooklm_logo.png)\n\n# Límites de los LLM\n\n::: notes\nNotes\n:::\n\n## Límites de los LLM (Clásicos)\n\n- **Falta de comprensión del mundo real**: Un LLM podría tener dificultades para responder preguntas sobre eventos actuales, lugares geográficos o conceptos abstractos.\n\n- **Sesgo**: Los LLMs pueden perpetuar y amplificar los sesgos presentes en los datos de entrenamiento. Esto puede resultar en respuestas discriminatorias, ofensivas o estereotipadas. Es importante abordar el sesgo en los LLMs para garantizar la equidad y la inclusión en las aplicaciones de IA.\n\n**Incapacidad para razonar**: Un LLM podría tener dificultades para resolver un problema matemático o para comprender las relaciones causales entre diferentes eventos.\n\n**Dificultad para manejar información nueva o inesperada**: Los LLMs se basan en los datos con los que fueron entrenados y pueden tener dificultades para procesar información nueva o inesperada. Esto puede llevar a respuestas inexactas o irrelevantes cuando se enfrentan a situaciones novedosas.\n\n**Dificultad con el lenguaje complejo**: Los LLMs pueden tener dificultades para comprender y procesar el lenguaje complejo o matizado, como el sarcasmo, las metáforas y otras figuras retóricas..\n\n**Limitaciones en la creatividad**: Los LLMs pueden generar diferentes tipos de contenido creativo de formato textual, como poemas, guiones, piezas musicales y correos electrónicos. Sin embargo, su capacidad para producir contenido verdaderamente original y creativo es limitada.\n\n\n## NOtebook LM\n\n[Enlace](https://notebooklm.google.com/notebook/40a3cbbf-0d09-4cec-a129-3efd69592ee8){preview-links=\"true\"}\n\n",
    "supporting": [
      "LLMs_files"
    ],
    "filters": [],
    "includes": {}
  }
}