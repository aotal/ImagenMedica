[
  {
    "objectID": "tmp/limpieza.html",
    "href": "tmp/limpieza.html",
    "title": "Instrucciones para incrustar un chat de gemini",
    "section": "",
    "text": "Primeros pasos\n\n\n\n\nCompartir el chat\n\n\n\n\n\nCopiar ruta\n\n\nPegarlo en la barra de un navegador\n\n\n\nGuardar página web como **Página web (Completa)\n\n\nAbrir el html guardado (se guarda una carpeta con el mismo nombre). En vscode al dar al botón derecho sale una opción que se llama Dar formato al documento\nBuscar la cadena\nBuscar la cadena\n\nen el fichero html. Si pulsamos en el editor el\n\nde partida nos remarcará el\n\nde terminar y viceversa. Borramos ese\n\n.\n\n\n\n_ngcontent-ng-c1920989603=“” politica de privacidad _ngcontent-ng-c2593308738=“” ventana de reanudar el chat"
  },
  {
    "objectID": "pre-post/Presentaciones/ImagenAI.html#índice",
    "href": "pre-post/Presentaciones/ImagenAI.html#índice",
    "title": "Inteligencia artificial en imagen médica",
    "section": "Índice",
    "text": "Índice\n\nDiagnóstico asistido por IA\nSegmentación de órganos y tejidos\nGeneración de imágenes sintéticas\nMejora de la calidad de la imagen"
  },
  {
    "objectID": "pre-post/Presentaciones/ImagenAI.html#casos-de-uso",
    "href": "pre-post/Presentaciones/ImagenAI.html#casos-de-uso",
    "title": "Inteligencia artificial en imagen médica",
    "section": "Casos de uso",
    "text": "Casos de uso\n\nDetección y clasificación de anomalías\nDetección temprana (screening)\nReducción de errores"
  },
  {
    "objectID": "pre-post/Presentaciones/ImagenAI.html#knowledge-map",
    "href": "pre-post/Presentaciones/ImagenAI.html#knowledge-map",
    "title": "Inteligencia artificial en imagen médica",
    "section": "Knowledge map",
    "text": "Knowledge map\n\n\n\nNotes"
  },
  {
    "objectID": "pre-post/Presentaciones/ImagenAI.html#convolutional-neural-network",
    "href": "pre-post/Presentaciones/ImagenAI.html#convolutional-neural-network",
    "title": "Inteligencia artificial en imagen médica",
    "section": "Convolutional Neural Network",
    "text": "Convolutional Neural Network"
  },
  {
    "objectID": "pre-post/Presentaciones/ImagenAI.html#vision-transformer",
    "href": "pre-post/Presentaciones/ImagenAI.html#vision-transformer",
    "title": "Inteligencia artificial en imagen médica",
    "section": "Vision Transformer",
    "text": "Vision Transformer\n\n\n\n\n\n\n(Dosovitskiy et al. 2020)"
  },
  {
    "objectID": "pre-post/Presentaciones/ImagenAI.html#knowledge-map-1",
    "href": "pre-post/Presentaciones/ImagenAI.html#knowledge-map-1",
    "title": "Inteligencia artificial en imagen médica",
    "section": "Knowledge map",
    "text": "Knowledge map"
  },
  {
    "objectID": "pre-post/Presentaciones/ImagenAI.html#u-net",
    "href": "pre-post/Presentaciones/ImagenAI.html#u-net",
    "title": "Inteligencia artificial en imagen médica",
    "section": "U-Net",
    "text": "U-Net\n\n(Ronneberger, Fischer, y Brox 2015)"
  },
  {
    "objectID": "pre-post/Presentaciones/ImagenAI.html#u-net-1",
    "href": "pre-post/Presentaciones/ImagenAI.html#u-net-1",
    "title": "Inteligencia artificial en imagen médica",
    "section": "U-Net",
    "text": "U-Net"
  },
  {
    "objectID": "pre-post/Presentaciones/ImagenAI.html#proyecto-monai",
    "href": "pre-post/Presentaciones/ImagenAI.html#proyecto-monai",
    "title": "Inteligencia artificial en imagen médica",
    "section": "Proyecto MONAI",
    "text": "Proyecto MONAI"
  },
  {
    "objectID": "pre-post/Presentaciones/ImagenAI.html#knowledge-map-2",
    "href": "pre-post/Presentaciones/ImagenAI.html#knowledge-map-2",
    "title": "Inteligencia artificial en imagen médica",
    "section": "Knowledge map",
    "text": "Knowledge map"
  },
  {
    "objectID": "pre-post/Presentaciones/ImagenAI.html#ejemplo-creación-de-cts-sintéticos",
    "href": "pre-post/Presentaciones/ImagenAI.html#ejemplo-creación-de-cts-sintéticos",
    "title": "Inteligencia artificial en imagen médica",
    "section": "Ejemplo: Creación de CTs sintéticos",
    "text": "Ejemplo: Creación de CTs sintéticos"
  },
  {
    "objectID": "pre-post/Presentaciones/ImagenAI.html#knowledge-map-3",
    "href": "pre-post/Presentaciones/ImagenAI.html#knowledge-map-3",
    "title": "Inteligencia artificial en imagen médica",
    "section": "Knowledge map",
    "text": "Knowledge map\n\n\n\n\n\n\n\n\n\n\n \n\n\n \n\n\n \n\n\n\n\nDosovitskiy, Alexey, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, et al. 2020. «An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale». https://doi.org/10.48550/ARXIV.2010.11929.\n\n\nRonneberger, Olaf, Philipp Fischer, y Thomas Brox. 2015. «U-Net: Convolutional Networks for Biomedical Image Segmentation». En, 234-41. Springer International Publishing. https://doi.org/10.1007/978-3-319-24574-4_28."
  },
  {
    "objectID": "pre-post/code/PruebaMonai.html",
    "href": "pre-post/code/PruebaMonai.html",
    "title": "Servidor Monai",
    "section": "",
    "text": "Colab\n\n\nfrom google.colab import userdata\n\n\n!pip install pyngrok\n\nCollecting pyngrok\n  Downloading pyngrok-7.1.2-py3-none-any.whl (22 kB)\nRequirement already satisfied: PyYAML&gt;=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.1)\nInstalling collected packages: pyngrok\nSuccessfully installed pyngrok-7.1.2\n\n\n\nfrom pyngrok import ngrok\nngrok.set_auth_token(userdata.get('ngrok'))\nport = \"8000\"\n# Open a ngrok tunnel to the HTTP server\npublic_url = ngrok.connect(port).public_url\npublic_url\n\n\n\n\n'https://50d7-34-126-84-49.ngrok-free.app'\n\n\n\n# install MONAI Label\n!pip install monailabel\n\nCollecting monailabel\n  Downloading monailabel-0.8.1-202311030103-py3-none-any.whl (11.9 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.9/11.9 MB 8.6 MB/s eta 0:00:00\nCollecting bcrypt==4.0.1 (from monailabel)\n  Downloading bcrypt-4.0.1-cp36-abi3-manylinux_2_28_x86_64.whl (593 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 593.7/593.7 kB 46.7 MB/s eta 0:00:00\nCollecting cachetools==5.3.0 (from monailabel)\n  Downloading cachetools-5.3.0-py3-none-any.whl (9.3 kB)\nCollecting dicomweb-client==0.59.1 (from monailabel)\n  Downloading dicomweb_client-0.59.1-py3-none-any.whl (60 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 60.9/60.9 kB 8.7 MB/s eta 0:00:00\nCollecting einops&gt;=0.6.0 (from monailabel)\n  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 44.6/44.6 kB 5.8 MB/s eta 0:00:00\nCollecting expiring-dict==1.1.0 (from monailabel)\n  Downloading expiring_dict-1.1.0-py3-none-any.whl (3.6 kB)\nCollecting expiringdict==1.2.2 (from monailabel)\n  Downloading expiringdict-1.2.2-py3-none-any.whl (8.5 kB)\nCollecting fastapi==0.95.0 (from monailabel)\n  Downloading fastapi-0.95.0-py3-none-any.whl (57 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.1/57.1 kB 6.6 MB/s eta 0:00:00\nCollecting filelock==3.11.0 (from monailabel)\n  Downloading filelock-3.11.0-py3-none-any.whl (10.0 kB)\nCollecting girder-client==3.1.17 (from monailabel)\n  Downloading girder-client-3.1.17.tar.gz (20 kB)\n  Preparing metadata (setup.py) ... done\nCollecting httpx==0.23.3 (from monailabel)\n  Downloading httpx-0.23.3-py3-none-any.whl (71 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 71.5/71.5 kB 10.2 MB/s eta 0:00:00\nCollecting monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0 (from monailabel)\n  Downloading monai-1.3.0-202310121228-py3-none-any.whl (1.3 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 76.8 MB/s eta 0:00:00\nCollecting ninja==1.11.1 (from monailabel)\n  Downloading ninja-1.11.1-py2.py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (145 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 146.0/146.0 kB 20.0 MB/s eta 0:00:00\nCollecting numpymaxflow==0.0.6 (from monailabel)\n  Downloading numpymaxflow-0.0.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (51 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 51.6/51.6 kB 7.1 MB/s eta 0:00:00\nCollecting opencv-python-headless==4.7.0.72 (from monailabel)\n  Downloading opencv_python_headless-4.7.0.72-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.2 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 49.2/49.2 MB 19.1 MB/s eta 0:00:00\nCollecting passlib==1.7.4 (from monailabel)\n  Downloading passlib-1.7.4-py2.py3-none-any.whl (525 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 525.6/525.6 kB 57.5 MB/s eta 0:00:00\nRequirement already satisfied: pydantic&gt;=1.10.7 in /usr/local/lib/python3.10/dist-packages (from monailabel) (2.6.1)\nCollecting pydicom-seg==0.4.1 (from monailabel)\n  Downloading pydicom_seg-0.4.1-py3-none-any.whl (27 kB)\nCollecting pydicom==2.3.1 (from monailabel)\n  Downloading pydicom-2.3.1-py3-none-any.whl (2.0 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.0/2.0 MB 89.8 MB/s eta 0:00:00\nCollecting pynetdicom==2.0.2 (from monailabel)\n  Downloading pynetdicom-2.0.2-py3-none-any.whl (1.6 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.6/1.6 MB 86.1 MB/s eta 0:00:00\nCollecting pynrrd==1.0.0 (from monailabel)\n  Downloading pynrrd-1.0.0-py2.py3-none-any.whl (19 kB)\nCollecting python-dotenv==1.0.0 (from monailabel)\n  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\nCollecting python-jose[cryptography]==3.3.0 (from monailabel)\n  Downloading python_jose-3.3.0-py2.py3-none-any.whl (33 kB)\nCollecting python-multipart==0.0.6 (from monailabel)\n  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 45.7/45.7 kB 7.5 MB/s eta 0:00:00\nCollecting pyyaml==6.0 (from monailabel)\n  Downloading PyYAML-6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (682 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 682.2/682.2 kB 57.0 MB/s eta 0:00:00\nCollecting requests-toolbelt==0.10.1 (from monailabel)\n  Downloading requests_toolbelt-0.10.1-py2.py3-none-any.whl (54 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 54.5/54.5 kB 8.4 MB/s eta 0:00:00\nCollecting requests==2.28.2 (from monailabel)\n  Downloading requests-2.28.2-py3-none-any.whl (62 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.8/62.8 kB 8.3 MB/s eta 0:00:00\nCollecting schedule==1.1.0 (from monailabel)\n  Downloading schedule-1.1.0-py2.py3-none-any.whl (10 kB)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from monailabel) (1.2.2)\nRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from monailabel) (1.11.4)\nCollecting shapely==2.0.1 (from monailabel)\n  Downloading shapely-2.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.3/2.3 MB 96.7 MB/s eta 0:00:00\nCollecting timeloop==1.0.2 (from monailabel)\n  Downloading timeloop-1.0.2.tar.gz (2.9 kB)\n  Preparing metadata (setup.py) ... done\nCollecting uvicorn==0.21.1 (from monailabel)\n  Downloading uvicorn-0.21.1-py3-none-any.whl (57 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.8/57.8 kB 8.3 MB/s eta 0:00:00\nCollecting watchdog==3.0.0 (from monailabel)\n  Downloading watchdog-3.0.0-py3-none-manylinux2014_x86_64.whl (82 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 82.1/82.1 kB 11.5 MB/s eta 0:00:00\nRequirement already satisfied: numpy&gt;=1.19 in /usr/local/lib/python3.10/dist-packages (from dicomweb-client==0.59.1-&gt;monailabel) (1.25.2)\nCollecting retrying&gt;=1.3.3 (from dicomweb-client==0.59.1-&gt;monailabel)\n  Downloading retrying-1.3.4-py3-none-any.whl (11 kB)\nRequirement already satisfied: Pillow&gt;=8.3 in /usr/local/lib/python3.10/dist-packages (from dicomweb-client==0.59.1-&gt;monailabel) (9.4.0)\nRequirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from expiring-dict==1.1.0-&gt;monailabel) (2.4.0)\nCollecting pydantic&gt;=1.10.7 (from monailabel)\n  Downloading pydantic-1.10.14-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.1/3.1 MB 97.8 MB/s eta 0:00:00\nCollecting starlette&lt;0.27.0,&gt;=0.26.1 (from fastapi==0.95.0-&gt;monailabel)\n  Downloading starlette-0.26.1-py3-none-any.whl (66 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 66.9/66.9 kB 9.5 MB/s eta 0:00:00\nRequirement already satisfied: click&gt;=6.7 in /usr/local/lib/python3.10/dist-packages (from girder-client==3.1.17-&gt;monailabel) (8.1.7)\nCollecting diskcache (from girder-client==3.1.17-&gt;monailabel)\n  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 45.5/45.5 kB 6.4 MB/s eta 0:00:00\nRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx==0.23.3-&gt;monailabel) (2024.2.2)\nCollecting httpcore&lt;0.17.0,&gt;=0.15.0 (from httpx==0.23.3-&gt;monailabel)\n  Downloading httpcore-0.16.3-py3-none-any.whl (69 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 69.6/69.6 kB 10.2 MB/s eta 0:00:00\nCollecting rfc3986[idna2008]&lt;2,&gt;=1.3 (from httpx==0.23.3-&gt;monailabel)\n  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\nRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx==0.23.3-&gt;monailabel) (1.3.0)\nCollecting SimpleITK&gt;1.2.4 (from pydicom-seg==0.4.1-&gt;monailabel)\n  Downloading SimpleITK-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (52.7 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 52.7/52.7 MB 11.0 MB/s eta 0:00:00\nCollecting jsonschema&lt;4.0.0,&gt;=3.2.0 (from pydicom-seg==0.4.1-&gt;monailabel)\n  Downloading jsonschema-3.2.0-py2.py3-none-any.whl (56 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.3/56.3 kB 8.1 MB/s eta 0:00:00\nCollecting nptyping (from pynrrd==1.0.0-&gt;monailabel)\n  Downloading nptyping-2.5.0-py3-none-any.whl (37 kB)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from pynrrd==1.0.0-&gt;monailabel) (4.9.0)\nCollecting ecdsa!=0.15 (from python-jose[cryptography]==3.3.0-&gt;monailabel)\n  Downloading ecdsa-0.18.0-py2.py3-none-any.whl (142 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 142.9/142.9 kB 16.7 MB/s eta 0:00:00\nRequirement already satisfied: rsa in /usr/local/lib/python3.10/dist-packages (from python-jose[cryptography]==3.3.0-&gt;monailabel) (4.9)\nRequirement already satisfied: pyasn1 in /usr/local/lib/python3.10/dist-packages (from python-jose[cryptography]==3.3.0-&gt;monailabel) (0.5.1)\nRequirement already satisfied: cryptography&gt;=3.4.0 in /usr/local/lib/python3.10/dist-packages (from python-jose[cryptography]==3.3.0-&gt;monailabel) (42.0.3)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /usr/local/lib/python3.10/dist-packages (from requests==2.28.2-&gt;monailabel) (3.3.2)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.10/dist-packages (from requests==2.28.2-&gt;monailabel) (3.6)\nCollecting urllib3&lt;1.27,&gt;=1.21.1 (from requests==2.28.2-&gt;monailabel)\n  Downloading urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 143.8/143.8 kB 19.0 MB/s eta 0:00:00\nCollecting h11&gt;=0.8 (from uvicorn==0.21.1-&gt;monailabel)\n  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.3/58.3 kB 8.5 MB/s eta 0:00:00\nRequirement already satisfied: torch&gt;=1.9 in /usr/local/lib/python3.10/dist-packages (from monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (2.1.0+cu121)\nCollecting mlflow (from monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel)\n  Downloading mlflow-2.10.2-py3-none-any.whl (19.5 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.5/19.5 MB 45.7 MB/s eta 0:00:00\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (5.9.5)\nCollecting itk&gt;=5.2 (from monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel)\n  Downloading itk-5.3.0-cp310-cp310-manylinux_2_28_x86_64.whl (8.3 kB)\nCollecting pytorch-ignite==0.4.11 (from monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel)\n  Downloading pytorch_ignite-0.4.11-py3-none-any.whl (266 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 266.5/266.5 kB 31.6 MB/s eta 0:00:00\nRequirement already satisfied: scikit-image&gt;=0.14.2 in /usr/local/lib/python3.10/dist-packages (from monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (0.19.3)\nRequirement already satisfied: gdown&gt;=4.4.0 in /usr/local/lib/python3.10/dist-packages (from monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (4.7.3)\nCollecting openslide-python==1.1.2 (from monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel)\n  Downloading openslide-python-1.1.2.tar.gz (316 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 316.6/316.6 kB 36.7 MB/s eta 0:00:00\n  Preparing metadata (setup.py) ... done\nRequirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (2.15.2)\nCollecting lmdb (from monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel)\n  Downloading lmdb-1.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (299 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 299.2/299.2 kB 34.5 MB/s eta 0:00:00\nRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (0.16.0+cu121)\nRequirement already satisfied: tqdm&gt;=4.47.0 in /usr/local/lib/python3.10/dist-packages (from monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (4.66.2)\nRequirement already satisfied: nibabel in /usr/local/lib/python3.10/dist-packages (from monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (4.0.2)\nCollecting fire (from monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel)\n  Downloading fire-0.5.0.tar.gz (88 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 88.3/88.3 kB 11.8 MB/s eta 0:00:00\n  Preparing metadata (setup.py) ... done\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pytorch-ignite==0.4.11-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (23.2)\nRequirement already satisfied: joblib&gt;=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn-&gt;monailabel) (1.3.2)\nRequirement already satisfied: threadpoolctl&gt;=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn-&gt;monailabel) (3.3.0)\nRequirement already satisfied: cffi&gt;=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography&gt;=3.4.0-&gt;python-jose[cryptography]==3.3.0-&gt;monailabel) (1.16.0)\nRequirement already satisfied: six&gt;=1.9.0 in /usr/local/lib/python3.10/dist-packages (from ecdsa!=0.15-&gt;python-jose[cryptography]==3.3.0-&gt;monailabel) (1.16.0)\nRequirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown&gt;=4.4.0-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (2.31.0)\nRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown&gt;=4.4.0-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (4.12.3)\nRequirement already satisfied: anyio&lt;5.0,&gt;=3.0 in /usr/local/lib/python3.10/dist-packages (from httpcore&lt;0.17.0,&gt;=0.15.0-&gt;httpx==0.23.3-&gt;monailabel) (3.7.1)\nCollecting itk-core==5.3.0 (from itk&gt;=5.2-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel)\n  Downloading itk_core-5.3.0-cp310-cp310-manylinux_2_28_x86_64.whl (81.2 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 81.2/81.2 MB 9.4 MB/s eta 0:00:00\nCollecting itk-numerics==5.3.0 (from itk&gt;=5.2-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel)\n  Downloading itk_numerics-5.3.0-cp310-cp310-manylinux_2_28_x86_64.whl (58.8 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.8/58.8 MB 11.1 MB/s eta 0:00:00\nCollecting itk-io==5.3.0 (from itk&gt;=5.2-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel)\n  Downloading itk_io-5.3.0-cp310-cp310-manylinux_2_28_x86_64.whl (25.6 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 25.6/25.6 MB 59.7 MB/s eta 0:00:00\nCollecting itk-filtering==5.3.0 (from itk&gt;=5.2-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel)\n  Downloading itk_filtering-5.3.0-cp310-cp310-manylinux_2_28_x86_64.whl (73.5 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 73.5/73.5 MB 9.0 MB/s eta 0:00:00\nCollecting itk-registration==5.3.0 (from itk&gt;=5.2-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel)\n  Downloading itk_registration-5.3.0-cp310-cp310-manylinux_2_28_x86_64.whl (26.6 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 26.6/26.6 MB 42.1 MB/s eta 0:00:00\nCollecting itk-segmentation==5.3.0 (from itk&gt;=5.2-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel)\n  Downloading itk_segmentation-5.3.0-cp310-cp310-manylinux_2_28_x86_64.whl (16.5 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.5/16.5 MB 52.8 MB/s eta 0:00:00\nRequirement already satisfied: attrs&gt;=17.4.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema&lt;4.0.0,&gt;=3.2.0-&gt;pydicom-seg==0.4.1-&gt;monailabel) (23.2.0)\nCollecting pyrsistent&gt;=0.14.0 (from jsonschema&lt;4.0.0,&gt;=3.2.0-&gt;pydicom-seg==0.4.1-&gt;monailabel)\n  Downloading pyrsistent-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (117 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 117.7/117.7 kB 17.9 MB/s eta 0:00:00\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from jsonschema&lt;4.0.0,&gt;=3.2.0-&gt;pydicom-seg==0.4.1-&gt;monailabel) (67.7.2)\nRequirement already satisfied: networkx&gt;=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image&gt;=0.14.2-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (3.2.1)\nRequirement already satisfied: imageio&gt;=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image&gt;=0.14.2-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (2.31.6)\nRequirement already satisfied: tifffile&gt;=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image&gt;=0.14.2-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (2024.2.12)\nRequirement already satisfied: PyWavelets&gt;=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image&gt;=0.14.2-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (1.5.0)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch&gt;=1.9-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (1.12)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch&gt;=1.9-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (3.1.3)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch&gt;=1.9-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (2023.6.0)\nRequirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch&gt;=1.9-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (2.1.0)\nRequirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (2.4.0)\nRequirement already satisfied: cloudpickle&lt;4 in /usr/local/lib/python3.10/dist-packages (from mlflow-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (2.2.1)\nRequirement already satisfied: entrypoints&lt;1 in /usr/local/lib/python3.10/dist-packages (from mlflow-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (0.4)\nCollecting gitpython&lt;4,&gt;=2.1.0 (from mlflow-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel)\n  Downloading GitPython-3.1.42-py3-none-any.whl (195 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 195.4/195.4 kB 24.2 MB/s eta 0:00:00\nRequirement already satisfied: protobuf&lt;5,&gt;=3.12.0 in /usr/local/lib/python3.10/dist-packages (from mlflow-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (3.20.3)\nRequirement already satisfied: pytz&lt;2024 in /usr/local/lib/python3.10/dist-packages (from mlflow-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (2023.4)\nRequirement already satisfied: importlib-metadata!=4.7.0,&lt;8,&gt;=3.7.0 in /usr/local/lib/python3.10/dist-packages (from mlflow-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (7.0.1)\nRequirement already satisfied: sqlparse&lt;1,&gt;=0.4.0 in /usr/local/lib/python3.10/dist-packages (from mlflow-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (0.4.4)\nCollecting alembic!=1.10.0,&lt;2 (from mlflow-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel)\n  Downloading alembic-1.13.1-py3-none-any.whl (233 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 233.4/233.4 kB 31.5 MB/s eta 0:00:00\nCollecting docker&lt;8,&gt;=4.0.0 (from mlflow-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel)\n  Downloading docker-7.0.0-py3-none-any.whl (147 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 147.6/147.6 kB 21.2 MB/s eta 0:00:00\nRequirement already satisfied: Flask&lt;4 in /usr/local/lib/python3.10/dist-packages (from mlflow-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (2.2.5)\nRequirement already satisfied: pandas&lt;3 in /usr/local/lib/python3.10/dist-packages (from mlflow-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (1.5.3)\nCollecting querystring-parser&lt;2 (from mlflow-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel)\n  Downloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\nRequirement already satisfied: sqlalchemy&lt;3,&gt;=1.4.0 in /usr/local/lib/python3.10/dist-packages (from mlflow-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (2.0.27)\nRequirement already satisfied: pyarrow&lt;16,&gt;=4.0.0 in /usr/local/lib/python3.10/dist-packages (from mlflow-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (14.0.2)\nRequirement already satisfied: markdown&lt;4,&gt;=3.3 in /usr/local/lib/python3.10/dist-packages (from mlflow-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (3.5.2)\nRequirement already satisfied: matplotlib&lt;4 in /usr/local/lib/python3.10/dist-packages (from mlflow-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (3.7.1)\nCollecting gunicorn&lt;22 (from mlflow-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel)\n  Downloading gunicorn-21.2.0-py3-none-any.whl (80 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 80.2/80.2 kB 11.8 MB/s eta 0:00:00\nRequirement already satisfied: absl-py&gt;=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (1.4.0)\nRequirement already satisfied: grpcio&gt;=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (1.60.1)\nRequirement already satisfied: google-auth&lt;3,&gt;=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (2.27.0)\nRequirement already satisfied: google-auth-oauthlib&lt;2,&gt;=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (1.2.0)\nRequirement already satisfied: tensorboard-data-server&lt;0.8.0,&gt;=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (0.7.2)\nRequirement already satisfied: werkzeug&gt;=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (3.0.1)\nCollecting Mako (from alembic!=1.10.0,&lt;2-&gt;mlflow-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel)\n  Downloading Mako-1.3.2-py3-none-any.whl (78 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.7/78.7 kB 11.2 MB/s eta 0:00:00\nRequirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio&lt;5.0,&gt;=3.0-&gt;httpcore&lt;0.17.0,&gt;=0.15.0-&gt;httpx==0.23.3-&gt;monailabel) (1.2.0)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi&gt;=1.12-&gt;cryptography&gt;=3.4.0-&gt;python-jose[cryptography]==3.3.0-&gt;monailabel) (2.21)\nRequirement already satisfied: itsdangerous&gt;=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask&lt;4-&gt;mlflow-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (2.1.2)\nCollecting gitdb&lt;5,&gt;=4.0.1 (from gitpython&lt;4,&gt;=2.1.0-&gt;mlflow-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel)\n  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.7/62.7 kB 8.9 MB/s eta 0:00:00\nRequirement already satisfied: pyasn1-modules&gt;=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (0.3.0)\nRequirement already satisfied: requests-oauthlib&gt;=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib&lt;2,&gt;=0.5-&gt;tensorboard-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (1.3.1)\nRequirement already satisfied: zipp&gt;=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata!=4.7.0,&lt;8,&gt;=3.7.0-&gt;mlflow-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (3.17.0)\nRequirement already satisfied: MarkupSafe&gt;=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2-&gt;torch&gt;=1.9-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (2.1.5)\nRequirement already satisfied: contourpy&gt;=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib&lt;4-&gt;mlflow-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (1.2.0)\nRequirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib&lt;4-&gt;mlflow-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (0.12.1)\nRequirement already satisfied: fonttools&gt;=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib&lt;4-&gt;mlflow-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (4.49.0)\nRequirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib&lt;4-&gt;mlflow-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (1.4.5)\nRequirement already satisfied: pyparsing&gt;=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib&lt;4-&gt;mlflow-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (3.1.1)\nRequirement already satisfied: python-dateutil&gt;=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib&lt;4-&gt;mlflow-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (2.8.2)\nRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy&lt;3,&gt;=1.4.0-&gt;mlflow-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (3.0.3)\nRequirement already satisfied: soupsieve&gt;1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4-&gt;gdown&gt;=4.4.0-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (2.5)\nINFO: pip is looking at multiple versions of requests[socks] to determine which version is compatible with other requirements. This could take a while.\nCollecting requests[socks] (from gdown&gt;=4.4.0-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel)\n  Downloading requests-2.30.0-py3-none-any.whl (62 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.5/62.5 kB 8.5 MB/s eta 0:00:00\n  Downloading requests-2.29.0-py3-none-any.whl (62 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.5/62.5 kB 9.2 MB/s eta 0:00:00\nRequirement already satisfied: PySocks!=1.5.7,&gt;=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests==2.28.2-&gt;monailabel) (1.7.1)\nRequirement already satisfied: mpmath&gt;=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy-&gt;torch&gt;=1.9-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (1.3.0)\nCollecting smmap&lt;6,&gt;=3.0.1 (from gitdb&lt;5,&gt;=4.0.1-&gt;gitpython&lt;4,&gt;=2.1.0-&gt;mlflow-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel)\n  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\nRequirement already satisfied: oauthlib&gt;=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib&gt;=0.7.0-&gt;google-auth-oauthlib&lt;2,&gt;=0.5-&gt;tensorboard-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (3.2.2)\nBuilding wheels for collected packages: girder-client, timeloop, openslide-python, fire\n  Building wheel for girder-client (setup.py) ... done\n  Created wheel for girder-client: filename=girder_client-3.1.17-py3-none-any.whl size=21171 sha256=062009bed2af6eedc07541fde059c47b66d0b635608c8140ae1f30c9435d11c0\n  Stored in directory: /root/.cache/pip/wheels/1c/88/79/eb68de788a8c7a6cf3d101905b31a05dc69aa9ac212f5db922\n  Building wheel for timeloop (setup.py) ... done\n  Created wheel for timeloop: filename=timeloop-1.0.2-py3-none-any.whl size=3702 sha256=67f0c94dc264d6286b729b4a635e7a8613a027fd59bf2ba771d04ad29549bdb9\n  Stored in directory: /root/.cache/pip/wheels/8b/df/32/f72b9fd897c185cd70103331f70e4cb66e3df1de24bd476548\n  Building wheel for openslide-python (setup.py) ... done\n  Created wheel for openslide-python: filename=openslide_python-1.1.2-cp310-cp310-linux_x86_64.whl size=22698 sha256=cec9573e1a7f27acc170782361b3f6c726d05bfb6af42859a57b68e5ba31c6d4\n  Stored in directory: /root/.cache/pip/wheels/93/ac/a7/517e63825943228057758b7f42d0c238290fc21e2a53b1043e\n  Building wheel for fire (setup.py) ... done\n  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116934 sha256=5206dc513f9b778aad8796bdbb9353131f26c330a86036f82a534025a7de7467\n  Stored in directory: /root/.cache/pip/wheels/90/d4/f7/9404e5db0116bd4d43e5666eaa3e70ab53723e1e3ea40c9a95\nSuccessfully built girder-client timeloop openslide-python fire\nInstalling collected packages: timeloop, SimpleITK, rfc3986, passlib, ninja, lmdb, expiringdict, watchdog, urllib3, smmap, shapely, schedule, retrying, querystring-parser, pyyaml, python-multipart, python-dotenv, pyrsistent, pydicom, pydantic, openslide-python, opencv-python-headless, numpymaxflow, nptyping, Mako, itk-core, h11, gunicorn, fire, filelock, expiring-dict, einops, ecdsa, diskcache, cachetools, bcrypt, uvicorn, starlette, requests, python-jose, pynrrd, pynetdicom, jsonschema, itk-numerics, itk-io, httpcore, gitdb, alembic, requests-toolbelt, pydicom-seg, itk-filtering, httpx, gitpython, fastapi, docker, dicomweb-client, pytorch-ignite, monai, mlflow, itk-segmentation, itk-registration, girder-client, itk, monailabel\n  Attempting uninstall: urllib3\n    Found existing installation: urllib3 2.0.7\n    Uninstalling urllib3-2.0.7:\n      Successfully uninstalled urllib3-2.0.7\n  Attempting uninstall: shapely\n    Found existing installation: shapely 2.0.3\n    Uninstalling shapely-2.0.3:\n      Successfully uninstalled shapely-2.0.3\n  Attempting uninstall: pyyaml\n    Found existing installation: PyYAML 6.0.1\n    Uninstalling PyYAML-6.0.1:\n      Successfully uninstalled PyYAML-6.0.1\n  Attempting uninstall: pydantic\n    Found existing installation: pydantic 2.6.1\n    Uninstalling pydantic-2.6.1:\n      Successfully uninstalled pydantic-2.6.1\n  Attempting uninstall: opencv-python-headless\n    Found existing installation: opencv-python-headless 4.9.0.80\n    Uninstalling opencv-python-headless-4.9.0.80:\n      Successfully uninstalled opencv-python-headless-4.9.0.80\n  Attempting uninstall: filelock\n    Found existing installation: filelock 3.13.1\n    Uninstalling filelock-3.13.1:\n      Successfully uninstalled filelock-3.13.1\n  Attempting uninstall: cachetools\n    Found existing installation: cachetools 5.3.2\n    Uninstalling cachetools-5.3.2:\n      Successfully uninstalled cachetools-5.3.2\n  Attempting uninstall: requests\n    Found existing installation: requests 2.31.0\n    Uninstalling requests-2.31.0:\n      Successfully uninstalled requests-2.31.0\n  Attempting uninstall: jsonschema\n    Found existing installation: jsonschema 4.19.2\n    Uninstalling jsonschema-4.19.2:\n      Successfully uninstalled jsonschema-4.19.2\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-colab 1.0.0 requires requests==2.31.0, but you have requests 2.28.2 which is incompatible.\nyfinance 0.2.36 requires requests&gt;=2.31, but you have requests 2.28.2 which is incompatible.\nSuccessfully installed Mako-1.3.2 SimpleITK-2.3.1 alembic-1.13.1 bcrypt-4.0.1 cachetools-5.3.0 dicomweb-client-0.59.1 diskcache-5.6.3 docker-7.0.0 ecdsa-0.18.0 einops-0.7.0 expiring-dict-1.1.0 expiringdict-1.2.2 fastapi-0.95.0 filelock-3.11.0 fire-0.5.0 girder-client-3.1.17 gitdb-4.0.11 gitpython-3.1.42 gunicorn-21.2.0 h11-0.14.0 httpcore-0.16.3 httpx-0.23.3 itk-5.3.0 itk-core-5.3.0 itk-filtering-5.3.0 itk-io-5.3.0 itk-numerics-5.3.0 itk-registration-5.3.0 itk-segmentation-5.3.0 jsonschema-3.2.0 lmdb-1.4.1 mlflow-2.10.2 monai-1.3.0 monailabel-0.8.1 ninja-1.11.1 nptyping-2.5.0 numpymaxflow-0.0.6 opencv-python-headless-4.7.0.72 openslide-python-1.1.2 passlib-1.7.4 pydantic-1.10.14 pydicom-2.3.1 pydicom-seg-0.4.1 pynetdicom-2.0.2 pynrrd-1.0.0 pyrsistent-0.20.0 python-dotenv-1.0.0 python-jose-3.3.0 python-multipart-0.0.6 pytorch-ignite-0.4.11 pyyaml-6.0 querystring-parser-1.2.4 requests-2.28.2 requests-toolbelt-0.10.1 retrying-1.3.4 rfc3986-1.5.0 schedule-1.1.0 shapely-2.0.1 smmap-5.0.1 starlette-0.26.1 timeloop-1.0.2 urllib3-1.26.18 uvicorn-0.21.1 watchdog-3.0.0\n\n\nUnable to display output for mime type(s): application/vnd.colab-display-data+json\n\n\n\n# download Radiology sample app to local directory\n!monailabel apps --name radiology --download --output .\n\nUsing PYTHONPATH=/usr:/env/python\n\n2024-02-24 17:28:35.241855: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-02-24 17:28:35.241925: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-02-24 17:28:35.243427: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2024-02-24 17:28:36.519537: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\nradiology is copied at: /content/radiology\n\n\n\n# download Task 2 MSD dataset\n!monailabel datasets --download --name Task09_Spleen --output .\n\nUsing PYTHONPATH=/usr:/env/python\n\n2024-02-24 17:28:54.798816: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-02-24 17:28:54.798878: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-02-24 17:28:54.800232: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2024-02-24 17:28:55.888070: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\nTask09_Spleen.tar: 1.50GB [01:39, 16.2MB/s]                \n2024-02-24 17:30:36,412 - INFO - Downloaded: Task09_Spleen.tar\n2024-02-24 17:30:39,553 - INFO - Verified 'Task09_Spleen.tar', md5: 410d4a301da4e5b2f6f86ec3ddba524e.\n2024-02-24 17:30:39,553 - INFO - Writing into directory: /content.\nTask09_Spleen is downloaded at: ./Task09_Spleen\n\n\n\n# start the Radiology app in MONAI label server\n# and start annotating the downloaded images using deepedit model\n!monailabel start_server --app radiology --studies Task09_Spleen/imagesTr --conf models deepedit\n\nUsing PYTHONPATH=/usr:/env/python\n\n2024-02-24 17:31:04.670173: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-02-24 17:31:04.670226: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-02-24 17:31:04.671731: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2024-02-24 17:31:05.799085: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n[2024-02-24 17:31:06,936] [4401] [MainThread] [INFO] (__main__:285) - USING:: version = False\n[2024-02-24 17:31:06,936] [4401] [MainThread] [INFO] (__main__:285) - USING:: app = /content/radiology\n[2024-02-24 17:31:06,936] [4401] [MainThread] [INFO] (__main__:285) - USING:: studies = /content/Task09_Spleen/imagesTr\n[2024-02-24 17:31:06,936] [4401] [MainThread] [INFO] (__main__:285) - USING:: verbose = INFO\n[2024-02-24 17:31:06,936] [4401] [MainThread] [INFO] (__main__:285) - USING:: conf = [['models', 'deepedit']]\n[2024-02-24 17:31:06,936] [4401] [MainThread] [INFO] (__main__:285) - USING:: host = 0.0.0.0\n[2024-02-24 17:31:06,936] [4401] [MainThread] [INFO] (__main__:285) - USING:: port = 8000\n[2024-02-24 17:31:06,937] [4401] [MainThread] [INFO] (__main__:285) - USING:: uvicorn_app = monailabel.app:app\n[2024-02-24 17:31:06,937] [4401] [MainThread] [INFO] (__main__:285) - USING:: ssl_keyfile = None\n[2024-02-24 17:31:06,937] [4401] [MainThread] [INFO] (__main__:285) - USING:: ssl_certfile = None\n[2024-02-24 17:31:06,937] [4401] [MainThread] [INFO] (__main__:285) - USING:: ssl_keyfile_password = None\n[2024-02-24 17:31:06,937] [4401] [MainThread] [INFO] (__main__:285) - USING:: ssl_ca_certs = None\n[2024-02-24 17:31:06,937] [4401] [MainThread] [INFO] (__main__:285) - USING:: workers = None\n[2024-02-24 17:31:06,937] [4401] [MainThread] [INFO] (__main__:285) - USING:: limit_concurrency = None\n[2024-02-24 17:31:06,937] [4401] [MainThread] [INFO] (__main__:285) - USING:: access_log = False\n[2024-02-24 17:31:06,937] [4401] [MainThread] [INFO] (__main__:285) - USING:: root_path = /\n[2024-02-24 17:31:06,937] [4401] [MainThread] [INFO] (__main__:285) - USING:: log_level = info\n[2024-02-24 17:31:06,937] [4401] [MainThread] [INFO] (__main__:285) - USING:: log_config = None\n[2024-02-24 17:31:06,937] [4401] [MainThread] [INFO] (__main__:285) - USING:: dryrun = False\n[2024-02-24 17:31:06,937] [4401] [MainThread] [INFO] (__main__:285) - USING:: action = start_server\n[2024-02-24 17:31:06,937] [4401] [MainThread] [INFO] (__main__:296) - \nAllow Origins: ['*']\n[2024-02-24 17:31:07,714] [4401] [MainThread] [INFO] (uvicorn.error:74) - Started server process [4401]\n[2024-02-24 17:31:07,715] [4401] [MainThread] [INFO] (uvicorn.error:48) - Waiting for application startup.\n[2024-02-24 17:31:07,715] [4401] [MainThread] [INFO] (monailabel.interfaces.utils.app:37) - Initializing App from: /content/radiology; studies: /content/Task09_Spleen/imagesTr; conf: {'models': 'deepedit'}\n[2024-02-24 17:31:07,885] [4401] [MainThread] [INFO] (monailabel.utils.others.class_utils:57) - Subclass for MONAILabelApp Found: &lt;class 'main.MyApp'&gt;\n[2024-02-24 17:31:07,899] [4401] [MainThread] [INFO] (monailabel.utils.others.class_utils:57) - Subclass for TaskConfig Found: &lt;class 'lib.configs.localization_vertebra.LocalizationVertebra'&gt;\n[2024-02-24 17:31:07,902] [4401] [MainThread] [INFO] (monailabel.utils.others.class_utils:57) - Subclass for TaskConfig Found: &lt;class 'lib.configs.segmentation_spleen.SegmentationSpleen'&gt;\n[2024-02-24 17:31:07,903] [4401] [MainThread] [INFO] (monailabel.utils.others.class_utils:57) - Subclass for TaskConfig Found: &lt;class 'lib.configs.segmentation.Segmentation'&gt;\n[2024-02-24 17:31:07,904] [4401] [MainThread] [INFO] (monailabel.utils.others.class_utils:57) - Subclass for TaskConfig Found: &lt;class 'lib.configs.deepgrow_2d.Deepgrow2D'&gt;\n[2024-02-24 17:31:07,905] [4401] [MainThread] [INFO] (monailabel.utils.others.class_utils:57) - Subclass for TaskConfig Found: &lt;class 'lib.configs.segmentation_vertebra.SegmentationVertebra'&gt;\n[2024-02-24 17:31:07,907] [4401] [MainThread] [INFO] (monailabel.utils.others.class_utils:57) - Subclass for TaskConfig Found: &lt;class 'lib.configs.deepgrow_3d.Deepgrow3D'&gt;\n[2024-02-24 17:31:07,908] [4401] [MainThread] [INFO] (monailabel.utils.others.class_utils:57) - Subclass for TaskConfig Found: &lt;class 'lib.configs.deepedit.DeepEdit'&gt;\n[2024-02-24 17:31:07,910] [4401] [MainThread] [INFO] (monailabel.utils.others.class_utils:57) - Subclass for TaskConfig Found: &lt;class 'lib.configs.localization_spine.LocalizationSpine'&gt;\n[2024-02-24 17:31:07,910] [4401] [MainThread] [INFO] (main:93) - +++ Adding Model: deepedit =&gt; lib.configs.deepedit.DeepEdit\n[2024-02-24 17:31:07,910] [4401] [MainThread] [INFO] (monailabel.utils.others.generic:187) - Downloading resource: /content/radiology/model/pretrained_deepedit_dynunet.pt from https://github.com/Project-MONAI/MONAILabel/releases/download/pretrained/radiology_deepedit_dynunet_multilabel.pt\npretrained_deepedit_dynunet.pt: 118MB [00:09, 13.6MB/s]               \n2024-02-24 17:31:17,041 - INFO - Downloaded: /content/radiology/model/pretrained_deepedit_dynunet.pt\n2024-02-24 17:31:17,042 - INFO - Expected md5 is None, skip md5 check for file /content/radiology/model/pretrained_deepedit_dynunet.pt.\n[2024-02-24 17:31:19,102] [4401] [MainThread] [INFO] (lib.configs.deepedit:141) - EPISTEMIC Enabled: False; Samples: 5\n[2024-02-24 17:31:19,102] [4401] [MainThread] [INFO] (main:96) - +++ Using Models: ['deepedit']\n[2024-02-24 17:31:19,102] [4401] [MainThread] [INFO] (monailabel.interfaces.app:135) - Init Datastore for: /content/Task09_Spleen/imagesTr\n[2024-02-24 17:31:19,103] [4401] [MainThread] [INFO] (monailabel.datastore.local:130) - Auto Reload: True; Extensions: ['*.nii.gz', '*.nii', '*.nrrd', '*.jpg', '*.png', '*.tif', '*.svs', '*.xml']\n[2024-02-24 17:31:19,104] [4401] [MainThread] [INFO] (monailabel.datastore.local:594) - Adding New Image: spleen_31 =&gt; spleen_31.nii.gz\n[2024-02-24 17:31:19,105] [4401] [MainThread] [INFO] (monailabel.datastore.local:594) - Adding New Image: spleen_14 =&gt; spleen_14.nii.gz\n[2024-02-24 17:31:19,105] [4401] [MainThread] [INFO] (monailabel.datastore.local:594) - Adding New Image: spleen_18 =&gt; spleen_18.nii.gz\n[2024-02-24 17:31:19,105] [4401] [MainThread] [INFO] (monailabel.datastore.local:594) - Adding New Image: spleen_16 =&gt; spleen_16.nii.gz\n[2024-02-24 17:31:19,105] [4401] [MainThread] [INFO] (monailabel.datastore.local:594) - Adding New Image: spleen_9 =&gt; spleen_9.nii.gz\n[2024-02-24 17:31:19,105] [4401] [MainThread] [INFO] (monailabel.datastore.local:594) - Adding New Image: spleen_53 =&gt; spleen_53.nii.gz\n[2024-02-24 17:31:19,105] [4401] [MainThread] [INFO] (monailabel.datastore.local:594) - Adding New Image: spleen_44 =&gt; spleen_44.nii.gz\n[2024-02-24 17:31:19,106] [4401] [MainThread] [INFO] (monailabel.datastore.local:594) - Adding New Image: spleen_60 =&gt; spleen_60.nii.gz\n[2024-02-24 17:31:19,106] [4401] [MainThread] [INFO] (monailabel.datastore.local:594) - Adding New Image: spleen_12 =&gt; spleen_12.nii.gz\n[2024-02-24 17:31:19,106] [4401] [MainThread] [INFO] (monailabel.datastore.local:594) - Adding New Image: spleen_32 =&gt; spleen_32.nii.gz\n[2024-02-24 17:31:19,106] [4401] [MainThread] [INFO] (monailabel.datastore.local:594) - Adding New Image: spleen_20 =&gt; spleen_20.nii.gz\n[2024-02-24 17:31:19,106] [4401] [MainThread] [INFO] (monailabel.datastore.local:594) - Adding New Image: spleen_41 =&gt; spleen_41.nii.gz\n[2024-02-24 17:31:19,107] [4401] [MainThread] [INFO] (monailabel.datastore.local:594) - Adding New Image: spleen_40 =&gt; spleen_40.nii.gz\n[2024-02-24 17:31:19,107] [4401] [MainThread] [INFO] (monailabel.datastore.local:594) - Adding New Image: spleen_63 =&gt; spleen_63.nii.gz\n[2024-02-24 17:31:19,107] [4401] [MainThread] [INFO] (monailabel.datastore.local:594) - Adding New Image: spleen_61 =&gt; spleen_61.nii.gz\n[2024-02-24 17:31:19,107] [4401] [MainThread] [INFO] (monailabel.datastore.local:594) - Adding New Image: spleen_56 =&gt; spleen_56.nii.gz\n[2024-02-24 17:31:19,107] [4401] [MainThread] [INFO] (monailabel.datastore.local:594) - Adding New Image: spleen_52 =&gt; spleen_52.nii.gz\n[2024-02-24 17:31:19,107] [4401] [MainThread] [INFO] (monailabel.datastore.local:594) - Adding New Image: spleen_27 =&gt; spleen_27.nii.gz\n[2024-02-24 17:31:19,108] [4401] [MainThread] [INFO] (monailabel.datastore.local:594) - Adding New Image: spleen_45 =&gt; spleen_45.nii.gz\n[2024-02-24 17:31:19,108] [4401] [MainThread] [INFO] (monailabel.datastore.local:594) - Adding New Image: spleen_21 =&gt; spleen_21.nii.gz\n[2024-02-24 17:31:19,108] [4401] [MainThread] [INFO] (monailabel.datastore.local:594) - Adding New Image: spleen_33 =&gt; spleen_33.nii.gz\n[2024-02-24 17:31:19,108] [4401] [MainThread] [INFO] (monailabel.datastore.local:594) - Adding New Image: spleen_10 =&gt; spleen_10.nii.gz\n[2024-02-24 17:31:19,108] [4401] [MainThread] [INFO] (monailabel.datastore.local:594) - Adding New Image: spleen_62 =&gt; spleen_62.nii.gz\n[2024-02-24 17:31:19,108] [4401] [MainThread] [INFO] (monailabel.datastore.local:594) - Adding New Image: spleen_46 =&gt; spleen_46.nii.gz\n[2024-02-24 17:31:19,109] [4401] [MainThread] [INFO] (monailabel.datastore.local:594) - Adding New Image: spleen_19 =&gt; spleen_19.nii.gz\n[2024-02-24 17:31:19,109] [4401] [MainThread] [INFO] (monailabel.datastore.local:594) - Adding New Image: spleen_25 =&gt; spleen_25.nii.gz\n[2024-02-24 17:31:19,109] [4401] [MainThread] [INFO] (monailabel.datastore.local:594) - Adding New Image: spleen_2 =&gt; spleen_2.nii.gz\n[2024-02-24 17:31:19,109] [4401] [MainThread] [INFO] (monailabel.datastore.local:594) - Adding New Image: spleen_26 =&gt; spleen_26.nii.gz\n[2024-02-24 17:31:19,109] [4401] [MainThread] [INFO] (monailabel.datastore.local:594) - Adding New Image: spleen_6 =&gt; spleen_6.nii.gz\n[2024-02-24 17:31:19,109] [4401] [MainThread] [INFO] (monailabel.datastore.local:594) - Adding New Image: spleen_22 =&gt; spleen_22.nii.gz\n[2024-02-24 17:31:19,110] [4401] [MainThread] [INFO] (monailabel.datastore.local:594) - Adding New Image: spleen_49 =&gt; spleen_49.nii.gz\n[2024-02-24 17:31:19,110] [4401] [MainThread] [INFO] (monailabel.datastore.local:594) - Adding New Image: spleen_28 =&gt; spleen_28.nii.gz\n[2024-02-24 17:31:19,110] [4401] [MainThread] [INFO] (monailabel.datastore.local:594) - Adding New Image: spleen_47 =&gt; spleen_47.nii.gz\n[2024-02-24 17:31:19,110] [4401] [MainThread] [INFO] (monailabel.datastore.local:594) - Adding New Image: spleen_29 =&gt; spleen_29.nii.gz\n[2024-02-24 17:31:19,110] [4401] [MainThread] [INFO] (monailabel.datastore.local:594) - Adding New Image: spleen_8 =&gt; spleen_8.nii.gz\n[2024-02-24 17:31:19,110] [4401] [MainThread] [INFO] (monailabel.datastore.local:594) - Adding New Image: spleen_24 =&gt; spleen_24.nii.gz\n[2024-02-24 17:31:19,111] [4401] [MainThread] [INFO] (monailabel.datastore.local:594) - Adding New Image: spleen_59 =&gt; spleen_59.nii.gz\n[2024-02-24 17:31:19,111] [4401] [MainThread] [INFO] (monailabel.datastore.local:594) - Adding New Image: spleen_17 =&gt; spleen_17.nii.gz\n[2024-02-24 17:31:19,111] [4401] [MainThread] [INFO] (monailabel.datastore.local:594) - Adding New Image: spleen_3 =&gt; spleen_3.nii.gz\n[2024-02-24 17:31:19,111] [4401] [MainThread] [INFO] (monailabel.datastore.local:594) - Adding New Image: spleen_38 =&gt; spleen_38.nii.gz\n[2024-02-24 17:31:19,111] [4401] [MainThread] [INFO] (monailabel.datastore.local:594) - Adding New Image: spleen_13 =&gt; spleen_13.nii.gz\n[2024-02-24 17:31:19,113] [4401] [MainThread] [INFO] (monailabel.datastore.local:577) - Invalidate count: 41\n[2024-02-24 17:31:19,116] [4401] [MainThread] [INFO] (monailabel.datastore.local:151) - Start observing external modifications on datastore (AUTO RELOAD)\n[2024-02-24 17:31:19,143] [4401] [MainThread] [INFO] (main:126) - +++ Adding Inferer:: deepedit =&gt; &lt;lib.infers.deepedit.DeepEdit object at 0x79df9887a830&gt;\n[2024-02-24 17:31:19,143] [4401] [MainThread] [INFO] (main:126) - +++ Adding Inferer:: deepedit_seg =&gt; &lt;lib.infers.deepedit.DeepEdit object at 0x79df988613c0&gt;\n[2024-02-24 17:31:19,143] [4401] [MainThread] [INFO] (main:191) - {'deepedit': &lt;lib.infers.deepedit.DeepEdit object at 0x79df9887a830&gt;, 'deepedit_seg': &lt;lib.infers.deepedit.DeepEdit object at 0x79df988613c0&gt;, 'Histogram+GraphCut': &lt;monailabel.scribbles.infer.HistogramBasedGraphCut object at 0x79df98860700&gt;, 'GMM+GraphCut': &lt;monailabel.scribbles.infer.GMMBasedGraphCut object at 0x79df98862dd0&gt;}\n[2024-02-24 17:31:19,143] [4401] [MainThread] [INFO] (main:206) - +++ Adding Trainer:: deepedit =&gt; &lt;lib.trainers.deepedit.DeepEdit object at 0x79df98860580&gt;\n[2024-02-24 17:31:19,144] [4401] [MainThread] [INFO] (monailabel.utils.sessions:51) - Session Path: /root/.cache/monailabel/sessions\n[2024-02-24 17:31:19,144] [4401] [MainThread] [INFO] (monailabel.utils.sessions:52) - Session Expiry (max): 3600\n[2024-02-24 17:31:19,144] [4401] [MainThread] [INFO] (monailabel.interfaces.app:469) - App Init - completed\n[2024-02-24 17:31:19,145] [timeloop] [INFO] Starting Timeloop..\n[2024-02-24 17:31:19,145] [4401] [MainThread] [INFO] (timeloop:60) - Starting Timeloop..\n[2024-02-24 17:31:19,145] [timeloop] [INFO] Registered job &lt;function MONAILabelApp.on_init_complete.&lt;locals&gt;.run_scheduler at 0x79df983080d0&gt;\n[2024-02-24 17:31:19,145] [4401] [MainThread] [INFO] (timeloop:42) - Registered job &lt;function MONAILabelApp.on_init_complete.&lt;locals&gt;.run_scheduler at 0x79df983080d0&gt;\n[2024-02-24 17:31:19,145] [timeloop] [INFO] Timeloop now started. Jobs will run based on the interval set\n[2024-02-24 17:31:19,145] [4401] [MainThread] [INFO] (timeloop:63) - Timeloop now started. Jobs will run based on the interval set\n[2024-02-24 17:31:19,145] [4401] [MainThread] [INFO] (uvicorn.error:62) - Application startup complete.\n[2024-02-24 17:31:19,146] [4401] [MainThread] [INFO] (uvicorn.error:217) - Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)"
  },
  {
    "objectID": "pre-post/code/2_ConvolutionalNN_PNEUMONIA_X_ray.html",
    "href": "pre-post/code/2_ConvolutionalNN_PNEUMONIA_X_ray.html",
    "title": "CNN_Neumonía",
    "section": "",
    "text": "Colab"
  },
  {
    "objectID": "pre-post/code/2_ConvolutionalNN_PNEUMONIA_X_ray.html#notebook-de-preprocesamiento",
    "href": "pre-post/code/2_ConvolutionalNN_PNEUMONIA_X_ray.html#notebook-de-preprocesamiento",
    "title": "CNN_Neumonía",
    "section": "Notebook de preprocesamiento",
    "text": "Notebook de preprocesamiento\nJoel Ricci López, 2021.\n\n\n\nOpen In Colab"
  },
  {
    "objectID": "pre-post/code/2_ConvolutionalNN_PNEUMONIA_X_ray.html#planteamiento",
    "href": "pre-post/code/2_ConvolutionalNN_PNEUMONIA_X_ray.html#planteamiento",
    "title": "CNN_Neumonía",
    "section": "Planteamiento",
    "text": "Planteamiento\nVamos a implementar un modelo de clasificación mediante una red neuronal convolucional para la identificación de casos de neumonía presentado en Kaggle. Usaremos Keras y su API Sequential para crear una red capa por capa. Además compararemos el desempeño de nuestra red contra un modelo creado a partir de Transfer Learning, utilizando la red preentrenada (VGG16)[https://arxiv.org/abs/1409.1556]."
  },
  {
    "objectID": "pre-post/code/2_ConvolutionalNN_PNEUMONIA_X_ray.html#estrategia",
    "href": "pre-post/code/2_ConvolutionalNN_PNEUMONIA_X_ray.html#estrategia",
    "title": "CNN_Neumonía",
    "section": "Estrategia",
    "text": "Estrategia\nLa estrategia a seguir es la siguiente: 1. Descargar los datos de Kaggle: Chest X-ray Images (Pneumonia). 2. Los datos fueron descargados en Colab y los conjuntos fueron modificados para que se cumpliera lo siguiente:\n- Conjunto de Entrenamiento: 80% de las imágenes. - Conjuntos de Validación y Prueba: 10% de las imágenes cada uno. - Se aseguró que la proporción de imágenes de las clases PNEUMONIA y NORMAL fuera la misma en cada conjunto. 3. Esta fase preliminar se realizó en el siguiente notebook:  4. Se realizó una fase de Análisis Exploratorio con las imágenes. 5. Se implementó una red neuronal convolucional (CNN) con Keras para la clasificación de las imágenes.\na) Se evaluó el desempeño de la CNN con el conjunto de prueba. 6. Se implementó una red neuronal convolucional basada en transferencia de conocimiento.\na) Se tomó como base la red preentrenada VGG16 con los pesos de imagenet.\nb) Se evaluó el desempeño de la red VGG16-base con el conjunto de prueba."
  },
  {
    "objectID": "pre-post/code/2_ConvolutionalNN_PNEUMONIA_X_ray.html#preliminares-y-carga-de-datos",
    "href": "pre-post/code/2_ConvolutionalNN_PNEUMONIA_X_ray.html#preliminares-y-carga-de-datos",
    "title": "CNN_Neumonía",
    "section": "Preliminares y carga de datos",
    "text": "Preliminares y carga de datos\n\nMontamos el acceso a Drive para cargar los datos\n\nfrom google.colab import drive\n\ndrive.mount('/content/gdrive/')\nroot_path = '/content/gdrive/My Drive/kaggle/ap-jrl-neumonia/chest_xray'\n%cd $root_path/\n%ls\n\nDrive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n/content/gdrive/My Drive/kaggle/ap-jrl-neumonia/chest_xray\nchest_xray/        modelo_neu_jrl_TRAINED.h5              test/\nhistory_cnn.pkl    modelo_vgg16_jrl.h5                    train/\n__MACOSX/          modelo_VGG16_jrl_trained_20_epocas.h5  val/\nmodelo_neu_jrl.h5  output.png\n\n\n\n\nCarga de las ibrerías a utilizar\n\nimport pandas as pd\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style='ticks', context='talk', palette='Spectral', font_scale=0.9)\n\n\n# Variables con los paths a los directorios de cada conjunto\ntest_dir = os.path.join(root_path, 'test')\ntrain_dir = os.path.join(root_path, 'train')\nval_dir = os.path.join(root_path, 'val')"
  },
  {
    "objectID": "pre-post/code/2_ConvolutionalNN_PNEUMONIA_X_ray.html#evaluación-del-modelo",
    "href": "pre-post/code/2_ConvolutionalNN_PNEUMONIA_X_ray.html#evaluación-del-modelo",
    "title": "CNN_Neumonía",
    "section": "Evaluación del modelo",
    "text": "Evaluación del modelo\n\n#@title Desempeño de la red durante el entrenamiento\n#@markdown (desplegar para ver el código)\n\n# Calculamos el F1 score a partir de los resultados de Precision y Recall\ndef get_f1_score(precision_arr, recall_arr):\n  epsilon       = 1e-7\n  precision_arr = np.array(precision_arr)\n  recall_arr    = np.array(recall_arr)\n  f1 = (2) * (precision_arr*recall_arr) / \\\n      (precision_arr + recall_arr + epsilon)\n  return f1\n\ndef plot_history(train_hist, suptitle=''):\n  # Agregar F1 score\n  train_hist['F1_score']     = get_f1_score(train_hist['precision'],\n                                    train_hist['recall'])\n  train_hist['val_F1_score'] = get_f1_score(train_hist['val_precision'],\n                                    train_hist['val_recall'])\n\n  # Gráficas\n  metrics = [key for key in train_hist.keys()\n              if not key.startswith('val')]\n\n  fig, ax = plt.subplots(2, int(len(metrics)/2),\n                        figsize = (16, 8))\n  for i, metric in enumerate(metrics):\n    if i &lt; 3:\n      j = 0\n    else:\n      i, j = i - 3, 1\n    ax[j, i].plot(train_hist[metric])\n    ax[j, i].plot(train_hist[f\"val_{metric}\"])\n    ax[j, i].set_title(metric.replace('_', ' ').capitalize())\n    ax[j, i].set_xlabel('Épocas')\n    ax[j, i].grid()\n    if metric != 'loss':\n      ax[j, i].set_ylim(0.7, 1.05)\n    else:\n      ax[j, i].legend(['Train', 'Val'])\n  plt.tight_layout()\n  plt.suptitle(suptitle, y = 1.02)\n  plt.show()\n\n\n# Ejecutamos la función\ntrain_hist = history #.history.copy()\nplot_history(train_hist,\n      suptitle='CNN: Métricas de evaluación en ' +\n      'los conjuntos de Entrenamiento y Validación')\n\n\n\n\n\n\n\n\n\nEvaluación con el conjunto de prueba\n\nscore_test = model.evaluate(test_generator, batch_size=128)\n\nfor i, j in zip(history.keys(), score_test):\n  print(f'{i.upper()}: {j:.3f}')\n\n19/19 [==============================] - 5s 226ms/step - loss: 0.4822 - accuracy: 0.8904 - roc_auc: 0.9339 - precision: 0.8740 - recall: 0.9930\nLOSS: 0.482\nACCURACY: 0.890\nROC_AUC: 0.934\nPRECISION: 0.874\nRECALL: 0.993\n\n\nOtenemos las etiquetas reales y las etiquetas predichas del conjunto de prueba.\n\n# Etiquetas reales\ny_test = test_generator.classes\n# Predicciones\ny_pred = model.predict(test_generator)\ny_pred_classes = (y_pred &gt;= 0.5).astype('int32')\n\n\nMatriz de confusión\n\n#@markdown (desplegar para ver el código)\n\nfrom sklearn.metrics import confusion_matrix\n\ndef plot_cfn_matrix(y_test, y_pred_classes, suptitle=''):\n  cf_matrix = confusion_matrix(y_test, y_pred_classes)\n\n  labels_a = np.array(\n      [f'{i}\\n{j}'\n      for i, j in\n        zip(['TN', 'FP', 'FN', 'TP'],\n              cf_matrix.flatten()\n          )\n      ]).reshape(2,2)\n\n  fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n  # Valores crudos\n  sns.heatmap(cf_matrix,\n              cmap = 'Reds', ax = ax[0],\n              fmt = '', annot = labels_a,\n              cbar_kws={'label': 'Imágenes'})\n  # Porcentajes\n  sns.heatmap(cf_matrix/np.sum(cf_matrix),\n              cmap = 'Blues', ax = ax[1],\n              fmt = '.2%', annot = True,\n              cbar_kws={'label': 'Porcentaje'})\n  for i in range(2):\n    ax[i].set(aspect = \"equal\",\n              xlabel = 'Predichos', ylabel = 'Reales')\n    for _, spine in ax[i].spines.items():\n      spine.set_visible(True)\n  plt.suptitle(suptitle, y = 1.05)\n  plt.tight_layout()\n\n# Genera la gráfica\nplot_cfn_matrix(y_test, y_pred_classes,\n    suptitle='Matriz de confusión CNN (conjunto de prueba)')\n\n\n\n\n\n\n\n\n\n\nCurva ROC y Curva de Precisión Sensibilidad\nPara esta evaluación usamos las métricas: - Área bajo la curva ROC. - Promedio de Precisión-Sensibilidad (avgPR): &gt; \\(\\text{AP} = \\sum_n (R_n - R_{n-1}) P_n\\)\n\n#@markdown (desplegar para ver el código)\n\nfrom sklearn.metrics import roc_auc_score, roc_curve, \\\n          f1_score, precision_recall_curve, average_precision_score\n\ndef plot_roc_and_pr_curves(y_test, y_pred,\n                           suptitle='', name = ''):\n  auc_ = roc_auc_score(y_test, y_pred)\n  f1_score_ = roc_auc_score(y_test, y_pred)\n  vg_pr_score = average_precision_score(y_test, y_pred)\n\n  fpr, tpr, _ = roc_curve(y_test, y_pred)\n  prec_, recall_, _ = precision_recall_curve(y_test, y_pred)\n  pr_baseline = y_test.sum() / len(y_test)\n\n  fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n\n  # Curva ROC\n  ax[0].plot(fpr, tpr, c = '#00A3A4',\n            linewidth = 4, label = f'{name}: {auc_:.3f} AUC')\n  ax[0].plot([0, 1], [0, 1], c = 'gray',\n            linestyle = '--', label = 'Aleatorio')\n  ax[0].legend()\n  ax[0].set(xlabel = '1 - Especificidad (FPR)',\n            ylabel = 'Sensibilidad (TPR)')\n\n  # Curva de Precision-Sensibilidad\n  ax[1].plot(recall_, prec_, c = '#F48000',\n            linewidth = 4, label = f'{name}: {vg_pr_score:.3f} avgPR')\n  ax[1].plot([0, 1], [pr_baseline, pr_baseline], c = 'gray',\n            linestyle = '--', label = 'Aleatorio')\n  ax[1].legend()\n  ax[1].set(xlabel = 'Sensibilidad',\n            ylabel = 'Precisión')\n  for i, title in enumerate(['Curva ROC',\n                            'Curva Precisión-Sensibilidad']):\n    ax[i].set(aspect='equal', ylim=(0,1.02), title=title)\n    ax[i].grid(True)\n  plt.tight_layout()\n  plt.suptitle(suptitle, y = 1.02)\n  plt.show()\n\n# Genera la gráfica\nplot_roc_and_pr_curves(y_test, y_pred,\n          suptitle='Desempeño CNN', name = 'CNN')\n\n\n\n\n\n\n\n\n\n\n\nInterpretación de Resultados\nVemos que la CNN obtiene buenos valores de desempeño en todas las métricas. - En particular nos interesa que la sensibilidad del modelo sea alta (0.98) y por consecuencia que el número de Falsos Negativos sea bajo (9 FN). - Se observa que el modelo sólo identificó 9 Falsos Negativos, lo cual es un buen resultado pues nos interesa que casos verdaderos de neumonía NO sean identificados como casos NORMALES. - El modelo tiene una mayor sensibilidad (0.98) que precisión (0.89). Es un buen resultado, pero hay que considerar que esto también es consecuencia de que en el conjunto de datos haya considerablemente más ejemplos positivos que negativos."
  },
  {
    "objectID": "pre-post/code/2_ConvolutionalNN_PNEUMONIA_X_ray.html#uso-de-la-red-vgg16",
    "href": "pre-post/code/2_ConvolutionalNN_PNEUMONIA_X_ray.html#uso-de-la-red-vgg16",
    "title": "CNN_Neumonía",
    "section": "Uso de la red VGG16",
    "text": "Uso de la red VGG16\nPara esta fase vamos a implementar el método de transferencia de conocimiento (Transfer learning) usando la red VGG16 (Simonyan and Zisserman, 2015).\n\nfrom tensorflow.keras.applications import VGG16\n\nvgg16 = VGG16(weights     = 'imagenet',\n              include_top = False,\n              # Definimos el tamaño de la capa de entrada\n              # igual a las dimensiones de las imágenes\n              input_shape = (IMG_HEIGHT, IMG_WIDTH, 3))\n\n\nVisualizamos la arquitectura de la VGG16\n\nvisualkeras.layered_view(vgg16, scale_xy=1)\n\n\n\n\n\n\n\n\n\nfrom tensorflow.keras.layers import Input\n# Creamos nuestro nuevo modelo\n\n# Primero congelamos los pesos de las primeras capas de VGG16\nvgg16.trainable = False\n\nmodel_vgg16 = Sequential()\n# Añadimos a VGG16 como si fuese una capa\nmodel_vgg16.add(Input(shape = (IMG_HEIGHT, IMG_WIDTH, 3)))\nmodel_vgg16.add(vgg16)\nmodel_vgg16.add(Flatten())\nmodel_vgg16.add(Dense(128,\n                  activation='relu'))\nmodel_vgg16.add(Dropout(0.4))\nmodel_vgg16.add(Dense(1,\n                      activation='sigmoid',\n                      bias_initializer = output_bias))\nmodel_vgg16.summary()\n\nModel: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nvgg16 (Functional)           (None, 4, 4, 512)         14714688  \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 8192)              0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 128)               1048704   \n_________________________________________________________________\ndropout_3 (Dropout)          (None, 128)               0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 1)                 129       \n=================================================================\nTotal params: 15,763,521\nTrainable params: 1,048,833\nNon-trainable params: 14,714,688\n_________________________________________________________________\n\n\nListo, el modelo tiene 524,417 de parámetros entrenables.\n\n\nCompilamos el modelo basado en VGG16\n\n# Usamos las mismas métricas de evaluación\nmetrics = [BinaryAccuracy(name = 'accuracy'),\n           AUC(name = 'roc_auc'),\n           Precision(name ='precision'),\n           Recall(name = 'recall')\n          ]\n\n# Compilamos el modelo\nmodel_vgg16.compile(\n    optimizer = optimizer,\n    loss      = 'binary_crossentropy',\n    metrics   = metrics\n)\n\n\n\nEntrenamiento del moelo basado en VGG16\n\nCallbacks\n\nBATCH_SIZE = 64\nEPOCHS = 50\n\n# ModelCheckpoint\nmodel_filepath = f\"{root_path}/modelo_vgg16_jrl.h5\"\ncheckpoint_vgg16 = ModelCheckpoint(model_filepath,\n                monitor        = 'val_loss',\n                verbose        = 1,\n                save_best_only = True,\n                mode           = 'min'\n)\n\n# Lista de Callbacks\ncallbacks_list_vgg16 = [checkpoint_vgg16, early_stop]\n\n\n\n\nEntrenamiento de la red\n\n%%time\n# Entrenamiento\nhistory_vgg16 = model_vgg16.fit(\n          train_generator,\n          validation_data = val_generator,\n          epochs          = EPOCHS,\n          batch_size      = BATCH_SIZE,\n          callbacks       = callbacks_list_vgg16\n)\n\nEpoch 1/50\n147/147 [==============================] - 86s 555ms/step - loss: 1.1479 - accuracy: 0.8489 - roc_auc: 0.8704 - precision: 0.8874 - recall: 0.9027 - val_loss: 0.2040 - val_accuracy: 0.9195 - val_roc_auc: 0.9656 - val_precision: 0.9397 - val_recall: 0.9507\n\nEpoch 00001: val_loss improved from inf to 0.20395, saving model to /content/gdrive/My Drive/kaggle/ap-jrl-neumonia/chest_xray/modelo_vgg16_jrl.h5\nEpoch 2/50\n147/147 [==============================] - 78s 532ms/step - loss: 0.1513 - accuracy: 0.9360 - roc_auc: 0.9830 - precision: 0.9473 - recall: 0.9647 - val_loss: 0.2019 - val_accuracy: 0.9195 - val_roc_auc: 0.9677 - val_precision: 0.9566 - val_recall: 0.9319\n\nEpoch 00002: val_loss improved from 0.20395 to 0.20186, saving model to /content/gdrive/My Drive/kaggle/ap-jrl-neumonia/chest_xray/modelo_vgg16_jrl.h5\nEpoch 3/50\n147/147 [==============================] - 79s 534ms/step - loss: 0.1478 - accuracy: 0.9409 - roc_auc: 0.9826 - precision: 0.9522 - recall: 0.9677 - val_loss: 0.1855 - val_accuracy: 0.9315 - val_roc_auc: 0.9702 - val_precision: 0.9447 - val_recall: 0.9624\n\nEpoch 00003: val_loss improved from 0.20186 to 0.18551, saving model to /content/gdrive/My Drive/kaggle/ap-jrl-neumonia/chest_xray/modelo_vgg16_jrl.h5\nEpoch 4/50\n147/147 [==============================] - 79s 536ms/step - loss: 0.1204 - accuracy: 0.9525 - roc_auc: 0.9894 - precision: 0.9611 - recall: 0.9739 - val_loss: 0.2082 - val_accuracy: 0.9161 - val_roc_auc: 0.9646 - val_precision: 0.9255 - val_recall: 0.9624\n\nEpoch 00004: val_loss did not improve from 0.18551\nEpoch 5/50\n147/147 [==============================] - 79s 535ms/step - loss: 0.1245 - accuracy: 0.9509 - roc_auc: 0.9870 - precision: 0.9662 - recall: 0.9675 - val_loss: 0.2068 - val_accuracy: 0.9281 - val_roc_auc: 0.9661 - val_precision: 0.9507 - val_recall: 0.9507\n\nEpoch 00005: val_loss did not improve from 0.18551\nEpoch 6/50\n147/147 [==============================] - 78s 529ms/step - loss: 0.1235 - accuracy: 0.9502 - roc_auc: 0.9887 - precision: 0.9623 - recall: 0.9682 - val_loss: 0.2046 - val_accuracy: 0.9264 - val_roc_auc: 0.9652 - val_precision: 0.9423 - val_recall: 0.9577\n\nEpoch 00006: val_loss did not improve from 0.18551\nEpoch 7/50\n147/147 [==============================] - 78s 529ms/step - loss: 0.1050 - accuracy: 0.9607 - roc_auc: 0.9913 - precision: 0.9653 - recall: 0.9817 - val_loss: 0.2287 - val_accuracy: 0.9264 - val_roc_auc: 0.9627 - val_precision: 0.9362 - val_recall: 0.9648\n\nEpoch 00007: val_loss did not improve from 0.18551\nEpoch 8/50\n147/147 [==============================] - 77s 526ms/step - loss: 0.1014 - accuracy: 0.9596 - roc_auc: 0.9922 - precision: 0.9672 - recall: 0.9774 - val_loss: 0.1935 - val_accuracy: 0.9332 - val_roc_auc: 0.9651 - val_precision: 0.9510 - val_recall: 0.9577\n\nEpoch 00008: val_loss did not improve from 0.18551\nEpoch 9/50\n147/147 [==============================] - 78s 528ms/step - loss: 0.1184 - accuracy: 0.9494 - roc_auc: 0.9893 - precision: 0.9589 - recall: 0.9726 - val_loss: 0.1990 - val_accuracy: 0.9298 - val_roc_auc: 0.9657 - val_precision: 0.9385 - val_recall: 0.9671\n\nEpoch 00009: val_loss did not improve from 0.18551\nEpoch 10/50\n147/147 [==============================] - 77s 527ms/step - loss: 0.1041 - accuracy: 0.9570 - roc_auc: 0.9919 - precision: 0.9625 - recall: 0.9785 - val_loss: 0.2304 - val_accuracy: 0.9195 - val_roc_auc: 0.9627 - val_precision: 0.9165 - val_recall: 0.9789\n\nEpoch 00010: val_loss did not improve from 0.18551\nEpoch 11/50\n147/147 [==============================] - 77s 526ms/step - loss: 0.1035 - accuracy: 0.9583 - roc_auc: 0.9921 - precision: 0.9638 - recall: 0.9792 - val_loss: 0.2082 - val_accuracy: 0.9229 - val_roc_auc: 0.9659 - val_precision: 0.9281 - val_recall: 0.9695\n\nEpoch 00011: val_loss did not improve from 0.18551\nEpoch 12/50\n147/147 [==============================] - 77s 526ms/step - loss: 0.0845 - accuracy: 0.9629 - roc_auc: 0.9940 - precision: 0.9687 - recall: 0.9814 - val_loss: 0.1949 - val_accuracy: 0.9315 - val_roc_auc: 0.9654 - val_precision: 0.9488 - val_recall: 0.9577\n\nEpoch 00012: val_loss did not improve from 0.18551\nEpoch 13/50\n147/147 [==============================] - 78s 527ms/step - loss: 0.1085 - accuracy: 0.9587 - roc_auc: 0.9905 - precision: 0.9679 - recall: 0.9766 - val_loss: 0.1864 - val_accuracy: 0.9298 - val_roc_auc: 0.9678 - val_precision: 0.9385 - val_recall: 0.9671\n\nEpoch 00013: val_loss did not improve from 0.18551\n\n\n\nmodel_vgg16.save(f\"{root_path}/modelo_vgg16_jrl_TRAINED.h5\")\n\n\nhistory_vgg16 = history_vgg16.history\nwith open(f\"{root_path}/history_cnn_vgg16.pkl\", 'wb') as file:\n  pickle.dump(history_vgg16, file)\n\n\n# Ejecutamos la función\ntrain_hist_v16 = history_vgg16.copy()\nplot_history(train_hist_v16,\n      suptitle='VGG16-base: Métricas de evaluación en ' +\n      'los conjuntos de Entrenamiento y Validación')\n\n\n\n\n\n\n\n\n\nscore_test_vgg16 = model_vgg16.evaluate(test_generator, batch_size=64)\n\nfor i, j in zip(train_hist_v16.keys(), score_test_vgg16):\n  print(f'{i.upper()}: {j:.2f}')\n\n19/19 [==============================] - 5s 251ms/step - loss: 0.2621 - accuracy: 0.9092 - roc_auc: 0.9625 - precision: 0.8943 - recall: 0.9930\nLOSS: 0.26\nACCURACY: 0.91\nROC_AUC: 0.96\nPRECISION: 0.89\nRECALL: 0.99\n\n\n\n# Etiquetas reales\n# y_test = test_generator.classes\n# Predicciones\ny_pred_v16 = model_vgg16.predict(test_generator)\ny_pred_classes_v16 = (y_pred_v16 &gt;= 0.5).astype('int32')\n\n\n#@markdown (desplegar para ver el código)\n\n# Genera la gráfica\nplot_cfn_matrix(y_test, y_pred_classes_v16,\n    suptitle='Matriz de confusión VGG16-base (conjunto de prueba)')\n\n\n\n\n\n\n\n\n\n# Genera la gráfica\nplot_roc_and_pr_curves(y_test, y_pred_v16,\n          suptitle = 'Desempeño VGG16-base',\n          name = 'VGG16')"
  },
  {
    "objectID": "pre-post/code/2_ConvolutionalNN_PNEUMONIA_X_ray.html#interpretación-de-resultados-1",
    "href": "pre-post/code/2_ConvolutionalNN_PNEUMONIA_X_ray.html#interpretación-de-resultados-1",
    "title": "CNN_Neumonía",
    "section": "Interpretación de Resultados",
    "text": "Interpretación de Resultados\nLa red VGG16 mostró buenos resultados, teniendo un desempeño similar al de la primer red (CNN) en términos de Sensibilidad. Sin embargo, mostró una mejora en la precisión y por consiguiente en el Área bajo la curva ROC, así como un mayor valor de avgPR."
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#índice",
    "href": "posts/Presentaciones/Datos_1.html#índice",
    "title": "Datos en el ámbito de la salud",
    "section": "Índice",
    "text": "Índice\n\nIntroducción\nClasificación de los datos\nBases de datos\nAnotación de datos\nPrivacidad y confidencialidad\nRegistros electrónicos de salud\nPreprocesado de datos médicos\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#disponibilidad-masiva-de-datos",
    "href": "posts/Presentaciones/Datos_1.html#disponibilidad-masiva-de-datos",
    "title": "Datos en el ámbito de la salud",
    "section": "Disponibilidad masiva de datos",
    "text": "Disponibilidad masiva de datos\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#machine-learning",
    "href": "posts/Presentaciones/Datos_1.html#machine-learning",
    "title": "Datos en el ámbito de la salud",
    "section": "Machine learning",
    "text": "Machine learning\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#problema-con-los-datos",
    "href": "posts/Presentaciones/Datos_1.html#problema-con-los-datos",
    "title": "Datos en el ámbito de la salud",
    "section": "Problema con los datos",
    "text": "Problema con los datos\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#por-tipos-de-datos",
    "href": "posts/Presentaciones/Datos_1.html#por-tipos-de-datos",
    "title": "Datos en el ámbito de la salud",
    "section": "Por tipos de datos",
    "text": "Por tipos de datos\n\n\n\nDatos estructurados\nDatos no estructurados\nDatos semiestructurados\n\n\n\n\n\n\n\n\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#datos-estructurados",
    "href": "posts/Presentaciones/Datos_1.html#datos-estructurados",
    "title": "Datos en el ámbito de la salud",
    "section": "Datos estructurados",
    "text": "Datos estructurados\n\n\n\n\n\n\n\n\n\n\n\nA_id\nSize\nWeight\nSweetness\nCrunchiness\nJuiciness\nRipeness\nAcidity\nQuality\n\n\n\n\n0\n0.0\n-3.970049\n-2.512336\n5.346330\n-1.012009\n1.844900\n0.329840\n-0.491590483\ngood\n\n\n1\n1.0\n-1.195217\n-2.839257\n3.664059\n1.588232\n0.853286\n0.867530\n-0.722809367\ngood\n\n\n2\n2.0\n-0.292024\n-1.351282\n-1.738429\n-0.342616\n2.838636\n-0.038033\n2.621636473\nbad\n\n\n3\n3.0\n-0.657196\n-2.271627\n1.324874\n-0.097875\n3.637970\n-3.413761\n0.790723217\ngood\n\n\n4\n4.0\n1.364217\n-1.296612\n-0.384658\n-0.553006\n3.030874\n-1.303849\n0.501984036\ngood\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3996\n3996.0\n-0.293118\n1.949253\n-0.204020\n-0.640196\n0.024523\n-1.087900\n1.854235285\ngood\n\n\n3997\n3997.0\n-2.634515\n-2.138247\n-2.440461\n0.657223\n2.199709\n4.763859\n-1.334611391\nbad\n\n\n3998\n3998.0\n-4.008004\n-1.779337\n2.366397\n-0.200329\n2.161435\n0.214488\n-2.229719806\ngood\n\n\n3999\n3999.0\n0.278540\n-1.715505\n0.121217\n-1.154075\n1.266677\n-0.776571\n1.599796456\ngood\n\n\n4000\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nCreated_by_Nidula_Elgiriyewithana\nNaN\n\n\n\n\n4001 rows × 9 columns\n\n\n\n\n\n\n\n\n\n\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#ejemplos-de-datos-estructurados",
    "href": "posts/Presentaciones/Datos_1.html#ejemplos-de-datos-estructurados",
    "title": "Datos en el ámbito de la salud",
    "section": "Ejemplos de datos estructurados",
    "text": "Ejemplos de datos estructurados\n\nRegistros Electrónicos de Salud\nDatos de Laboratorio Clínico"
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#datos-no-estructurados",
    "href": "posts/Presentaciones/Datos_1.html#datos-no-estructurados",
    "title": "Datos en el ámbito de la salud",
    "section": "Datos no estructurados",
    "text": "Datos no estructurados\n\n\n\nTexto\nImágenes\nAudio\nVideo\n…\n\n\n\n\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#datos-semiestructurados",
    "href": "posts/Presentaciones/Datos_1.html#datos-semiestructurados",
    "title": "Datos en el ámbito de la salud",
    "section": "Datos semiestructurados",
    "text": "Datos semiestructurados\n\n\n\n\n\n\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#por-naturaleza-de-los-datos",
    "href": "posts/Presentaciones/Datos_1.html#por-naturaleza-de-los-datos",
    "title": "Datos en el ámbito de la salud",
    "section": "Por naturaleza de los datos",
    "text": "Por naturaleza de los datos\n\nCategóricos\nNuméricos\nTemporales\nEspaciales\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#por-granularidad-de-los-datos",
    "href": "posts/Presentaciones/Datos_1.html#por-granularidad-de-los-datos",
    "title": "Datos en el ámbito de la salud",
    "section": "Por granularidad de los datos",
    "text": "Por granularidad de los datos\n\nDatos a nivel de instancia: Datos individuales que representan observaciones o eventos específicos, como registros de pacientes o imágenes médicas.\nDatos a nivel de atributo: Datos que representan características o atributos de las instancias, como la edad, el peso o los síntomas clínicos.\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#fuente-de-los-datos",
    "href": "posts/Presentaciones/Datos_1.html#fuente-de-los-datos",
    "title": "Datos en el ámbito de la salud",
    "section": "Fuente de los datos",
    "text": "Fuente de los datos\n\nDatos primarios: Datos recopilados directamente de la fuente original, como registros médicos electrónicos (EHR) o resultados de pruebas de laboratorio.\nDatos secundarios: Datos que han sido recopilados o procesados por terceros, como conjuntos de datos disponibles en repositorios públicos o bases de datos comerciales.\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#datos-anotados-vs-no-anotados",
    "href": "posts/Presentaciones/Datos_1.html#datos-anotados-vs-no-anotados",
    "title": "Datos en el ámbito de la salud",
    "section": "Datos anotados vs no anotados",
    "text": "Datos anotados vs no anotados\n\nEstructura semántica: Los datos anotados tienen metadatos adicionales que proporcionan contexto y significado, como las etiquetas que marcan entidades nombradas.\nInteroperabilidad y procesabilidad: Los datos anotados son más interoperables y procesables, lo que facilita su integración con otros sistemas y su procesamiento automatizado.\nFacilidad de búsqueda y recuperación: Los datos anotados son más fáciles de buscar y recuperar, ya que las anotaciones permiten identificar elementos específicos.\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#introducción-a-las-bases-de-datos",
    "href": "posts/Presentaciones/Datos_1.html#introducción-a-las-bases-de-datos",
    "title": "Datos en el ámbito de la salud",
    "section": "Introducción a las Bases de Datos",
    "text": "Introducción a las Bases de Datos\n\nLas bases de datos son herramientas fundamentales en el mundo digital actual. Permiten almacenar, organizar y acceder a grandes cantidades de información de manera eficiente… Desde las redes sociales hasta las instituciones financieras, las bases de datos son esenciales para el funcionamiento de innumerables aplicaciones y sistemas.\nUna base de datos es, en esencia, una colección organizada de información estructurada, almacenada electrónicamente en un sistema informático… Esta información puede incluir cualquier tipo de datos, como texto, números, imágenes, videos y archivos…\nPara crear, editar y mantener los archivos y registros de una base de datos, se utiliza un software de base de datos, también llamado sistema de gestión de bases de datos (DBMS). Un DBMS actúa como intermediario entre la base de datos y los usuarios o programas que la utilizan, permitiendo realizar operaciones como agregar, modificar, eliminar y organizar datos.\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#tipos-de-bases-de-datos",
    "href": "posts/Presentaciones/Datos_1.html#tipos-de-bases-de-datos",
    "title": "Datos en el ámbito de la salud",
    "section": "Tipos de Bases de Datos",
    "text": "Tipos de Bases de Datos\nExisten diferentes tipos de bases de datos, cada una con sus propias características y usos. Algunos de los tipos más comunes son:\n\nRelacionales\nNoSQL\nOrientadas a Objetos\nGrafos (KG)\nVectoriales\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#bases-de-datos-relacionalesrdbms",
    "href": "posts/Presentaciones/Datos_1.html#bases-de-datos-relacionalesrdbms",
    "title": "Datos en el ámbito de la salud",
    "section": "Bases de Datos Relacionales(RDBMS)",
    "text": "Bases de Datos Relacionales(RDBMS)\n\nModelo: Datos organizados en tablas con filas (registros) y columnas (atributos).\nEsquema: Rígido. La estructura se define antes de insertar datos.\nRelaciones: Explícitas, a través de claves primarias y claves foráneas.\nLenguaje: SQL (Structured Query Language).\nEjemplos: MySQL, PostgreSQL, Oracle, SQL Server, MariaDB.\nVentajas:\n\nMadurez, amplia adopción y gran cantidad de herramientas.\nFuertes garantías ACID (Atomicidad, Consistencia, Aislamiento, Durabilidad).\nIdeal para datos estructurados y transacciones.\n\nDesventajas:\n\nLa escalabilidad vertical puede ser costosa (sistemas monoservidor).\nDificultad para manejar datos no estructurados o semiestructurados a gran escala.\n\n\n\nLas transacciones ACID son un conjunto de cuatro propiedades (Atomicidad, Consistencia, Aislamiento y Durabilidad) que garantizan la fiabilidad de las operaciones en una base de datos. Atomicidad asegura que una transacción se ejecute completa o no se ejecute; Consistencia mantiene la validez de los datos según las reglas definidas; Aislamiento evita interferencias entre transacciones concurrentes; y Durabilidad asegura que los cambios sean permanentes incluso ante fallos. En esencia, ACID proporciona un mecanismo para que las bases de datos manejen operaciones de forma segura y predecible, manteniendo la integridad de los datos en todo momento, siendo fundamental en aplicaciones donde la precisión y la fiabilidad son críticas."
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#bases-de-datos-nosql",
    "href": "posts/Presentaciones/Datos_1.html#bases-de-datos-nosql",
    "title": "Datos en el ámbito de la salud",
    "section": "Bases de Datos NoSQL",
    "text": "Bases de Datos NoSQL\n\nModelo: Alternativas al modelo tabular.\nEsquema: Flexible o sin esquema. Mayor adaptabilidad a cambios en los datos.\nRelaciones: Implícitas o mediante referencias (varía según el tipo).\nLenguaje: Algunas usan lenguajes tipo SQL, otras APIs.\nVentajas:\n\nEscalabilidad horizontal (más fácil y económico añadir servidores).\nFlexibilidad para datos no estructurados, semiestructurados o cambiantes.\nAlto rendimiento para ciertas operaciones (lectura/escritura rápida).\n\nDesventajas:Generalmente, menor soporte para transacciones ACID completas (En general las documentales). Mayor complejidad en consultas complejas que requieran “unir” datos.\n\n\n(Application Programming Interface)"
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#nosql-documentales",
    "href": "posts/Presentaciones/Datos_1.html#nosql-documentales",
    "title": "Datos en el ámbito de la salud",
    "section": "NoSQL: Documentales",
    "text": "NoSQL: Documentales\n\nModelo: Documentos (JSON o BSON). Colecciones de pares clave-valor.\nEjemplos: MongoDB, Couchbase, Amazon DocumentDB.\nCasos de Uso:\n\nAplicaciones web y móviles.\nGestión de contenido (CMS).\nCatálogos de productos.\nDatos de sesión.\n\nVentajas:\n\nFlexibilidad para esquemas cambiantes.\nBuena escalabilidad horizontal.\nDesarrollo ágil.\n\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#nosql-clave-valor",
    "href": "posts/Presentaciones/Datos_1.html#nosql-clave-valor",
    "title": "Datos en el ámbito de la salud",
    "section": "NoSQL: Clave-Valor",
    "text": "NoSQL: Clave-Valor\n\nModelo: Pares clave-valor. El tipo más simple de NoSQL.\nEjemplos: Redis, Memcached, Amazon DynamoDB (con capacidades clave-valor).\nCasos de Uso: Caché (almacenamiento temporal de datos de acceso frecuente), gestión de sesiones, contadores.\nVentajas: Extremadamente rápidas para búsquedas por clave, alta escalabilidad.\nDesventajas: Funcionalidad muy limitada más allá de la búsqueda por clave.\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#nosql-columnares",
    "href": "posts/Presentaciones/Datos_1.html#nosql-columnares",
    "title": "Datos en el ámbito de la salud",
    "section": "NoSQL: Columnares",
    "text": "NoSQL: Columnares\n\nModelo: Columnar. Almacenamiento optimizado por columnas, no por filas.\nEjemplos: Cassandra, HBase, Google Cloud Bigtable.\nCasos de Uso: Big Data y análisis, series temporales, aplicaciones con alta carga de escritura.\nVentajas: Eficientes para leer un subconjunto de columnas de muchas filas, compresión eficiente, escalabilidad horizontal.\nDesventajas: Menos eficientes para leer filas completas.\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#nosql-de-grafos-kg",
    "href": "posts/Presentaciones/Datos_1.html#nosql-de-grafos-kg",
    "title": "Datos en el ámbito de la salud",
    "section": "NoSQL: De Grafos (KG)",
    "text": "NoSQL: De Grafos (KG)\n\nModelo: Grafos (nodos y aristas). Representan entidades y relaciones.\nEjemplos: Neo4j, Amazon Neptune, JanusGraph.\nCasos de Uso: Redes sociales, sistemas de recomendación, análisis de fraude, gestión del conocimiento.\nVentajas: Optimizadas para consultar y explorar relaciones, lenguajes de consulta especializados (Cypher, Gremlin).\nDesventajas: Pueden ser menos eficientes si las relaciones no son el foco principal de las consultas.\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#bases-de-datos-orientadas-a-objetos",
    "href": "posts/Presentaciones/Datos_1.html#bases-de-datos-orientadas-a-objetos",
    "title": "Datos en el ámbito de la salud",
    "section": "Bases de Datos Orientadas a Objetos",
    "text": "Bases de Datos Orientadas a Objetos\n\nModelo: Objetos (atributos y métodos). Integración con la POO.\nEsquema: Definido por clases.\nLenguaje: Extensiones de lenguajes de POO o lenguajes específicos.\nEjemplos: db4o, ObjectDB, Versant.\nCasos de Uso (Nichos): CAD/CAM,Modelado científico y Multimedia.\nVentajas: Buena integración con POO, manejo de objetos complejos.\nDesventajas: Menor adopción que RDBMS y NoSQL, falta de estandarización como SQL.\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#bases-de-datos-vectoriales",
    "href": "posts/Presentaciones/Datos_1.html#bases-de-datos-vectoriales",
    "title": "Datos en el ámbito de la salud",
    "section": "Bases de Datos Vectoriales",
    "text": "Bases de Datos Vectoriales\n\nModelo: Vectores de alta dimensión (embeddings).\nEsquema: Flexible (dimensión fija del vector, datos originales variables).\nLenguaje: APIs y, a veces, extensiones de SQL.\nEjemplos: Pinecone, Weaviate, Qdrant, Milvus, Faiss, Chroma.\nCasos de Uso: Búsqueda semántica, sistemas de recomendación, RAG (Retrieval-Augmented Generation) para LLMs, Detección de anomalías.\nVentajas: Búsqueda por similitud, algoritmos ANN (Approximate Nearest Neighbor), optimizadas para altas dimensiones.\nDesventajas: Tecnología relativamente nueva, generalmente no ofrecen transacciones ACID completas.\n\n\nLas transacciones ACID son un conjunto de cuatro propiedades (Atomicidad, Consistencia, Aislamiento y Durabilidad) que garantizan la fiabilidad de las operaciones en una base de datos. Atomicidad asegura que una transacción se ejecute completa o no se ejecute; Consistencia mantiene la validez de los datos según las reglas definidas; Aislamiento evita interferencias entre transacciones concurrentes; y Durabilidad asegura que los cambios sean permanentes incluso ante fallos. En esencia, ACID proporciona un mecanismo para que las bases de datos manejen operaciones de forma segura y predecible, manteniendo la integridad de los datos en todo momento, siendo fundamental en aplicaciones donde la precisión y la fiabilidad son críticas."
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#la-elección-de-una-base-de-datos-depende-de",
    "href": "posts/Presentaciones/Datos_1.html#la-elección-de-una-base-de-datos-depende-de",
    "title": "Datos en el ámbito de la salud",
    "section": "La elección de una base de datos depende de:",
    "text": "La elección de una base de datos depende de:\n\nLa estructura de los datos.\nLas necesidades de la aplicación (escalabilidad, transacciones, etc.).\nEl tipo de consultas que se realizarán.\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#qué-es-la-anotación-de-datos",
    "href": "posts/Presentaciones/Datos_1.html#qué-es-la-anotación-de-datos",
    "title": "Datos en el ámbito de la salud",
    "section": "¿Qué es la anotación de datos?",
    "text": "¿Qué es la anotación de datos?\nLa anotación de datos es el proceso de atribución o etiquetado de datos para ayudar a los algoritmos de aprendizaje automático a comprender y clasificar la información que procesan.\n\nTexto\nImágenes\nAudio\nVideo"
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#anotación-de-imágenes",
    "href": "posts/Presentaciones/Datos_1.html#anotación-de-imágenes",
    "title": "Datos en el ámbito de la salud",
    "section": "Anotación de imágenes",
    "text": "Anotación de imágenes\n\n\n\nClasificación\nReconocimiento/Detección\nSegmentación\n\n\n\nVGG"
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#anotación-de-audio",
    "href": "posts/Presentaciones/Datos_1.html#anotación-de-audio",
    "title": "Datos en el ámbito de la salud",
    "section": "Anotación de audio",
    "text": "Anotación de audio"
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#anotación-de-video",
    "href": "posts/Presentaciones/Datos_1.html#anotación-de-video",
    "title": "Datos en el ámbito de la salud",
    "section": "Anotación de video",
    "text": "Anotación de video"
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#anotación-de-texto",
    "href": "posts/Presentaciones/Datos_1.html#anotación-de-texto",
    "title": "Datos en el ámbito de la salud",
    "section": "Anotación de texto",
    "text": "Anotación de texto\n\n\n\nSemántica\nIntención\nOpinión\n\ntabula"
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#antecedentes",
    "href": "posts/Presentaciones/Datos_1.html#antecedentes",
    "title": "Datos en el ámbito de la salud",
    "section": "Antecedentes",
    "text": "Antecedentes\n\n\n\n\n\n\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#problemas-de-la-historia-clínica-en-papel",
    "href": "posts/Presentaciones/Datos_1.html#problemas-de-la-historia-clínica-en-papel",
    "title": "Datos en el ámbito de la salud",
    "section": "Problemas de la historia clínica en papel",
    "text": "Problemas de la historia clínica en papel\n\nAccesibilidad limitada\nRiesgo de pérdida o daño\nDificultad para mantener actualizada la información\nEspacio de almacenamiento y organización\nLimitaciones en la compartición y colaboración\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#componentes-de-los-ehr",
    "href": "posts/Presentaciones/Datos_1.html#componentes-de-los-ehr",
    "title": "Datos en el ámbito de la salud",
    "section": "Componentes de los EHR",
    "text": "Componentes de los EHR\n\nMódulos de Registro y Documentación\nPedidos y Resultados de laboratorio\nGestión de Medicamentos\nIntegración de Imágenes Médicas (PACS)\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#sap-1",
    "href": "posts/Presentaciones/Datos_1.html#sap-1",
    "title": "Datos en el ámbito de la salud",
    "section": "SAP 1",
    "text": "SAP 1\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#sap-2",
    "href": "posts/Presentaciones/Datos_1.html#sap-2",
    "title": "Datos en el ámbito de la salud",
    "section": "SAP 2",
    "text": "SAP 2\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#sap-3",
    "href": "posts/Presentaciones/Datos_1.html#sap-3",
    "title": "Datos en el ámbito de la salud",
    "section": "SAP 3",
    "text": "SAP 3\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#aria",
    "href": "posts/Presentaciones/Datos_1.html#aria",
    "title": "Datos en el ámbito de la salud",
    "section": "ARIA",
    "text": "ARIA\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#limpieza-de-datos",
    "href": "posts/Presentaciones/Datos_1.html#limpieza-de-datos",
    "title": "Datos en el ámbito de la salud",
    "section": "Limpieza de datos",
    "text": "Limpieza de datos\n\nEliminación de observaciones incompletas\nImputación de valores\nAnálisis de patrones de datos faltantes\nUso de modelos predictivos\nTécnicas de imputación múltiple\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#normalización-de-variables-continuas",
    "href": "posts/Presentaciones/Datos_1.html#normalización-de-variables-continuas",
    "title": "Datos en el ámbito de la salud",
    "section": "Normalización de variables continuas",
    "text": "Normalización de variables continuas\n\nLas variables continuas pueden tener diferentes escalas y rangos de valores\nLa normalización ayuda a mitigar este problema al transformar las variables para que estén en una escala similar\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#técnicas-de-normalización",
    "href": "posts/Presentaciones/Datos_1.html#técnicas-de-normalización",
    "title": "Datos en el ámbito de la salud",
    "section": "Técnicas de normalización",
    "text": "Técnicas de normalización\n\nDatos con una media de 0 y desviación estándar 1\nNormalización min-max (0-1)\nPuede mejorar la convergencia\n\n\n\n\n\n\n\nAdvertencia\n\n\nEs importante aplicar la misma transformación de normalización tanto al conjunto de entrenamiento como al conjunto de prueba para evitar sesgos en los datos.\n\n\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#integración-de-datos",
    "href": "posts/Presentaciones/Datos_1.html#integración-de-datos",
    "title": "Datos en el ámbito de la salud",
    "section": "Integración de datos",
    "text": "Integración de datos\n\nFuentes de Datos Diversas\nResolución de inconsistencias\nLimpieza de Datos\nGestión de Datos Temporales\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#la-ingeniería-de-características",
    "href": "posts/Presentaciones/Datos_1.html#la-ingeniería-de-características",
    "title": "Datos en el ámbito de la salud",
    "section": "La ingeniería de características",
    "text": "La ingeniería de características\n\nCreación de Nuevas características\nSelección de Características Relevantes\n\n\nPor ejemplo el índice de masa corporal"
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#anonimización-de-los-datos",
    "href": "posts/Presentaciones/Datos_1.html#anonimización-de-los-datos",
    "title": "Datos en el ámbito de la salud",
    "section": "Anonimización de los datos",
    "text": "Anonimización de los datos\n\nLa anonimización es el proceso de eliminar o modificar datos personales de tal manera que ya no sea posible identificar a un individuo a partir de los datos.\nLa anonimización está vinculada a lo que dicen las leyes de protección de datos en la UE (RGPD)\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#técnicas-de-anonimización",
    "href": "posts/Presentaciones/Datos_1.html#técnicas-de-anonimización",
    "title": "Datos en el ámbito de la salud",
    "section": "Técnicas de anonimización",
    "text": "Técnicas de anonimización\n\nEliminación de identificadores directos\nPseudonimización\nEnmascaramiento\nAgregación\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#consideraciones-al-elegir-una-técnica-de-anonimización",
    "href": "posts/Presentaciones/Datos_1.html#consideraciones-al-elegir-una-técnica-de-anonimización",
    "title": "Datos en el ámbito de la salud",
    "section": "Consideraciones al elegir una técnica de anonimización",
    "text": "Consideraciones al elegir una técnica de anonimización\n\nEl tipo de datos que se procesan\nEl nivel de riesgo de reidentificación\nLos fines para los que se utilizarán los datos anonimizados\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#data-augmentation",
    "href": "posts/Presentaciones/Datos_1.html#data-augmentation",
    "title": "Datos en el ámbito de la salud",
    "section": "Data Augmentation",
    "text": "Data Augmentation\nEs una técnica que consiste en crear nuevas muestras de datos a partir de datos existentes. Estas nuevas muestras se generan mediante la aplicación de transformaciones predefinidas a los datos originales\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#ventajas-de-data-augmentation",
    "href": "posts/Presentaciones/Datos_1.html#ventajas-de-data-augmentation",
    "title": "Datos en el ámbito de la salud",
    "section": "Ventajas de Data Augmentation",
    "text": "Ventajas de Data Augmentation\n\nAumenta el tamaño del conjunto de datos\nMejora la generalización del modelo\nReduce el sesgo del modelo\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#data-augmentation-en-imágenes-médicas",
    "href": "posts/Presentaciones/Datos_1.html#data-augmentation-en-imágenes-médicas",
    "title": "Datos en el ámbito de la salud",
    "section": "Data Augmentation en imágenes médicas",
    "text": "Data Augmentation en imágenes médicas\n\nRotaciones\nZoom\nCambio de contraste\nAdición de ruido\nSimulación de artefactos\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#data-augmentation-en-señales-biomédicas",
    "href": "posts/Presentaciones/Datos_1.html#data-augmentation-en-señales-biomédicas",
    "title": "Datos en el ámbito de la salud",
    "section": "Data Augmentation en señales biomédicas",
    "text": "Data Augmentation en señales biomédicas\nComo el Electrocardiograma o el Electroencefalograma\n\nAdición de ruido\nCambio de escala\nSegmentación de señales\nSimulación de artefactos"
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#la-imagen-médica-más-allá-de-lo-visual",
    "href": "posts/Presentaciones/Datos_1.html#la-imagen-médica-más-allá-de-lo-visual",
    "title": "Datos en el ámbito de la salud",
    "section": "La Imagen Médica: Más Allá de lo Visual",
    "text": "La Imagen Médica: Más Allá de lo Visual\n\nModalidades tradicionales:\n\nRayos X\nTomografía Computarizada (TC)\nResonancia Magnética (RM)\nMedicina Nuclear (PET/CT)\n\nInterpretación: Principalmente cualitativa y subjetiva.\nLimitaciones:\n\nVariabilidad inter-observador.\nInformación potencialmente oculta."
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#qué-es-la-radiómica",
    "href": "posts/Presentaciones/Datos_1.html#qué-es-la-radiómica",
    "title": "Datos en el ámbito de la salud",
    "section": "¿Qué es la Radiómica?",
    "text": "¿Qué es la Radiómica?\n\nLa radiómica es un enfoque cuantitativo de la imagenología médica, que tiene como objetivo mejorar los datos existentes disponibles para los clínicos mediante análisis matemáticos avanzados. A través de la extracción matemática de la distribución espacial de las intensidades de señal y las interrelaciones de los píxeles, la radiómica cuantifica la información textural utilizando métodos de análisis del campo de la inteligencia artificial.\n\n\nTransforma imágenes médicas en datos cuantitativos.\n“Biopsia virtual”: extraer información sin procedimientos invasivos.\nLa imagen como un gran conjunto de datos a explotar."
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#qué-es-la-radiómica-1",
    "href": "posts/Presentaciones/Datos_1.html#qué-es-la-radiómica-1",
    "title": "Datos en el ámbito de la salud",
    "section": "¿Qué es la Radiómica?",
    "text": "¿Qué es la Radiómica?"
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#flujo-de-trabajo-de-la-radiómica-imagen",
    "href": "posts/Presentaciones/Datos_1.html#flujo-de-trabajo-de-la-radiómica-imagen",
    "title": "Datos en el ámbito de la salud",
    "section": "Flujo de Trabajo de la Radiómica: Imagen",
    "text": "Flujo de Trabajo de la Radiómica: Imagen\n\nAdquisición de Imágenes\n\nImágenes médicas estándar (TC, RM, etc.).\n¡Importancia de la estandarización!\n\nSegmentación (ROI)\n\nDelimitar la región de interés (tumor, órgano…).\nManual, semiautomática o automática."
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#flujo-de-trabajo-de-la-radiómica-parte-radiómica",
    "href": "posts/Presentaciones/Datos_1.html#flujo-de-trabajo-de-la-radiómica-parte-radiómica",
    "title": "Datos en el ámbito de la salud",
    "section": "Flujo de Trabajo de la Radiómica: Parte Radiómica",
    "text": "Flujo de Trabajo de la Radiómica: Parte Radiómica\n\n\n\nExtracción de Características: Los algoritmos extraen miles de características.\nForma: Tamaño, esfericidad…\nIntensidad: Distribución de valores.\nTextura: Patrones espaciales.\n\n\n\n\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#flujo-de-trabajo-de-la-radiómica-análisis",
    "href": "posts/Presentaciones/Datos_1.html#flujo-de-trabajo-de-la-radiómica-análisis",
    "title": "Datos en el ámbito de la salud",
    "section": "Flujo de Trabajo de la Radiómica: Análisis",
    "text": "Flujo de Trabajo de la Radiómica: Análisis\n\nSelección y Modelado\n\nNo todas las características son útiles.\nSelección: quedarnos con las más relevantes.\nModelado: crear algoritmos predictivos (Machine Learning).\n\nValidación del Modelo\n\nProbar con datos “nuevos”.\nAsegurar la generalización (evitar sobreajuste).\nMétricas: Precisión, Sensibilidad, Especificidad."
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#aplicaciones-clínicas-oncología",
    "href": "posts/Presentaciones/Datos_1.html#aplicaciones-clínicas-oncología",
    "title": "Datos en el ámbito de la salud",
    "section": "Aplicaciones Clínicas: Oncología",
    "text": "Aplicaciones Clínicas: Oncología\n\nDetección temprana\nDiferenciación benigno/maligno.\nPredicción de respuesta a tratamientos.\nPronóstico y supervivencia."
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#aplicaciones-clínicas-otras-áreas",
    "href": "posts/Presentaciones/Datos_1.html#aplicaciones-clínicas-otras-áreas",
    "title": "Datos en el ámbito de la salud",
    "section": "Aplicaciones Clínicas: Otras Áreas",
    "text": "Aplicaciones Clínicas: Otras Áreas\n\n\nCardiovasculares\n\nCaracterización de placas.\n\nNeurológicas\n\nAlzheimer.\nEsclerosis múltiple.\n\n\nPulmonares\n\nEPOC.\nFibrosis."
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#desafíos-y-limitaciones",
    "href": "posts/Presentaciones/Datos_1.html#desafíos-y-limitaciones",
    "title": "Datos en el ámbito de la salud",
    "section": "Desafíos y Limitaciones",
    "text": "Desafíos y Limitaciones\n\n\nEstandarización: protocolos consistentes.\n“Caja Negra”: interpretabilidad de los modelos.\nGrandes Datos: necesidad de muchos datos de calidad.\nIntegración Clínica: herramientas fáciles de usar."
  },
  {
    "objectID": "posts/code/Titanic.html",
    "href": "posts/code/Titanic.html",
    "title": "Datos sobre el desastre del Titanic",
    "section": "",
    "text": "Colab\n\nEste notebook está basado en un reto planteado por Kaggle titulado Titanic:Machine Learning from Disaster. En ese enlace teneis lo que significa cada elemento de la tabla. Nosotros simplemente nos centraremos en el pre-procesado de datos y no haremos nada de machine learning\nNo hace falta modificar el código, solamente hay que contestar a las preguntas planteadas.\nHaz una copia del colab en tu espacio personal\nSi tienes dudas está permitido preguntar al LLM que utilices habitualmente.\nImportar librerías necesarias\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import KNNImputer\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport gdown\n\nBajar los ficheros de datos\n\ngdown.download(\"https://drive.google.com/uc?id=1cMIDGxAwhM_U5KOPPYacLhgZB5jdJesP\")\ngdown.download(\"https://drive.google.com/uc?id=1pNBx8OWEzZEAwAH5Ge-bBu-wnlBICfrt\")\n\nDownloading...\nFrom: https://drive.google.com/uc?id=1cMIDGxAwhM_U5KOPPYacLhgZB5jdJesP\nTo: /content/train.csv\n100%|██████████| 61.2k/61.2k [00:00&lt;00:00, 46.8MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1pNBx8OWEzZEAwAH5Ge-bBu-wnlBICfrt\nTo: /content/test.csv\n100%|██████████| 28.6k/28.6k [00:00&lt;00:00, 41.1MB/s]\n\n\n'test.csv'\n\n\nLeer los datos de entrenamiento y de prueba\n\ntrain_data = pd.read_csv('train.csv')\ntest_data = pd.read_csv('test.csv')\n\nMostrar las primeras filas del conjunto de datos de entrenamiento\n\ntrain_data.head()\n\n\n  \n    \n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\nAgregar una columna ‘source’ para indicar si los datos son de entrenamiento o de prueba\n\ntrain_data['source'] = 'train'\ntest_data['source'] = 'test'\n\nConcatenar los conjuntos de datos de entrenamiento y de prueba\n\ntotal_data = pd.concat((train_data, test_data), axis=0)\ntotal_data.reset_index(inplace=True, drop=True)\n\n\n# @title Q1 ¿Por qué unimos los dos conjuntos en un mismo dataframe?\nQ1 = \"Para que el procesamiento de los datos sea el mismo en ambos conjuntos.\" # @param [\"_\",\"Para reducir el tamaño de los conjuntos de datos.\", \"Porque es una práctica estándar en la ciencia de datos.\", \"Para simplificar el análisis de los datos.\", \"Para que el procesamiento de los datos sea el mismo en ambos conjuntos.\", \"Porque es más eficiente computacionalmente.\"]\n\n\nInformación sobre el conjunto de datos combinado\n\ntotal_data.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1309 entries, 0 to 1308\nData columns (total 13 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  1309 non-null   int64  \n 1   Survived     891 non-null    float64\n 2   Pclass       1309 non-null   int64  \n 3   Name         1309 non-null   object \n 4   Sex          1309 non-null   object \n 5   Age          1046 non-null   float64\n 6   SibSp        1309 non-null   int64  \n 7   Parch        1309 non-null   int64  \n 8   Ticket       1309 non-null   object \n 9   Fare         1308 non-null   float64\n 10  Cabin        295 non-null    object \n 11  Embarked     1307 non-null   object \n 12  source       1309 non-null   object \ndtypes: float64(3), int64(4), object(6)\nmemory usage: 133.1+ KB\n\n\nCrear una nueva característica ‘HasCabin’ que indica si el pasajero tiene una cabina\n\ntotal_data[\"HasCabin\"] = pd.notna(total_data[\"Cabin\"])\n\nEliminar la columna original ‘Cabin’\n\ntotal_data.drop(['Cabin'], axis=1, inplace=True)\n\n\n# @title Q2: ¿Por qué hemos cambiado la variable Cabin por la variable HasCabin?\nQ2 = \"_\" # @param [\"_\", \"Porque la variable cabin era redundante.\", \"Para simplificar el conjunto de datos.\", \"Porque la variable hascabin ofrece más información relevante.\", \"Para mejorar la visualización de los datos.\", \"Porque no aporta nada saber en qué cabina iba el pasajero o si viajaba en camarote o no.\"]\n\nCrear una nueva característica ‘IsAlone’ que indica si el pasajero viaja solo\n\ntotal_data['IsAlone'] = (total_data['SibSp'] + total_data['Parch'] == 0)\n\nManejar valores faltantes: completar con el valor más frecuente para ‘Embarked’\n\ntotal_data[\"Embarked\"]\n\n\n\n\n\n\n\n\nEmbarked\n\n\n\n\n0\nS\n\n\n1\nC\n\n\n2\nS\n\n\n3\nS\n\n\n4\nS\n\n\n...\n...\n\n\n1304\nS\n\n\n1305\nC\n\n\n1306\nS\n\n\n1307\nS\n\n\n1308\nC\n\n\n\n\n1309 rows × 1 columns\ndtype: object\n\n\n\nembarked_top = total_data[\"Embarked\"].describe().top\ntotal_data[\"Embarked\"] = total_data[\"Embarked\"].fillna(embarked_top)\n\n\ntotal_data[\"Embarked\"].describe()\n\n\n\n\n\n\n\n\nEmbarked\n\n\n\n\ncount\n1309\n\n\nunique\n3\n\n\ntop\nS\n\n\nfreq\n916\n\n\n\n\ndtype: object\n\n\n\n# @title Q3:¿Por qué se ha tomado el valor más frecuente para llenar los huecos en la columna Embarked?\nQ3 = \"_\" # @param [\"_\", \"Por tomar una al azar.\", \"Para simplificar el procesamiento de los datos.\", \"Porque es mucho más frecuente subir en Southampton que en las otras dos opciones.\", \"Para mejorar la visualización de los datos.\", \"Por una cuestión alfabética.\"]\n\nCodificación One-Hot.\nLa codificación OneHot es una técnica para convertir variables categóricas en vectores binarios. Cada categoría única se representa con una columna binaria, donde un valor de 1 indica la presencia de la categoría y 0 su ausencia. Es ampliamente utilizada en el procesamiento de datos para preparar variables categóricas para su uso en algoritmos de aprendizaje automático.\n\n# Creamos una instancia de OneHotEncoder\nohe = OneHotEncoder()\n\n# Codificación One-Hot para la variable 'Sex'\nsex_column = total_data['Sex'].values.reshape(-1, 1)\nsex_encoded = ohe.fit_transform(sex_column)\nsex_df = pd.DataFrame(sex_encoded.toarray(), columns=ohe.get_feature_names_out())\n\n# Codificación One-Hot para la variable 'HasCabin'\nhas_cabin_column = total_data['HasCabin'].values.reshape(-1, 1)\nhas_cabin_encoded = ohe.fit_transform(has_cabin_column)\nhas_cabin_df = pd.DataFrame(has_cabin_encoded.toarray(), columns=ohe.get_feature_names_out())\n\n# Codificación One-Hot para la variable 'Embarked'\nembarked_column = total_data['Embarked'].values.reshape(-1, 1)\nembarked_encoded = ohe.fit_transform(embarked_column)\nembarked_df = pd.DataFrame(embarked_encoded.toarray(), columns=ohe.get_feature_names_out())\n\n\n# @title Q4: ¿Por qué crees que se han codificado así las variables Sex, HasCabin y Embarked?\nQ4 = \"_\" # @param [\"_\", \"Para aumentar la complejidad de los datos.\", \"Porque las variables originales eran numéricas y necesitaban ser transformadas.\", \"Porque son variables categóricas y son más fáciles de tratar así.\", \"Para reducir el tamaño del conjunto de datos.\", \"Porque las variables originales no eran relevantes para el análisis.\"]\n\nBorrar las columnas no necesarias\n\ntotal_data.drop(['Name', 'Ticket'], axis=1, inplace=True)\n\nEliminar las columnas originales después de la codificación One-Hot\n\ntotal_data.drop(['Sex', 'HasCabin', 'Embarked'], axis=1, inplace=True)\n\nConcatenar las nuevas características codificadas con el conjunto de datos\n\ntotal_data = pd.concat([total_data, sex_df, has_cabin_df, embarked_df], axis=1)\n\nSeparar el conjunto de datos combinado en conjuntos de entrenamiento y prueba\n\ntrain_data_p = total_data[total_data[\"source\"]==\"train\"]\ntest_data_p  = total_data[total_data[\"source\"]==\"test\"]\n\nEliminar la columna ‘source’\n\ntrain_data_p = train_data_p.drop(['source'], axis=1)  # Crear una copia del DataFrame sin la columna 'source'\ntest_data_p = test_data_p.drop(['source'], axis=1)  # Crear una copia del DataFrame sin la columna 'source'\n\nImputación de valores faltantes usando KNNImputer\nAyuda. Preguntadle a ChatGPT qué significa el código de la celda siguiente\n\nfrom sklearn.impute import KNNImputer\n\nimputer = KNNImputer(n_neighbors=8)\n\n# learn the parameters of kNN and apply the model\n# on train_data which includes target variable (Survived)\ntrain_data_p = pd.DataFrame(imputer.fit_transform(train_data_p), columns = train_data_p.columns)\n\n# Apply the kNN model on test_data to predict the missing Age values\ntest_data_p = pd.DataFrame(imputer.transform(test_data_p), columns = test_data_p.columns)\n\n\n# @title Q5: ¿Qué estrategia se ha seguido para llenar las edades faltantes?\nQ5 = \"_\" # @param [\"_\", \"Dividir el conjunto de datos en grupos basados en otras características y luego asignar una edad promedio a cada grupo.\", \"Utilizar técnicas de imputación basadas en la media o la mediana de las edades existentes.\", \"Eliminar las filas con valores de edad faltantes.\", \"Aplicar un promedio de las edades existentes.\", \"Utilizar técnicas de estimación de los valores faltantes con un algoritmo no supervisado.\"]\n\n\ntest_data_p\n\n\n  \n    \n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nAge\nSibSp\nParch\nFare\nIsAlone\nx0_female\nx0_male\nx0_False\nx0_True\nx0_C\nx0_Q\nx0_S\n\n\n\n\n891\n892\nNaN\n3\n34.5\n0\n0\n7.8292\nTrue\n0.0\n1.0\n1.0\n0.0\n0.0\n1.0\n0.0\n\n\n892\n893\nNaN\n3\n47.0\n1\n0\n7.0000\nFalse\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n\n\n893\n894\nNaN\n2\n62.0\n0\n0\n9.6875\nTrue\n0.0\n1.0\n1.0\n0.0\n0.0\n1.0\n0.0\n\n\n894\n895\nNaN\n3\n27.0\n0\n0\n8.6625\nTrue\n0.0\n1.0\n1.0\n0.0\n0.0\n0.0\n1.0\n\n\n895\n896\nNaN\n3\n22.0\n1\n1\n12.2875\nFalse\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1304\n1305\nNaN\n3\nNaN\n0\n0\n8.0500\nTrue\n0.0\n1.0\n1.0\n0.0\n0.0\n0.0\n1.0\n\n\n1305\n1306\nNaN\n1\n39.0\n0\n0\n108.9000\nTrue\n1.0\n0.0\n0.0\n1.0\n1.0\n0.0\n0.0\n\n\n1306\n1307\nNaN\n3\n38.5\n0\n0\n7.2500\nTrue\n0.0\n1.0\n1.0\n0.0\n0.0\n0.0\n1.0\n\n\n1307\n1308\nNaN\n3\nNaN\n0\n0\n8.0500\nTrue\n0.0\n1.0\n1.0\n0.0\n0.0\n0.0\n1.0\n\n\n1308\n1309\nNaN\n3\nNaN\n1\n1\n22.3583\nFalse\n0.0\n1.0\n1.0\n0.0\n1.0\n0.0\n0.0\n\n\n\n\n418 rows × 15 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\nVisualización de la correlación entre características\n\nplt.figure(figsize=(15,15))\nsns.heatmap(train_data_p.corr(), cbar=True, cmap='vlag', vmin=-1, vmax=1, annot=True)\n\n\n\n\n\n\n\n\nCalcular las tasas de supervivencia para mujeres y hombres en el conjunto de datos de entrenamiento\n\nwomen = train_data[train_data.Sex==\"female\"][\"Survived\"]\nmen = train_data[train_data.Sex==\"male\"][\"Survived\"]\nsurvivalRateForWomen = sum(women)/len(women)\nsurvivalRateForMen = sum(men)/len(men)\nprint(\"Tasa de supervivencia para mujeres: \", survivalRateForWomen)\nprint(\"Tasa de supervivencia para hombres: \", survivalRateForMen)\n\nTasa de supervivencia para mujeres:  0.7420382165605095\nTasa de supervivencia para hombres:  0.18890814558058924"
  },
  {
    "objectID": "posts/code/Embedding.html",
    "href": "posts/code/Embedding.html",
    "title": "Embeddings",
    "section": "",
    "text": "Colab\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom transformers import BertTokenizer, BertModel\nimport torch\nfrom scipy.spatial.distance import cosine\n\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.nn.functional as F\nimport random\nimport torchvision.models as models\n\nDefinimos vectores\n\nvector_1 = np.array([1, 2, 3])\nvector_2 = np.array([4, 5, 6])\nvector_3 = np.array([2, -1, 0])\nvector_4 = np.array([2, 4, 6])\n\nCalculamos la similitud del coseno de dos vectores\n\n\n\nSimilitud de dos vectores\n\n\n\ndef cosine_similarity(v1, v2):\n    dot_product = np.dot(v1, v2)\n    norm_v1 = np.linalg.norm(v1)\n    norm_v2 = np.linalg.norm(v2)\n    return dot_product / (norm_v1 * norm_v2)\n\n\nsimilarity_1_2 = cosine_similarity(vector_1, vector_2)\nsimilarity_1_3 = cosine_similarity(vector_1, vector_3)\nsimilarity_2_3 = cosine_similarity(vector_2, vector_3)\nsimilarity_1_4 = cosine_similarity(vector_1, vector_4)\n\nMostramos los resultados\n\nprint(\"Similitud de coseno entre vector_1 y vector_2:\", similarity_1_2)\nprint(\"Similitud de coseno entre vector_1 y vector_3:\", similarity_1_3)\nprint(\"Similitud de coseno entre vector_2 y vector_3:\", similarity_2_3)\nprint(\"Similitud de coseno entre vector_1 y vector_4:\", similarity_1_4)\n\nSimilitud de coseno entre vector_1 y vector_2: 0.9746318461970762\nSimilitud de coseno entre vector_1 y vector_3: 0.0\nSimilitud de coseno entre vector_2 y vector_3: 0.15289415743128765\nSimilitud de coseno entre vector_1 y vector_4: 1.0\n\n\nVamos ahora a por un modelo de texto.Cargamos el modelo\n\nmodel_name_bert = 'bert-base-uncased'\ntokenizer = BertTokenizer.from_pretrained(model_name_bert)\nmodel = BertModel.from_pretrained(model_name_bert)\n\n/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \nThe secret `HF_TOKEN` does not exist in your Colab secrets.\nTo authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\nYou will be able to reuse this secret in all of your notebooks.\nPlease note that authentication is recommended but still optional to access public models or datasets.\n  warnings.warn(\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndef cosine_similarity(v1, v2):\n    return 1 - cosine(v1, v2)\n\n\ndef get_bert_embeddings(text):\n    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n    with torch.no_grad():\n        outputs = model(**inputs)\n    embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n    return embeddings\n\n\ndef calculate_text_similarity(text1, text2):\n    \"\"\"\n    Función para calcular la similitud de coseno entre dos textos basada en sus representaciones vectoriales BERT.\n\n    Args:\n        text1 (str): Primer texto.\n        text2 (str): Segundo texto.\n\n    Returns:\n        float: Similitud de coseno entre las representaciones vectoriales de los dos textos.\n    \"\"\"\n    # Obtener las representaciones vectoriales BERT para ambos textos\n    embeddings_text1 = get_bert_embeddings(text1)\n    embeddings_text2 = get_bert_embeddings(text2)\n\n    # Calcular la similitud de coseno entre las representaciones vectoriales\n    similarity = cosine_similarity(embeddings_text1, embeddings_text2)\n    return similarity\n\n\n# Ejemplo de uso:\ntext1 = \"orange is a color\"\ntext2 = \"orange is a fruit\"\nsimilarity = calculate_text_similarity(text1, text2)\nprint(\"Similarity:\", similarity)\n\nSimilarity: 0.8144723176956177\n\n\nProbemos ahora con un modelo de imágenes\nPrimero bajamos las imágenes de nuestra querida base de datos MNIST\n\n# Transformación para las imágenes\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n])\n\n# Descargar la base de datos MNIST y aplicar transformaciones\ntrain_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\ntest_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n\n# Crear cargadores de datos para acceder a los datos por lotes\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=1, shuffle=True)\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=1, shuffle=True)\n\n\n\nDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n\n\n100%|██████████| 9912422/9912422 [00:00&lt;00:00, 175017306.71it/s]\n\n\nExtracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n\n\n100%|██████████| 28881/28881 [00:00&lt;00:00, 26847449.87it/s]\n\n\nExtracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n\n\n100%|██████████| 1648877/1648877 [00:00&lt;00:00, 43325866.23it/s]\n\n\nExtracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n\n\n\n\n\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n\n\n100%|██████████| 4542/4542 [00:00&lt;00:00, 4608255.63it/s]\n\n\nExtracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n\n\n\n\ndef get_random_images(number1, number2, dataset=test_dataset, seed=42):\n    \"\"\"\n    Función para obtener dos imágenes al azar de un conjunto de datos para dos números específicos.\n\n    Args:\n        dataset: Conjunto de datos del que se seleccionarán las imágenes.\n        number1: Primer número para seleccionar imágenes.\n        number2: Segundo número para seleccionar imágenes.\n        seed: Semilla para reproducibilidad. Por defecto, 42.\n\n    Returns:\n        tuple: Tupla de dos imágenes y etiquetas seleccionadas al azar.\n    \"\"\"\n    # Obtener los índices de las imágenes para cada número\n    indices1 = [i for i, (image, label) in enumerate(dataset) if label == number1]\n    indices2 = [i for i, (image, label) in enumerate(dataset) if label == number2]\n\n    # Seleccionar dos índices al azar para cada número\n    random.seed(seed)\n    rand_idx1 = random.randint(0, len(indices1) - 1)\n    rand_idx2 = random.randint(0, len(indices2) - 1)\n\n    # Obtener los datos de las imágenes seleccionadas\n    image1, label1 = dataset[indices1[rand_idx1]]\n    image2, label2 = dataset[indices2[rand_idx2]]\n\n    return (image1, label1), (image2, label2)\n\ndef show_images(image_label1, image_label2):\n    \"\"\"\n    Función para mostrar dos imágenes junto con sus etiquetas en la misma ventana.\n\n    Args:\n        image_label1: Tupla que contiene la primera imagen y su etiqueta.\n        image_label2: Tupla que contiene la segunda imagen y su etiqueta.\n    \"\"\"\n    plt.figure(figsize=(8, 4))\n\n    plt.subplot(1, 2, 1)\n    plt.imshow(image_label1[0].squeeze(), cmap='gray')\n    plt.title('Label: {}'.format(image_label1[1]))\n    plt.axis('off')\n\n    plt.subplot(1, 2, 2)\n    plt.imshow(image_label2[0].squeeze(), cmap='gray')\n    plt.title('Label: {}'.format(image_label2[1]))\n    plt.axis('off')\n\n    plt.show()\n\n\ntupla1,tupla2=get_random_images(1,5)\n\n\nshow_images(tupla1,tupla2)\n\n\n\n\n\n\n\n\nCargamos un modelo genérico, en nuestro caso ResNet\n\n# Cargar el modelo pre-entrenado\nmodel_resnet = models.resnet18(pretrained=True)\n# Eliminar la capa de clasificación\nmodel_resnet = torch.nn.Sequential(*(list(model_resnet.children())[:-1]))\nmodel_resnet.eval()\n\nSequential(\n  (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (2): ReLU(inplace=True)\n  (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (5): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (6): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (7): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (8): AdaptiveAvgPool2d(output_size=(1, 1))\n)\n\n\n\ndef calculate_image_similarity(image_label1, image_label2, model=model_resnet):\n    \"\"\"\n    Función para calcular la similitud de coseno entre dos imágenes basadas en sus representaciones vectoriales.\n\n    Args:\n        image_label1: Tupla que contiene la primera imagen y su etiqueta.\n        image_label2: Tupla que contiene la segunda imagen y su etiqueta.\n        model: Modelo utilizado para obtener las representaciones vectoriales de las imágenes.\n\n    Returns:\n        float: Similitud de coseno entre las representaciones vectoriales de las dos imágenes.\n    \"\"\"\n    def get_image_embeddings(image_tensor):\n        with torch.no_grad():\n            # Expande la dimensión de los canales para que coincida con lo que el modelo espera (de 1 a 3 canales)\n            image_tensor = image_tensor.repeat(1, 3, 1, 1)\n            # Pasar la imagen a través del modelo y obtener la representación vectorial\n            output = model(image_tensor)\n            # Aplanar la salida\n            output = output.view(output.size(0), -1)\n            # Normalizar la salida\n            output = F.normalize(output, p=2, dim=1)\n        return output\n\n    # Obtener los tensores de imagen de las tuplas\n    image_tensor1, _ = image_label1\n    image_tensor2, _ = image_label2\n\n    # Obtener las representaciones vectoriales de las imágenes\n    image1_embedding = get_image_embeddings(image_tensor1.unsqueeze(0))\n    image2_embedding = get_image_embeddings(image_tensor2.unsqueeze(0))\n\n    # Calcular la similitud de coseno entre las representaciones vectoriales\n    similarity = F.cosine_similarity(image1_embedding, image2_embedding)\n    return similarity.item()\n\n\ncalculate_image_similarity(tupla1,tupla2)\n\n0.6484431028366089\n\n\nCargamos ahora un modelo finetuneado para el conjunto MNIST\n\ndef preprocess_input(input_tensor, target_size=(224, 224)):\n    \"\"\"\n    Función para preprocesar un tensor de entrada para que coincida con el tamaño y formato esperado por el modelo.\n\n    Args:\n        input_tensor (torch.Tensor): Tensor de entrada con un solo canal.\n        target_size (tuple): Tamaño al que se debe redimensionar la imagen. Por defecto, (224, 224).\n\n    Returns:\n        torch.Tensor: Tensor de entrada preprocesado con el tamaño y formato correctos.\n    \"\"\"\n    # Definir una transformación para redimensionar la imagen\n    resize_transform = transforms.Compose([\n        transforms.Resize(target_size),  # Redimensionar la imagen al tamaño objetivo\n    ])\n\n    # Redimensionar el tensor de entrada\n    input_tensor_resized = resize_transform(input_tensor)\n\n    # Replicar el canal único en tres canales\n    input_tensor_rgb = input_tensor_resized.expand(1, 3, *target_size)  # Expandir el tensor a tres canales\n\n    return input_tensor_rgb\n\n\ndef preprocess_and_infer(tupla1, tupla2, model=model_mnist2):\n    \"\"\"\n    Función para preprocesar dos imágenes de entrada y realizar inferencia con un modelo dado.\n\n    Args:\n        tupla1: Tupla que contiene la primera imagen y su etiqueta.\n        tupla2: Tupla que contiene la segunda imagen y su etiqueta.\n        model: Modelo utilizado para la inferencia.\n\n    Returns:\n        torch.Tensor: Salida del modelo para la primera imagen.\n        torch.Tensor: Salida del modelo para la segunda imagen.\n    \"\"\"\n    # Añadir un nuevo eje de canal a las imágenes y realizar la inferencia con el modelo\n    with torch.no_grad():\n        image1_tensor = preprocess_input(tupla1[0])\n        image2_tensor = preprocess_input(tupla2[0])\n        outputs1 = model(image1_tensor)\n        outputs2 = model(image2_tensor)\n\n    return F.cosine_similarity(outputs1.to_tuple()[0], outputs2.to_tuple()[0], dim=1)\n\n# Ejemplo de uso:\n# outputs1, outputs2 = preprocess_and_infer(tupla1, tupla2, model_mnist2)\n\n\npreprocess_and_infer(tupla1, tupla2)\n\n/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n  warnings.warn(\n\n\ntensor([-0.1767])"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Acerca de el autor",
    "section": "",
    "text": "Material del curso"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Intel·ligència artificial en medicina",
    "section": "",
    "text": "Datos sobre el desastre del Titanic\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAI e Imagen Médica\n\n\n\n\n\n\nPresentación\n\n\n\n\n\n\n\n\n\n28 feb 2025\n\n\nAntonio Otal Palacín\n\n\n\n\n\n\n\n\n\n\n\n\nIntroducción a la imagen médica\n\n\n\n\n\n\nPresentación\n\n\n\n\n\n\n\n\n\n24 feb 2025\n\n\nAntonio Otal Palacín\n\n\n\n\n\n\n\n\n\n\n\n\nExtracción de características radiómicas\n\n\n\n\n\n\nColab\n\n\n\n\n\n\n\n\n\n14 feb 2025\n\n\n\n\n\n\n\n\n\n\n\n\nLARGE LANGUAGE MODELS (LLM) APLICADOS AL ENTORNO MÉDICO\n\n\n\n\n\n\nPresentación\n\n\n\n\n\n\n\n\n\n12 feb 2025\n\n\nAntonio Otal Palacín\n\n\n\n\n\n\n\n\n\n\n\n\nTEXTO, DATOS E IMAGEN\n\n\n\n\n\n\nIntroducción\n\n\n\n\n\n\n\n\n\n12 feb 2025\n\n\nAntonio Otal Palacín\n\n\n\n\n\n\n\n\n\n\n\n\nRadiomics training\n\n\n\n\n\n\nColab\n\n\n\n\n\n\n\n\n\n26 feb 2024\n\n\n\n\n\n\n\n\n\n\n\n\nProcesamiento de prospectos\n\n\n\n\n\n\nPresentación\n\n\n\n\n\n\n\n\n\n25 feb 2024\n\n\nAntonio Otal Palacín\n\n\n\n\n\n\n\n\n\n\n\n\nEmbeddings\n\n\n\n\n\n\nColab\n\n\n\n\n\n\n\n\n\n19 feb 2024\n\n\n\n\n\n\n\n\n\n\n\n\nDatos en el ámbito de la salud\n\n\n\n\n\n\nPresentación\n\n\n\n\n\n\n\n\n\n14 feb 2024\n\n\nAntonio Otal Palacín\n\n\n\n\n\n\n\n\n\n\n\n\nWord Embeddings\n\n\n\n\n\n\nColab\n\n\n\n\n\n\n\n\n\n12 feb 2024\n\n\n\n\n\n\nNo hay resultados"
  },
  {
    "objectID": "posts/code/Radiomics_Feature_Extraction.html",
    "href": "posts/code/Radiomics_Feature_Extraction.html",
    "title": "Extracción de características radiómicas",
    "section": "",
    "text": "Colab\nEn este notebook, te mostraremos cómo extraer características en modo batch utilizando el enfoque de la interfaz de línea de comandos (cli).\nUtilizaremos datos del repositorio https://github.com/rcuocolo/PROSTATEx_masks\nPrimero, necesitamos verificar e instalar las bibliotecas necesarias para realizar estos pasos más fácilmente.\n!pip install --progress-bar off -q gdown\n!pip install --upgrade --pre SimpleITK --find-links https://github.com/SimpleITK/SimpleITK/releases/tag/latest --progress-bar off -q\n!pip install --progress-bar off -q matplotlib\n!pip install --progress-bar off -q pyradiomics\n!pip install --progress-bar off -q pyyaml\n!pip install --progress-bar off -q pandas\n\n  Installing build dependencies ... done\n  Getting requirements to build wheel ... done\n  Preparing metadata (pyproject.toml) ... done\n  Preparing metadata (setup.py) ... done\n  Preparing metadata (setup.py) ... done\n  Building wheel for pyradiomics (setup.py) ... done\n  Building wheel for docopt (setup.py) ... done"
  },
  {
    "objectID": "posts/code/Radiomics_Feature_Extraction.html#importar-librerías-necesarias",
    "href": "posts/code/Radiomics_Feature_Extraction.html#importar-librerías-necesarias",
    "title": "Extracción de características radiómicas",
    "section": "Importar librerías necesarias",
    "text": "Importar librerías necesarias\n\nimport os\nimport pandas as pd\nimport csv\nimport yaml\nimport SimpleITK as sitk\nimport radiomics\nimport matplotlib.pyplot as plt\nimport gdown\nimport random\nimport numpy as np\nimport ipywidgets as widgets\nfrom ipywidgets import interact, fixed, Layout, Button, HBox\nfrom IPython.display import display, clear_output\nimport seaborn as sns\nfrom sklearn.feature_selection import VarianceThreshold\n\n\n# prompt: Montar mi unidad de google drive\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\nMounted at /content/drive"
  },
  {
    "objectID": "posts/code/word_embeddings.html",
    "href": "posts/code/word_embeddings.html",
    "title": "Word Embeddings",
    "section": "",
    "text": "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nColab"
  },
  {
    "objectID": "posts/code/word_embeddings.html#representación-de-texto-como-números",
    "href": "posts/code/word_embeddings.html#representación-de-texto-como-números",
    "title": "Word Embeddings",
    "section": "Representación de texto como números",
    "text": "Representación de texto como números\nLos modelos de aprendizaje automático toman vectores (matrices de números) como entrada. Cuando se trabaja con texto, lo primero que debes hacer es idear una estrategia para convertir cadenas en números (o “vectorizar” el texto) antes de introducirlo en el modelo. En esta sección, veremos tres estrategias para hacerlo.\n\nCodificaciones One-hot\nComo primera idea, podrías codificar cada palabra en tu vocabulario con “one-hot”. Considera la frase “El gato se sentó en el tapete”. El vocabulario (o palabras únicas) en esta frase es (gato, tapete, en, sentó, el). Para representar cada palabra, crearás un vector cero con una longitud igual al vocabulario y luego colocarás un uno en el índice que corresponde a la palabra. Este enfoque se muestra en el siguiente diagrama.\nPara crear un vector que contenga la codificación de la frase, podrías concatenar los vectores one-hot para cada palabra.\nPunto clave: Este enfoque es ineficiente. Un vector codificado con one-hot es disperso (es decir, la mayoría de los índices son cero). Imagina que tienes 10000 palabras en el vocabulario. Para codificar cada palabra con one-hot, crearías un vector donde el 99.99% de los elementos son cero.\n\n\n\nCodifica cada palabra con un número único\nUn segundo enfoque que podrías intentar es codificar cada palabra usando un número único. Continuando con el ejemplo anterior, podrías asignar 1 a “gato”, 2 a “tapete”, y así sucesivamente. Luego, podrías codificar la frase “El gato se sentó en el tapete” como un vector denso como [5, 1, 4, 3, 5, 2]. Este enfoque es eficiente. En lugar de un vector disperso, ahora tienes uno denso (donde todos los elementos están llenos).\nSin embargo, hay dos desventajas en este enfoque:\n\nLa codificación de enteros es arbitraria (no captura ninguna relación entre palabras).\nUna codificación de enteros puede ser difícil de interpretar para un modelo. Un clasificador lineal, por ejemplo, aprende un solo peso para cada característica. Debido a que no existe una relación entre la similitud de dos palabras y la similitud de sus codificaciones, esta combinación de peso de característica no tiene sentido.\n\n\n\nWord embeddings\nLos word embeddings nos brindan una forma de usar una representación eficiente y densa en la que palabras similares tienen una codificación similar. Es importante destacar que no tienes que especificar esta codificación manualmente. Una incrustación (embedding) es un vector denso de valores de punto flotante (la longitud del vector es un parámetro que especificas). En lugar de especificar los valores para la incrustación manualmente, son parámetros entrenables (pesos aprendidos por el modelo durante el entrenamiento, de la misma manera que un modelo aprende pesos para una capa densa). Es común ver word embeddings que son de 8 dimensiones (para conjuntos de datos pequeños), hasta 1024 dimensiones cuando se trabaja con conjuntos de datos grandes. Una incrustación de mayor dimensión puede capturar relaciones detalladas entre palabras, pero se necesitan más datos para aprender.\n\nArriba hay un diagrama para una incrustación de palabras (word embedding). Cada palabra se representa como un vector de 4 dimensiones de valores de punto flotante. Otra forma de pensar en una incrustación (embedding) es como una “tabla de búsqueda”. Después de que se han aprendido estos pesos, puedes codificar cada palabra buscando el vector denso al que corresponde en la tabla."
  },
  {
    "objectID": "posts/code/word_embeddings.html#preparación",
    "href": "posts/code/word_embeddings.html#preparación",
    "title": "Word Embeddings",
    "section": "Preparación",
    "text": "Preparación\n\nimport io\nimport os\nimport re\nimport shutil\nimport string\nimport tensorflow as tf\n\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\nfrom tensorflow.keras.layers import TextVectorization\n\n\nDescargar el conjunto de datos IMDb\nUtilizaremos el Large Movie Review Dataset a lo largo del tutorial. Entrenarás un modelo clasificador de sentimientos en este conjunto de datos y en el proceso aprenderás word embeddings desde cero. Para leer más sobre cómo cargar un conjunto de datos desde cero, consulta el Loading text tutorial.\nDescarga el conjunto de datos usando la utilidad de archivos de Keras y echa un vistazo a los directorios.\n\nurl = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n\ndataset = tf.keras.utils.get_file(\"aclImdb_v1.tar.gz\", url,\n                                  untar=True, cache_dir='.',\n                                  cache_subdir='')\n\ndataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')\nos.listdir(dataset_dir)\n\nEcha un vistazo al directorio train/. Tiene carpetas pos y neg con reseñas de películas etiquetadas como positivas y negativas, respectivamente. Utilizarás las reseñas de las carpetas pos y neg para entrenar un modelo de clasificación binaria.\n\ntrain_dir = os.path.join(dataset_dir, 'train')\nos.listdir(train_dir)\n\nThe train directory also has additional folders which should be removed before creating training dataset.\n\nremove_dir = os.path.join(train_dir, 'unsup')\nshutil.rmtree(remove_dir)\n\nA continuación, crea un tf.data.Dataset usando tf.keras.utils.text_dataset_from_directory. Puedes leer más sobre el uso de esta utilidad en este text classification tutorial.\nUtiliza el directorio train para crear conjuntos de datos de entrenamiento y validación con una división del 20% para la validación.\n\nbatch_size = 1024\nseed = 123\ntrain_ds = tf.keras.utils.text_dataset_from_directory(\n    'aclImdb/train', batch_size=batch_size, validation_split=0.2,\n    subset='training', seed=seed)\nval_ds = tf.keras.utils.text_dataset_from_directory(\n    'aclImdb/train', batch_size=batch_size, validation_split=0.2,\n    subset='validation', seed=seed)\n\nEcha un vistazo a algunas reseñas de películas y sus etiquetas (1: positivo, 0: negativo) del conjunto de datos de entrenamiento.\n\nfor text_batch, label_batch in train_ds.take(1):\n  for i in range(5):\n    print(label_batch[i].numpy(), text_batch.numpy()[i])\n\n\n\nConfigurar el conjunto de datos para el rendimiento\nEstos son dos métodos importantes que debes usar al cargar datos para asegurarte de que la E/S no se bloquee.\n.cache() mantiene los datos en la memoria después de que se cargan desde el disco. Esto asegurará que el conjunto de datos no se convierta en un cuello de botella mientras se entrena tu modelo. Si tu conjunto de datos es demasiado grande para caber en la memoria, también puedes usar este método para crear una caché en disco de alto rendimiento, que es más eficiente de leer que muchos archivos pequeños.\n.prefetch() superpone el preprocesamiento de datos y la ejecución del modelo durante el entrenamiento.\nPuedes obtener más información sobre ambos métodos, así como sobre cómo almacenar en caché los datos en el disco en la data performance guide.\n\nAUTOTUNE = tf.data.AUTOTUNE\n\ntrain_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\nval_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
  },
  {
    "objectID": "posts/code/word_embeddings.html#usando-la-capa-embedding",
    "href": "posts/code/word_embeddings.html#usando-la-capa-embedding",
    "title": "Word Embeddings",
    "section": "Usando la capa Embedding",
    "text": "Usando la capa Embedding\nKeras facilita el uso de word embeddings. Echa un vistazo a la capa Embedding.\nLa capa Embedding se puede entender como una tabla de búsqueda que mapea desde índices enteros (que representan palabras específicas) a vectores densos (sus incrustaciones - embeddings). La dimensionalidad (o ancho) de la incrustación es un parámetro con el que puedes experimentar para ver qué funciona bien para tu problema, de la misma manera que experimentarías con la cantidad de neuronas en una capa densa.\n\n# Embed a 1,000 word vocabulary into 5 dimensions.\nembedding_layer = tf.keras.layers.Embedding(1000, 5)\n\nCuando creas una capa Embedding, los pesos para la incrustación se inicializan aleatoriamente (como cualquier otra capa). Durante el entrenamiento, se ajustan gradualmente mediante retropropagación. Una vez entrenados, los word embeddings aprendidos codificarán aproximadamente las similitudes entre las palabras (ya que se aprendieron para el problema específico en el que se entrena tu modelo).\nSi pasas un entero a una capa Embedding, el resultado reemplaza cada entero con el vector de la tabla de incrustaciones:\n\nresult = embedding_layer(tf.constant([1, 2, 3]))\nresult.numpy()\n\nPara problemas de texto o secuencia, la capa Embedding toma un tensor 2D de enteros, de forma (muestras, longitud_de_secuencia), donde cada entrada es una secuencia de enteros. Puede incrustar secuencias de longitudes variables. Podrías alimentar a la capa de incrustación anterior lotes con formas (32, 10) (lote de 32 secuencias de longitud 10) or (64, 15) (lote de 64 secuencias de longitud 15).\nEl tensor devuelto tiene un eje más que la entrada, los vectores de incrustación se alinean a lo largo del nuevo último eje. Pásale un lote de entrada (2, 3) y la salida es (2, 3, N)\n\nresult = embedding_layer(tf.constant([[0, 1, 2], [3, 4, 5]]))\nresult.shape\n\nCuando se le da un lote de secuencias como entrada, una capa Embedding devuelve un tensor de punto flotante 3D, de forma (muestras, longitud_de_secuencia, dimensionalidad_de_la_incrustación). Para convertir de esta secuencia de longitud variable a una representación fija, hay una variedad de enfoques estándar. Podrías usar una RNN, Atención o una capa de agrupación antes de pasarla a una capa Densa. Este tutorial utiliza la agrupación porque es la más simple. ElText Classification with an RNN es un buen siguiente paso."
  },
  {
    "objectID": "posts/code/word_embeddings.html#preprocesamiento-de-texto",
    "href": "posts/code/word_embeddings.html#preprocesamiento-de-texto",
    "title": "Word Embeddings",
    "section": "Preprocesamiento de texto",
    "text": "Preprocesamiento de texto\nA continuación, define los pasos de preprocesamiento del conjunto de datos necesarios para tu modelo de clasificación de sentimientos. Inicializa una capa TextVectorization con los parámetros deseados para vectorizar las reseñas de películas. Puedes obtener más información sobre el uso de esta capa en el tutorial Text Classification.\n\n# Create a custom standardization function to strip HTML break tags '&lt;br /&gt;'.\ndef custom_standardization(input_data):\n  lowercase = tf.strings.lower(input_data)\n  stripped_html = tf.strings.regex_replace(lowercase, '&lt;br /&gt;', ' ')\n  return tf.strings.regex_replace(stripped_html,\n                                  '[%s]' % re.escape(string.punctuation), '')\n\n\n# Vocabulary size and number of words in a sequence.\nvocab_size = 10000\nsequence_length = 100\n\n# Use the text vectorization layer to normalize, split, and map strings to\n# integers. Note that the layer uses the custom standardization defined above.\n# Set maximum_sequence length as all samples are not of the same length.\nvectorize_layer = TextVectorization(\n    standardize=custom_standardization,\n    max_tokens=vocab_size,\n    output_mode='int',\n    output_sequence_length=sequence_length)\n\n# Make a text-only dataset (no labels) and call adapt to build the vocabulary.\ntext_ds = train_ds.map(lambda x, y: x)\nvectorize_layer.adapt(text_ds)"
  },
  {
    "objectID": "posts/code/word_embeddings.html#crea-un-modelo-de-clasificación",
    "href": "posts/code/word_embeddings.html#crea-un-modelo-de-clasificación",
    "title": "Word Embeddings",
    "section": "Crea un modelo de clasificación",
    "text": "Crea un modelo de clasificación\nUtiliza la Keras Sequential API para definir el modelo de clasificación de sentimientos. En este caso, es un modelo de estilo “bolsa de palabras continua”. * La capa TextVectorization transforma cadenas en índices de vocabulario. Ya has inicializado vectorize_layer como una capa TextVectorization y has creado su vocabulario llamando a adapt en text_ds. Ahora vectorize_layer se puede utilizar como la primera capa de tu modelo de clasificación de extremo a extremo, alimentando cadenas transformadas en la capa. * La capa Embedding toma el vocabulario codificado en enteros y busca el vector de incrustación (embedding) para cada índice de palabra. Estos vectores se aprenden a medida que se entrena el modelo. Los vectores agregan una dimensión a la matriz de salida. Las dimensiones resultantes son: (batch, sequence, embedding).\n\nLa capa GlobalAveragePooling1D devuelve un vector de salida de longitud fija para cada ejemplo promediando sobre la dimensión de la secuencia. Esto permite que el modelo maneje la entrada de longitud variable, de la manera más simple posible.\nEl vector de salida de longitud fija se canaliza a través de una capa completamente conectada (Dense) con 16 unidades ocultas.\nLa última capa está densamente conectada con un solo nodo de salida.\n\nPrecaución: Este modelo no utiliza enmascaramiento, por lo que el relleno de ceros se utiliza como parte de la entrada y, por lo tanto, la longitud del relleno puede afectar la salida. Para solucionar esto, consulta masking and padding guide.\n\nembedding_dim=16\n\nmodel = Sequential([\n  vectorize_layer,\n  Embedding(vocab_size, embedding_dim, name=\"embedding\"),\n  GlobalAveragePooling1D(),\n  Dense(16, activation='relu'),\n  Dense(1)\n])"
  },
  {
    "objectID": "posts/code/word_embeddings.html#compila-y-entrena-el-modelo",
    "href": "posts/code/word_embeddings.html#compila-y-entrena-el-modelo",
    "title": "Word Embeddings",
    "section": "Compila y entrena el modelo",
    "text": "Compila y entrena el modelo\nUtilizarás TensorBoard ara visualizar métricas, incluidas la pérdida y la precisión. Crea un tf.keras.callbacks.TensorBoard.\n\ntensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")\n\nCompila y entrena el modelo utilizando el optimizador Adam y la pérdida BinaryCrossentropy.\n\nmodel.compile(optimizer='adam',\n              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n\n\nmodel.fit(\n    train_ds,\n    validation_data=val_ds,\n    epochs=15,\n    callbacks=[tensorboard_callback])\n\nCon este enfoque, el modelo alcanza una precisión de validación de alrededor del 78% (ten en cuenta que el modelo está sobreajustado ya que la precisión del entrenamiento es mayor).\nNota: Tus resultados pueden ser un poco diferentes, dependiendo de cómo se inicializaron aleatoriamente los pesos antes de entrenar la capa Embedding.\nPuedes consultar el resumen del modelo para obtener más información sobre cada capa del modelo.\n\nmodel.summary()\n\nVisualiza las métricas del modelo en TensorBoard.\n\n#docs_infra: no_execute\n%load_ext tensorboard\n%tensorboard --logdir logs\n\n\n\n\nembeddings_classifier_accuracy.png"
  },
  {
    "objectID": "posts/code/word_embeddings.html#recupere-las-incrustaciones-de-palabras-entrenadas-y-guárdelas-en-el-disco",
    "href": "posts/code/word_embeddings.html#recupere-las-incrustaciones-de-palabras-entrenadas-y-guárdelas-en-el-disco",
    "title": "Word Embeddings",
    "section": "Recupere las incrustaciones de palabras entrenadas y guárdelas en el disco",
    "text": "Recupere las incrustaciones de palabras entrenadas y guárdelas en el disco\nA continuación, recupere las incrustaciones de palabras aprendidas durante el entrenamiento. Las incrustaciones son pesos de la capa de incrustación en el modelo. La matriz de ponderaciones tiene la forma (vocab_size, embedding_dimension) .\nObtenga los pesos del modelo usando get_layer() y get_weights() . La función get_vocabulary() proporciona el vocabulario para crear un archivo de metadatos con un token por línea.\n\nweights = model.get_layer('embedding').get_weights()[0]\nvocab = vectorize_layer.get_vocabulary()\n\nEscriba los pesos en el disco. Para utilizar el proyector de incrustaciones , deberá cargar dos archivos en formato separado por tabulaciones: un archivo de vectores (que contiene la incrustación) y un archivo de metadatos (que contiene las palabras).\n\nout_v = io.open('vectors.tsv', 'w', encoding='utf-8')\nout_m = io.open('metadata.tsv', 'w', encoding='utf-8')\n\nfor index, word in enumerate(vocab):\n  if index == 0:\n    continue  # skip 0, it's padding.\n  vec = weights[index]\n  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n  out_m.write(word + \"\\n\")\nout_v.close()\nout_m.close()\n\nSi está ejecutando este tutorial en Colaboratory , puede usar el siguiente fragmento para descargar estos archivos a su máquina local (o usar el explorador de archivos, Ver -&gt; Tabla de contenido -&gt; Explorador de archivos ).\n\ntry:\n  from google.colab import files\n  files.download('vectors.tsv')\n  files.download('metadata.tsv')\nexcept Exception:\n  pass"
  },
  {
    "objectID": "posts/code/word_embeddings.html#visualiza-las-incrustaciones",
    "href": "posts/code/word_embeddings.html#visualiza-las-incrustaciones",
    "title": "Word Embeddings",
    "section": "Visualiza las incrustaciones",
    "text": "Visualiza las incrustaciones\nPara visualizar las incrustaciones, súbalas al proyector de incrustaciones.\nAbra el Embedding Projector (esto también se puede ejecutar en una instancia local de TensorBoard)).\n\nHaga clic en “Cargar datos”.\nCargue los dos archivos que creó anteriormente: vecs.tsv and meta.tsv.\n\nAhora se mostrarán las incrustaciones que ha entrenado. Puede buscar palabras para encontrar a sus vecinos más cercanos. Por ejemplo, intente buscar “hermoso”. Es posible que vea vecinos como “maravilloso”.\nNota: Experimentalmente, es posible que pueda producir incrustaciones más interpretables mediante el uso de un modelo más simple. Intente eliminar la capa Dense(16) , vuelva a entrenar el modelo y visualice las incrustaciones nuevamente.\nNota: Por lo general, se necesita un conjunto de datos mucho más grande para entrenar incrustaciones de palabras más interpretables. Este tutorial utiliza un pequeño conjunto de datos de IMDb con fines de demostración."
  },
  {
    "objectID": "posts/code/word_embeddings.html#próximos-pasos",
    "href": "posts/code/word_embeddings.html#próximos-pasos",
    "title": "Word Embeddings",
    "section": "Próximos pasos",
    "text": "Próximos pasos\nEste tutorial le ha mostrado cómo entrenar y visualizar incrustaciones de palabras desde cero en un pequeño conjunto de datos.\n\nPara entrenar incrustaciones de palabras usando el algoritmo de Word2Vec, pruebe el tutorial deWord2Vec .\nPara obtener más información sobre el procesamiento de texto avanzado, lea el modelo de Transformer para la comprensión del lenguaje."
  },
  {
    "objectID": "posts/Presentaciones/LLMs.html#índice",
    "href": "posts/Presentaciones/LLMs.html#índice",
    "title": "LARGE LANGUAGE MODELS (LLM) APLICADOS AL ENTORNO MÉDICO",
    "section": "Índice",
    "text": "Índice\n\nIntroducción breve a los LLM\nEmbedding Systems\nKnowledge Graphs\nFine-Tuning y Retrieval Augmentation Generation (RAG)\nLímites de los LLM\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/LLMs.html#llm",
    "href": "posts/Presentaciones/LLMs.html#llm",
    "title": "LARGE LANGUAGE MODELS (LLM) APLICADOS AL ENTORNO MÉDICO",
    "section": "LLM",
    "text": "LLM\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/LLMs.html#llm-pretraining",
    "href": "posts/Presentaciones/LLMs.html#llm-pretraining",
    "title": "LARGE LANGUAGE MODELS (LLM) APLICADOS AL ENTORNO MÉDICO",
    "section": "LLM Pretraining",
    "text": "LLM Pretraining\nA una velocidad de 1000 millones de operaciones por segundo entrenar a GPT3 nos costaría 100 millones de años. Se hubiese tenido que empezar a entrenar el modelo en el cretácico.\n\n\n\n\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/LLMs.html#llm-reinforcement-learning-with-human-feedback",
    "href": "posts/Presentaciones/LLMs.html#llm-reinforcement-learning-with-human-feedback",
    "title": "LARGE LANGUAGE MODELS (LLM) APLICADOS AL ENTORNO MÉDICO",
    "section": "LLM Reinforcement Learning with Human Feedback",
    "text": "LLM Reinforcement Learning with Human Feedback\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/LLMs.html#llm-gpu",
    "href": "posts/Presentaciones/LLMs.html#llm-gpu",
    "title": "LARGE LANGUAGE MODELS (LLM) APLICADOS AL ENTORNO MÉDICO",
    "section": "LLM GPU",
    "text": "LLM GPU\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/LLMs.html#transformers",
    "href": "posts/Presentaciones/LLMs.html#transformers",
    "title": "LARGE LANGUAGE MODELS (LLM) APLICADOS AL ENTORNO MÉDICO",
    "section": "TRANSFORMERS",
    "text": "TRANSFORMERS\n\n\n  \n\nCodificaciones posicionales: Los transformers agregan un número a cada palabra para indicar su posición en la oración. Esto ayuda al modelo a comprender el orden de las palabras, lo cual es crucial para el significado.\nAtención: Permite al modelo considerar todas las palabras de la oración al traducir o analizar una palabra específica. Esto ayuda a capturar relaciones complejas entre palabras y mejora la precisión de la traducción y la comprensión.\nAutoatención: Permite al modelo comprender una palabra en el contexto de las palabras que la rodean, lo que ayuda a desambiguar palabras con múltiples significados y captar matices del lenguaje.\n\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/LLMs.html#codificación-posicional",
    "href": "posts/Presentaciones/LLMs.html#codificación-posicional",
    "title": "LARGE LANGUAGE MODELS (LLM) APLICADOS AL ENTORNO MÉDICO",
    "section": "Codificación Posicional",
    "text": "Codificación Posicional\n\nProblema: A diferencia de las RNNs, los Transformers procesan palabras en paralelo, perdiendo información sobre el orden de las palabras.\nSolución: La codificación posicional añade información sobre la posición de cada palabra mediante vectores que se suman a los embeddings de las palabras.\nBeneficio: Permite al modelo distinguir oraciones con las mismas palabras en diferente orden, crucial para la interpretación del significado.\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/LLMs.html#mecanismo-de-atención-attention",
    "href": "posts/Presentaciones/LLMs.html#mecanismo-de-atención-attention",
    "title": "LARGE LANGUAGE MODELS (LLM) APLICADOS AL ENTORNO MÉDICO",
    "section": "Mecanismo de Atención (Attention)",
    "text": "Mecanismo de Atención (Attention)\n\nPropósito: Permite al modelo enfocarse en las partes más relevantes de la secuencia de entrada al generar la salida.\nFuncionamiento: Asigna pesos a cada palabra de la entrada, indicando cuánto debe “prestar atención” el modelo.\nCálculo: Se computan matrices Query (Q), Key (K) y Value (V). La atención se calcula como una función de Q y K, ponderando los valores en V para generar un vector de contexto.\nBeneficio: Mejora el rendimiento en tareas de secuencia a secuencia al proporcionar información contextualmente relevante.\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/LLMs.html#autoatención-self-attention",
    "href": "posts/Presentaciones/LLMs.html#autoatención-self-attention",
    "title": "LARGE LANGUAGE MODELS (LLM) APLICADOS AL ENTORNO MÉDICO",
    "section": "Autoatención (Self-Attention)",
    "text": "Autoatención (Self-Attention)\n\nPropósito: Permite a cada palabra prestar atención a todas las demás palabras dentro de la misma secuencia.\nFuncionamiento: Las matrices Q, K y V se calculan a partir de la misma secuencia de entrada.\nBeneficio: Permite el procesamiento paralelo y la captura de dependencias a largo alcance, superando las limitaciones de las RNNs.\nRelación con Atención: La autoatención es un caso particular del mecanismo de atención.\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/LLMs.html#embedding",
    "href": "posts/Presentaciones/LLMs.html#embedding",
    "title": "LARGE LANGUAGE MODELS (LLM) APLICADOS AL ENTORNO MÉDICO",
    "section": "EMBEDDING",
    "text": "EMBEDDING\n\nVector embeddings son representaciones numéricas de información, como texto, documentos, imágenes o audio. Capturan el significado semántico de la información\n\n\n\n\n\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/LLMs.html#vectores",
    "href": "posts/Presentaciones/LLMs.html#vectores",
    "title": "LARGE LANGUAGE MODELS (LLM) APLICADOS AL ENTORNO MÉDICO",
    "section": "VECTORES",
    "text": "VECTORES\n\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/LLMs.html#embedding-machine",
    "href": "posts/Presentaciones/LLMs.html#embedding-machine",
    "title": "LARGE LANGUAGE MODELS (LLM) APLICADOS AL ENTORNO MÉDICO",
    "section": "EMBEDDING MACHINE",
    "text": "EMBEDDING MACHINE\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/LLMs.html#chunks",
    "href": "posts/Presentaciones/LLMs.html#chunks",
    "title": "LARGE LANGUAGE MODELS (LLM) APLICADOS AL ENTORNO MÉDICO",
    "section": "CHUNKS",
    "text": "CHUNKS\n\nUn chunk es una unidad discreta de información extraída de un cuerpo de texto más grande. Puede ser una frase, un párrafo, una sección de un documento o incluso un documento completo, dependiendo del sistema y la aplicación. La idea principal es dividir la información en partes más pequeñas que sean más fáciles de procesar y analizar para la IA.\n\n\n\n\n\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/LLMs.html#métodos-de-división-en-chunks",
    "href": "posts/Presentaciones/LLMs.html#métodos-de-división-en-chunks",
    "title": "LARGE LANGUAGE MODELS (LLM) APLICADOS AL ENTORNO MÉDICO",
    "section": "Métodos de División en Chunks",
    "text": "Métodos de División en Chunks\n\nPor encabezados: Se usan los encabezados y subtítulos del documento para crear chunks.\nPor párrafos: Se divide el texto en chunks según los párrafos.\nVentanas deslizantes: Una ventana captura texto al desplazarse, creando chunks que se superponen.\nSegmentación semántica: Se usan algoritmos para identificar unidades de significado y dividir el texto en chunks.\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/LLMs.html#cómo-los-llms-utilizan-los-chunks",
    "href": "posts/Presentaciones/LLMs.html#cómo-los-llms-utilizan-los-chunks",
    "title": "LARGE LANGUAGE MODELS (LLM) APLICADOS AL ENTORNO MÉDICO",
    "section": "¿Cómo los LLMs Utilizan los Chunks?",
    "text": "¿Cómo los LLMs Utilizan los Chunks?\nLos LLMs utilizan los chunks para comprender el contexto y generar texto coherente. Al procesar un chunk, el LLM puede:\n\nIdentificar las ideas principales: Extraer la información más relevante del chunk.\nEstablecer conexiones: Relacionar la información del chunk con otros chunks o con el conocimiento previo del LLM. Para lograr esto, los LLMs utilizan mecanismos como la atención y la codificación posicional. La atención permite al modelo enfocarse en las partes más relevantes de la información dentro de un chunk y entre diferentes chunks. La codificación posicional proporciona información sobre la ubicación de las palabras dentro de un chunk y en relación con otros chunks, lo que ayuda al modelo a comprender el orden y la secuencia de la información.\nGenerar resúmenes: Condensar la información del chunk en un resumen conciso.\nResponder preguntas: Proporcionar respuestas precisas y relevantes a las preguntas basadas en la información del chunk.\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/LLMs.html#desafíos-de-los-chunks-en-llms",
    "href": "posts/Presentaciones/LLMs.html#desafíos-de-los-chunks-en-llms",
    "title": "LARGE LANGUAGE MODELS (LLM) APLICADOS AL ENTORNO MÉDICO",
    "section": "Desafíos de los Chunks en LLMs",
    "text": "Desafíos de los Chunks en LLMs\n\nPérdida de contexto: Dividir el texto en chunks puede dificultar la comprensión de relaciones entre datos, afectando la precisión de las respuestas.\nSesgo: La selección de chunks puede introducir sesgos, llevando a resultados incompletos o inexactos.\nInformación compleja: Cierta información (imágenes, datos, etc.) es difícil de representar en chunks.\nTamaño del chunk: Encontrar el tamaño óptimo es crucial para el rendimiento del modelo.\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/LLMs.html#word-embeddings-métodos-basados-en-frecuencia",
    "href": "posts/Presentaciones/LLMs.html#word-embeddings-métodos-basados-en-frecuencia",
    "title": "LARGE LANGUAGE MODELS (LLM) APLICADOS AL ENTORNO MÉDICO",
    "section": "Word Embeddings: Métodos Basados en Frecuencia",
    "text": "Word Embeddings: Métodos Basados en Frecuencia\n\nBag-of-Words (BoW): Conjunto no ordenado de palabras y sus frecuencias. Simple pero ignora el orden y la semántica.\nTF-IDF: Asigna pesos a las palabras según su frecuencia en un documento y en el corpus. Mejora BoW pero ignora el contexto.\nN-gramas: Considera secuencias de n palabras. Captura contexto local pero basado en frecuencia.\nMatrices de Co-ocurrencia: Capturan la frecuencia con la que las palabras aparecen juntas. Base para generar embeddings.\nOne-Hot Encoding: Vector binario para cada palabra. Sencillo pero de alta dimensionalidad y sin relaciones semánticas.\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/LLMs.html#word-embeddings-embeddings-estáticos",
    "href": "posts/Presentaciones/LLMs.html#word-embeddings-embeddings-estáticos",
    "title": "LARGE LANGUAGE MODELS (LLM) APLICADOS AL ENTORNO MÉDICO",
    "section": "Word Embeddings: Embeddings Estáticos",
    "text": "Word Embeddings: Embeddings Estáticos\nGeneran un vector único por palabra, independientemente del contexto.\n\nWord2Vec (CBOW y Skip-gram): Redes neuronales para predecir palabras a partir de su contexto.\nGloVe: Combina métodos de conteo y predicción, capturando relaciones semánticas locales y globales.\nFastText: Mejora Word2Vec considerando la estructura interna de las palabras.\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/LLMs.html#word-embeddings-embeddings-contextuales",
    "href": "posts/Presentaciones/LLMs.html#word-embeddings-embeddings-contextuales",
    "title": "LARGE LANGUAGE MODELS (LLM) APLICADOS AL ENTORNO MÉDICO",
    "section": "Word Embeddings: Embeddings Contextuales",
    "text": "Word Embeddings: Embeddings Contextuales\nGeneran embeddings que varían según el contexto, capturando la polisemia.\n\nSelf-Attention: Permite a cada palabra relacionarse con todas las demás. Fundamental en Transformers.\nELMo: Considera todo el contexto de la oración para embeddings dinámicos.\nBERT: Arquitectura Transformer para embeddings contextuales, capturando contexto izquierdo y derecho.\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/LLMs.html#word-embeddings-embeddings-contextuales-1",
    "href": "posts/Presentaciones/LLMs.html#word-embeddings-embeddings-contextuales-1",
    "title": "LARGE LANGUAGE MODELS (LLM) APLICADOS AL ENTORNO MÉDICO",
    "section": "Word Embeddings: Embeddings Contextuales",
    "text": "Word Embeddings: Embeddings Contextuales\nGeneran embeddings que varían según el contexto, capturando la polisemia.\n\nSelf-Attention: Permite a cada palabra relacionarse con todas las demás. Fundamental en Transformers.\nELMo: Considera todo el contexto de la oración para embeddings dinámicos.\nBERT: Arquitectura Transformer para embeddings contextuales, capturando contexto izquierdo y derecho.\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/LLMs.html#qué-son-los-kg",
    "href": "posts/Presentaciones/LLMs.html#qué-son-los-kg",
    "title": "LARGE LANGUAGE MODELS (LLM) APLICADOS AL ENTORNO MÉDICO",
    "section": "¿QUÉ SON LOS KG?",
    "text": "¿QUÉ SON LOS KG?\n\nLos grafos de conocimiento son bases de datos inteligentes que representan el conocimiento médico de manera estructurada, permitiendo a la IA comprender y razonar sobre la información de forma más efectiva.\n\n\n\n\nLord of the rings\n\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/LLMs.html#componentes-principales-de-un-grafo-de-conocimiento",
    "href": "posts/Presentaciones/LLMs.html#componentes-principales-de-un-grafo-de-conocimiento",
    "title": "LARGE LANGUAGE MODELS (LLM) APLICADOS AL ENTORNO MÉDICO",
    "section": "Componentes principales de un grafo de conocimiento",
    "text": "Componentes principales de un grafo de conocimiento\n\n\n\n\n\nNodos: Representan entidades, conceptos u objetos, como enfermedades, síntomas, genes o medicamentos.\nAristas: Representan las relaciones entre los nodos, como “causa”, “trata”, “se asocia con” o “es un tipo de”.\nEtiquetas: Proporcionan información adicional sobre los nodos y las aristas, como nombres, descripciones o atributos.\n\n\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/LLMs.html#beneficios-de-los-grafos-de-conocimiento",
    "href": "posts/Presentaciones/LLMs.html#beneficios-de-los-grafos-de-conocimiento",
    "title": "LARGE LANGUAGE MODELS (LLM) APLICADOS AL ENTORNO MÉDICO",
    "section": "Beneficios de los Grafos de Conocimiento",
    "text": "Beneficios de los Grafos de Conocimiento\n\n\n\n\n\nPrecisión en el diagnóstico: Integración de información de diversas fuentes.\nPersonalización de tratamientos: Uso de información genómica, clínica y de estilo de vida.\nDescubrimiento de fármacos: Identificación de nuevas relaciones entre genes, proteínas y enfermedades.\n\n\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/LLMs.html#limitaciones-de-los-grafos-de-conocimiento",
    "href": "posts/Presentaciones/LLMs.html#limitaciones-de-los-grafos-de-conocimiento",
    "title": "LARGE LANGUAGE MODELS (LLM) APLICADOS AL ENTORNO MÉDICO",
    "section": "Limitaciones de los Grafos de Conocimiento",
    "text": "Limitaciones de los Grafos de Conocimiento\n\n\n\n\n\nCalidad de los datos: La precisión y completitud son cruciales.\nEscalabilidad: Construcción y mantenimiento a gran escala pueden ser complejos.\nInteroperabilidad: Integración con otros sistemas y bases de datos puede ser un desafío.\n\n\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/LLMs.html#fine-tunning",
    "href": "posts/Presentaciones/LLMs.html#fine-tunning",
    "title": "LARGE LANGUAGE MODELS (LLM) APLICADOS AL ENTORNO MÉDICO",
    "section": "FINE-TUNNING",
    "text": "FINE-TUNNING\n\nEl ajuste fino de un LLM es una técnica de aprendizaje por transferencia en la que se toma un modelo pre-entrenado con un gran conjunto de datos para una tarea general, y se realizan pequeños ajustes a sus parámetros internos para optimizar su rendimiento en una nueva tarea específica\n\n\n\n\n\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/LLMs.html#transfer-learning",
    "href": "posts/Presentaciones/LLMs.html#transfer-learning",
    "title": "LARGE LANGUAGE MODELS (LLM) APLICADOS AL ENTORNO MÉDICO",
    "section": "TRANSFER LEARNING",
    "text": "TRANSFER LEARNING\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/LLMs.html#fine-tunning-esquema",
    "href": "posts/Presentaciones/LLMs.html#fine-tunning-esquema",
    "title": "LARGE LANGUAGE MODELS (LLM) APLICADOS AL ENTORNO MÉDICO",
    "section": "FINE-TUNNING (ESQUEMA)",
    "text": "FINE-TUNNING (ESQUEMA)\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/LLMs.html#fine-tunning-esquema-1",
    "href": "posts/Presentaciones/LLMs.html#fine-tunning-esquema-1",
    "title": "LARGE LANGUAGE MODELS (LLM) APLICADOS AL ENTORNO MÉDICO",
    "section": "FINE-TUNNING (ESQUEMA)",
    "text": "FINE-TUNNING (ESQUEMA)\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/LLMs.html#fine-tuning-demostración",
    "href": "posts/Presentaciones/LLMs.html#fine-tuning-demostración",
    "title": "LARGE LANGUAGE MODELS (LLM) APLICADOS AL ENTORNO MÉDICO",
    "section": "FINE-TUNING (DEMOSTRACIÓN)",
    "text": "FINE-TUNING (DEMOSTRACIÓN)\n\n\n\n\n\nBioMistral/BioMistral-7B\nmistralai/Mistral-7B-Instruct-v0.1\n\nEjemplo\n\n\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/LLMs.html#rag-2020",
    "href": "posts/Presentaciones/LLMs.html#rag-2020",
    "title": "LARGE LANGUAGE MODELS (LLM) APLICADOS AL ENTORNO MÉDICO",
    "section": "RAG (2020)",
    "text": "RAG (2020)\n\nVectorización del contenido: Cada fragmento de texto se convierte en un vector numérico utilizando un modelo de embeddings. Estos vectores representan el significado semántico del texto en un espacio multidimensional.\nBúsqueda de similitud: Cuando se realiza una consulta, esta también se vectoriza, y se busca en la base de datos vectorial los fragmentos cuyos vectores sean más cercanos al de la consulta (similaridad coseno, distancia euclidiana, etc.).\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/LLMs.html#esquema-rag",
    "href": "posts/Presentaciones/LLMs.html#esquema-rag",
    "title": "LARGE LANGUAGE MODELS (LLM) APLICADOS AL ENTORNO MÉDICO",
    "section": "ESQUEMA RAG",
    "text": "ESQUEMA RAG\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/LLMs.html#fine-tunning-vs-rag",
    "href": "posts/Presentaciones/LLMs.html#fine-tunning-vs-rag",
    "title": "LARGE LANGUAGE MODELS (LLM) APLICADOS AL ENTORNO MÉDICO",
    "section": "FINE-TUNNING VS RAG",
    "text": "FINE-TUNNING VS RAG\nhttps://notebooklm.google.com/notebook/d40fef92-ed62-4659-9b3e-0e5cde949f45 https://notebooklm.google.com/notebook/35c43bd2-d012-45fb-a514-aa2958b1fdda\n\n\n\n\nCaracterística\n\n\nFine-tuning\n\n\nRAG\n\n\n\n\nEnfoque principal\n\n\nAdaptación del modelo\n\n\nAumento de la información\n\n\n\n\nMétodo\n\n\nAjuste de parámetros\n\n\nRecuperación de información externa\n\n\n\n\nVentajas\n\n\nPersonalización del modelo, eficiencia\n\n\nRespuestas contextualmente relevantes, precisión\n\n\n\n\nLimitaciones\n\n\nDificultad con datos cambiantes, adaptación de estilo limitada\n\n\nAdaptación de estilo limitada, enfoque en recuperación\n\n\n\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/LLMs.html#fine-tunning-vs-rag-1",
    "href": "posts/Presentaciones/LLMs.html#fine-tunning-vs-rag-1",
    "title": "LARGE LANGUAGE MODELS (LLM) APLICADOS AL ENTORNO MÉDICO",
    "section": "FINE-TUNNING VS RAG",
    "text": "FINE-TUNNING VS RAG\n\n\n\n\n\n\nFinne-tunning\n\n\n\n\n \n\n\n\n\n\nRAG\n\n\n\n\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/LLMs.html#ejemplo-rag",
    "href": "posts/Presentaciones/LLMs.html#ejemplo-rag",
    "title": "LARGE LANGUAGE MODELS (LLM) APLICADOS AL ENTORNO MÉDICO",
    "section": "Ejemplo RAG",
    "text": "Ejemplo RAG\nPregunta:Basándote en la historia clínica proporcionada, ¿cuáles son los tres factores de riesgo cardiovascular más relevantes para la paciente María Pérez y por qué son importantes en este caso específico?\nHistoria mal estructurada\nHistoria bien estructurada"
  },
  {
    "objectID": "posts/Presentaciones/LLMs.html#límites-de-los-llm-clásicos",
    "href": "posts/Presentaciones/LLMs.html#límites-de-los-llm-clásicos",
    "title": "LARGE LANGUAGE MODELS (LLM) APLICADOS AL ENTORNO MÉDICO",
    "section": "Límites de los LLM (Clásicos)",
    "text": "Límites de los LLM (Clásicos)\n\nFalta de comprensión del mundo real: Un LLM podría tener dificultades para responder preguntas sobre eventos actuales, lugares geográficos o conceptos abstractos.\nSesgo: Los LLMs pueden perpetuar y amplificar los sesgos presentes en los datos de entrenamiento. Esto puede resultar en respuestas discriminatorias, ofensivas o estereotipadas. Es importante abordar el sesgo en los LLMs para garantizar la equidad y la inclusión en las aplicaciones de IA.\n\nIncapacidad para razonar: Un LLM podría tener dificultades para resolver un problema matemático o para comprender las relaciones causales entre diferentes eventos.\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/LLMs.html#límites-de-los-llm-clásicos-ii",
    "href": "posts/Presentaciones/LLMs.html#límites-de-los-llm-clásicos-ii",
    "title": "LARGE LANGUAGE MODELS (LLM) APLICADOS AL ENTORNO MÉDICO",
    "section": "Límites de los LLM (Clásicos) II",
    "text": "Límites de los LLM (Clásicos) II\nDificultad para manejar información nueva o inesperada: Los LLMs se basan en los datos con los que fueron entrenados y pueden tener dificultades para procesar información nueva o inesperada. Esto puede llevar a respuestas inexactas o irrelevantes cuando se enfrentan a situaciones novedosas.\nDificultad con el lenguaje complejo: Los LLMs pueden tener dificultades para comprender y procesar el lenguaje complejo o matizado, como el sarcasmo, las metáforas y otras figuras retóricas..\nLimitaciones en la creatividad: Los LLMs pueden generar diferentes tipos de contenido creativo de formato textual, como poemas, guiones, piezas musicales y correos electrónicos. Sin embargo, su capacidad para producir contenido verdaderamente original y creativo es limitada.\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/Procesamiento_datos.html#índice",
    "href": "posts/Presentaciones/Procesamiento_datos.html#índice",
    "title": "Procesamiento de prospectos",
    "section": "Índice",
    "text": "Índice\n\nPlanteamiento del problema\nExtracción de datos\nUNSTRUCTURED TO STRUCTURED\nDB vectorial: Pinecone\nDB grafos: Neo4j Aura\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/Procesamiento_datos.html#página-cima",
    "href": "posts/Presentaciones/Procesamiento_datos.html#página-cima",
    "title": "Procesamiento de prospectos",
    "section": "Página CIMA",
    "text": "Página CIMA\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/Procesamiento_datos.html#qué-es-una-api",
    "href": "posts/Presentaciones/Procesamiento_datos.html#qué-es-una-api",
    "title": "Procesamiento de prospectos",
    "section": "¿Qué es una API?",
    "text": "¿Qué es una API?\n\nAPI son las siglas de Application Programming Interface (Interfaz de Programación de Aplicaciones).\nEs un conjunto de reglas y especificaciones que permiten que diferentes aplicaciones de software se comuniquen e intercambien datos entre sí.\nLas APIs actúan como intermediarios, facilitando la interacción entre sistemas que de otro modo serían incompatibles.\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/Procesamiento_datos.html#cómo-funcionan-las-apis",
    "href": "posts/Presentaciones/Procesamiento_datos.html#cómo-funcionan-las-apis",
    "title": "Procesamiento de prospectos",
    "section": "¿Cómo funcionan las APIs?",
    "text": "¿Cómo funcionan las APIs?\n\nUna aplicación (el cliente) realiza una solicitud a través de la API a otra aplicación (el servidor).\nEl servidor recibe la solicitud, la procesa y envía una respuesta con los datos solicitados.\nEl cliente recibe la respuesta y puede utilizar los datos para realizar diversas funciones.\n\nEjemplo: Cuando utilizas una aplicación de viajes compartidos, la API de Google Maps se utiliza para mostrar el mapa y calcular la ruta. La API de la aplicación de viajes compartidos se encarga de gestionar las reservas y los pagos.\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/Procesamiento_datos.html#cima-rest-api",
    "href": "posts/Presentaciones/Procesamiento_datos.html#cima-rest-api",
    "title": "Procesamiento de prospectos",
    "section": "CIMA REST API",
    "text": "CIMA REST API\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/Procesamiento_datos.html#diseño-del-colab",
    "href": "posts/Presentaciones/Procesamiento_datos.html#diseño-del-colab",
    "title": "Procesamiento de prospectos",
    "section": "Diseño del Colab",
    "text": "Diseño del Colab\n\n\n\n\nDiálogo 1 con Gemini\n\n\n\nBúsqueda de paracetamol página 1\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/Procesamiento_datos.html#primer-json-prospectos",
    "href": "posts/Presentaciones/Procesamiento_datos.html#primer-json-prospectos",
    "title": "Procesamiento de prospectos",
    "section": "Primer JSON: Prospectos",
    "text": "Primer JSON: Prospectos\nFichero obtenido\n\nQuizá recordar que se podrían almacenar en una NoSQL de las de clave-valor. Aunque de poco sirve."
  },
  {
    "objectID": "posts/Presentaciones/Procesamiento_datos.html#buscando-la-estructura",
    "href": "posts/Presentaciones/Procesamiento_datos.html#buscando-la-estructura",
    "title": "Procesamiento de prospectos",
    "section": "Buscando la estructura",
    "text": "Buscando la estructura\n\n\n\n\nDiálogo 2 con Gemini\n\n\nProyecto baml\n\n\n\nDiálogo 3 con Gemini"
  },
  {
    "objectID": "posts/Presentaciones/Procesamiento_datos.html#iterando-por-todos-los-prospectos",
    "href": "posts/Presentaciones/Procesamiento_datos.html#iterando-por-todos-los-prospectos",
    "title": "Procesamiento de prospectos",
    "section": "Iterando por todos los prospectos",
    "text": "Iterando por todos los prospectos\n\n\n\n\nDiálogo 4 con Gemini\n\n\n\nFichero obtenido"
  },
  {
    "objectID": "posts/Presentaciones/Procesamiento_datos.html#nodesedges",
    "href": "posts/Presentaciones/Procesamiento_datos.html#nodesedges",
    "title": "Procesamiento de prospectos",
    "section": "Nodes&Edges",
    "text": "Nodes&Edges"
  },
  {
    "objectID": "pre-post/code/Neumonia.html",
    "href": "pre-post/code/Neumonia.html",
    "title": "Clasificador de Neumonia (ViT)",
    "section": "",
    "text": "Colab\n\nDetector de neumonía\n\n\n\nEsquema del Vision Transformer Model\n\n\n\nfrom google.colab import userdata\naccess_token = userdata.get('tokenHF')\n\n\nimport os\n\ndef descargar_imagen_desde_url(url, nombre_archivo):\n    try:\n        # Utilizar la instrucción curl para descargar la imagen\n        os.system(f\"curl -o {nombre_archivo} -L -H 'Accept: image/jpeg' {url}\")\n        print(f\"La imagen ha sido descargada y guardada como '{nombre_archivo}'.\")\n    except Exception as e:\n        print(f\"Error: {e}\")\n\n\n# Use a pipeline as a high-level helper\nfrom transformers import pipeline\n\npipe = pipeline(\"image-classification\", model=\"nickmuchi/vit-finetuned-chest-xray-pneumonia\",token=access_token)\n\n\n\n\n\n\n\n/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n\n\n\n\n\nCould not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n\n\n\nn=1\np=3\nN=\"https://raw.githubusercontent.com/aotal/ImagenMedica/master/posts/code/data/clasificatorio/normal/N\"+str(n)+\".jpeg\"\nP=\"https://raw.githubusercontent.com/aotal/ImagenMedica/master/posts/code/data/clasificatorio/neumo/P\"+str(p)+\".jpeg\"\n\n\ndescargar_imagen_desde_url(N, \"imagen1.jpg\")\ndescargar_imagen_desde_url(P, \"imagen2.jpg\")\n\nLa imagen ha sido descargada y guardada como 'imagen1.jpg'.\nLa imagen ha sido descargada y guardada como 'imagen2.jpg'.\n\n\n\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\n# Cargar las imágenes\nimagen1 = Image.open(\"imagen1.jpg\")\nimagen2 = Image.open(\"imagen2.jpg\")\n\n# Crear una figura y ejes de subtramas\nfig, axs = plt.subplots(1, 2, figsize=(10, 5))\n\n# Mostrar la primera imagen en el primer eje de subtrama\naxs[0].imshow(imagen1,cmap=\"gray\")\naxs[0].axis('off')  # Desactivar los ejes\n\n# Mostrar la segunda imagen en el segundo eje de subtrama\naxs[1].imshow(imagen2,cmap=\"gray\")\naxs[1].axis('off')  # Desactivar los ejes\n\n# Ajustar el espacio entre las subtramas\nplt.subplots_adjust(wspace=0.2)\n\n# Mostrar la figura\nplt.show()\n\n\n\n\n\n\n\n\n\npipe('imagen1.jpg')\n\n[{'label': 'NORMAL', 'score': 0.9813147187232971},\n {'label': 'PNEUMONIA', 'score': 0.01868528686463833}]\n\n\n\npipe('imagen2.jpg')\n\n[{'label': 'PNEUMONIA', 'score': 0.9920761585235596},\n {'label': 'NORMAL', 'score': 0.007923857308924198}]"
  },
  {
    "objectID": "pre-post/code/unet_segmentation.html",
    "href": "pre-post/code/unet_segmentation.html",
    "title": "U-NET",
    "section": "",
    "text": "Colab"
  },
  {
    "objectID": "pre-post/code/unet_segmentation.html#original-article-of-nikhil-tomar-on-github.",
    "href": "pre-post/code/unet_segmentation.html#original-article-of-nikhil-tomar-on-github.",
    "title": "U-NET",
    "section": "Original article of Nikhil Tomar on GitHub.",
    "text": "Original article of Nikhil Tomar on GitHub.\n\n#@title Unet segmentation in Keras TensorFlow\n#@markdown Video explanation:\nfrom IPython.display import YouTubeVideo\nYouTubeVideo('M3EZS__Z_XE', width=600, height=400)\n\n\n        \n        \n\n\nPaper Arxiv Link: U-Net: Convolutional Networks for Biomedical Image Segmentation\n\n\nUNet is a fully convolutional network(FCN) that does image segmentation. Its goal is to predict each pixel’s class.\n\n\nUNet is built upon the FCN and modified in a way that it yields better segmentation in medical imaging."
  },
  {
    "objectID": "pre-post/code/unet_segmentation.html#architecture",
    "href": "pre-post/code/unet_segmentation.html#architecture",
    "title": "U-NET",
    "section": "1.1 Architecture",
    "text": "1.1 Architecture\n\n\nUNet Architecture has 3 parts:\n\n\n\nThe Contracting/Downsampling Path\n\n\nBottleneck\n\n\nThe Expanding/Upsampling Path\n\n\n\nDownsampling Path:\n\n\n\nIt consists of two 3x3 convolutions (unpadded convolutions), each followed by a rectified linear unit (ReLU) and a 2x2 max pooling operation with stride 2 for downsampling.\n\n\nAt each downsampling step we double the number of feature channels.\n\n\n\nUpsampling Path:\n\n\n\nEvery step in the expansive path consists of an upsampling of the feature map followed by a 2x2 convolution (“up-convolution”), a concatenation with the correspondingly feature map from the downsampling path, and two 3x3 convolutions, each followed by a ReLU.\n\n\n\nSkip Connection:\n\nThe skip connection from the downsampling path are concatenated with feature map during upsampling path. These skip connection provide local information to global information while upsampling.\n\nFinal Layer:\n\nAt the final layer a 1x1 convolution is used to map each feature vector to the desired number of classes."
  },
  {
    "objectID": "pre-post/code/unet_segmentation.html#advantages",
    "href": "pre-post/code/unet_segmentation.html#advantages",
    "title": "U-NET",
    "section": "1.2 Advantages",
    "text": "1.2 Advantages\n\nAdvantages:\n\n\n\nThe UNet combines the location information from the downsampling path to finally obtain a general information combining localisation and context, which is necessary to predict a good segmentation map.\n\n\nNo Dense layer is used, so image sizes can be used."
  },
  {
    "objectID": "pre-post/code/unet_segmentation.html#dataset",
    "href": "pre-post/code/unet_segmentation.html#dataset",
    "title": "U-NET",
    "section": "1.3 Dataset",
    "text": "1.3 Dataset\nLink: Data Science Bowl 2018 Find the nuclei in divergent images to advance medical discovery\n\n# Get file data-science-bowl-2018/stage1_train.zip\n# Download it from here: https://www.kaggle.com/c/8089/download-all\n# And upload stage1_train.zip to the Google Colab\nfrom google.colab import files\nuploaded = files.upload()\n\n\n     \n     \n      Upload widget is only available when the cell has been executed in the\n      current browser session. Please rerun this cell to enable.\n      \n       \n\n\nSaving stage1_train.zip to stage1_train.zip\n\n\n\n# Unzip it\n!mkdir stage1_train  # create directory\n!unzip stage1_train.zip -d stage1_train  # unzip into 'stage1_train' dir\n!rm stage1_train.zip  # delete 'stage1_train.zip' file"
  },
  {
    "objectID": "pre-post/code/unet_segmentation.html#code",
    "href": "pre-post/code/unet_segmentation.html#code",
    "title": "U-NET",
    "section": "1.4 Code",
    "text": "1.4 Code\n\n## Imports\nimport os\nimport sys\nimport random\n\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nfrom tensorflow import keras\n\n## Seeding\nseed = 42\nrandom.seed = seed\nnp.random.seed = seed\ntf.seed = seed"
  },
  {
    "objectID": "pre-post/code/unet_segmentation.html#data-generator",
    "href": "pre-post/code/unet_segmentation.html#data-generator",
    "title": "U-NET",
    "section": "Data Generator",
    "text": "Data Generator\n\nclass DataGen(keras.utils.Sequence):\n    def __init__(self, ids, path, batch_size=8, image_size=128):\n        self.ids = ids\n        self.path = path\n        self.batch_size = batch_size\n        self.image_size = image_size\n        self.on_epoch_end()\n\n    def __load__(self, id_name):\n        ## Path\n        image_path = os.path.join(self.path, id_name, \"images\", id_name) + \".png\"\n        mask_path = os.path.join(self.path, id_name, \"masks/\")\n        all_masks = os.listdir(mask_path)\n\n        ## Reading Image\n        image = cv2.imread(image_path, 1)\n        image = cv2.resize(image, (self.image_size, self.image_size))\n\n        mask = np.zeros((self.image_size, self.image_size, 1))\n\n        ## Reading Masks\n        for name in all_masks:\n            _mask_path = mask_path + name\n            _mask_image = cv2.imread(_mask_path, -1)\n            _mask_image = cv2.resize(_mask_image, (self.image_size, self.image_size)) #128x128\n            _mask_image = np.expand_dims(_mask_image, axis=-1)\n            mask = np.maximum(mask, _mask_image)\n\n        ## Normalizing\n        image = image/255.0\n        mask = mask/255.0\n\n        return image, mask\n\n    def __getitem__(self, index):\n        if(index+1)*self.batch_size &gt; len(self.ids):\n            self.batch_size = len(self.ids) - index*self.batch_size\n\n        files_batch = self.ids[index*self.batch_size : (index+1)*self.batch_size]\n\n        image = []\n        mask  = []\n\n        for id_name in files_batch:\n            _img, _mask = self.__load__(id_name)\n            image.append(_img)\n            mask.append(_mask)\n\n        image = np.array(image)\n        mask  = np.array(mask)\n\n        return image, mask\n\n    def on_epoch_end(self):\n        pass\n\n    def __len__(self):\n        return int(np.ceil(len(self.ids)/float(self.batch_size)))"
  },
  {
    "objectID": "pre-post/code/unet_segmentation.html#hyperparameters",
    "href": "pre-post/code/unet_segmentation.html#hyperparameters",
    "title": "U-NET",
    "section": "Hyperparameters",
    "text": "Hyperparameters\n\nimage_size = 128\ntrain_path = \"stage1_train/\"\nepochs = 10\nbatch_size = 8\n\n## Training Ids\ntrain_ids = next(os.walk(train_path))[1]\n\n## Validation Data Size\nval_data_size = 10\n\nvalid_ids = train_ids[:val_data_size]\ntrain_ids = train_ids[val_data_size:]\n\n\ngen = DataGen(train_ids, train_path, batch_size=batch_size, image_size=image_size)\nx, y = gen.__getitem__(0)\nprint(x.shape, y.shape)\n\n(8, 128, 128, 3) (8, 128, 128, 1)\n\n\n\nr = random.randint(0, len(x)-1)\n\nfig = plt.figure()\nfig.subplots_adjust(hspace=0.4, wspace=0.4)\nax = fig.add_subplot(1, 2, 1)\nax.imshow(x[r])\nax = fig.add_subplot(1, 2, 2)\nax.imshow(np.reshape(y[r], (image_size, image_size)), cmap=\"gray\")"
  },
  {
    "objectID": "pre-post/code/unet_segmentation.html#different-convolutional-blocks",
    "href": "pre-post/code/unet_segmentation.html#different-convolutional-blocks",
    "title": "U-NET",
    "section": "Different Convolutional Blocks",
    "text": "Different Convolutional Blocks\n\ndef down_block(x, filters, kernel_size=(3, 3), padding=\"same\", strides=1):\n    c = keras.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(x)\n    c = keras.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(c)\n    p = keras.layers.MaxPool2D((2, 2), (2, 2))(c)\n    return c, p\n\ndef up_block(x, skip, filters, kernel_size=(3, 3), padding=\"same\", strides=1):\n    us = keras.layers.UpSampling2D((2, 2))(x)\n    concat = keras.layers.Concatenate()([us, skip])\n    c = keras.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(concat)\n    c = keras.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(c)\n    return c\n\ndef bottleneck(x, filters, kernel_size=(3, 3), padding=\"same\", strides=1):\n    c = keras.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(x)\n    c = keras.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(c)\n    return c"
  },
  {
    "objectID": "pre-post/code/unet_segmentation.html#unet-model",
    "href": "pre-post/code/unet_segmentation.html#unet-model",
    "title": "U-NET",
    "section": "UNet Model",
    "text": "UNet Model\n\ndef UNet():\n    f = [16, 32, 64, 128, 256]\n    inputs = keras.layers.Input((image_size, image_size, 3))\n\n    p0 = inputs\n    c1, p1 = down_block(p0, f[0])  # 128 --&gt; 64\n    c2, p2 = down_block(p1, f[1])  # 64  --&gt; 32\n    c3, p3 = down_block(p2, f[2])  # 32  --&gt; 16\n    c4, p4 = down_block(p3, f[3])  # 16  --&gt; 8\n\n    bn = bottleneck(p4, f[4])\n\n    u1 = up_block(bn, c4, f[3])  # 8  --&gt; 16\n    u2 = up_block(u1, c3, f[2])  # 16 --&gt; 32\n    u3 = up_block(u2, c2, f[1])  # 32 --&gt; 64\n    u4 = up_block(u3, c1, f[0])  # 64 --&gt; 128\n\n    outputs = keras.layers.Conv2D(1, (1, 1), padding=\"same\", activation=\"sigmoid\")(u4)\n    model = keras.models.Model(inputs, outputs)\n    return model\n\n\nmodel = UNet()\nmodel.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\nmodel.summary()\n\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_1 (InputLayer)            (None, 128, 128, 3)  0                                            \n__________________________________________________________________________________________________\nconv2d (Conv2D)                 (None, 128, 128, 16) 448         input_1[0][0]                    \n__________________________________________________________________________________________________\nconv2d_1 (Conv2D)               (None, 128, 128, 16) 2320        conv2d[0][0]                     \n__________________________________________________________________________________________________\nmax_pooling2d (MaxPooling2D)    (None, 64, 64, 16)   0           conv2d_1[0][0]                   \n__________________________________________________________________________________________________\nconv2d_2 (Conv2D)               (None, 64, 64, 32)   4640        max_pooling2d[0][0]              \n__________________________________________________________________________________________________\nconv2d_3 (Conv2D)               (None, 64, 64, 32)   9248        conv2d_2[0][0]                   \n__________________________________________________________________________________________________\nmax_pooling2d_1 (MaxPooling2D)  (None, 32, 32, 32)   0           conv2d_3[0][0]                   \n__________________________________________________________________________________________________\nconv2d_4 (Conv2D)               (None, 32, 32, 64)   18496       max_pooling2d_1[0][0]            \n__________________________________________________________________________________________________\nconv2d_5 (Conv2D)               (None, 32, 32, 64)   36928       conv2d_4[0][0]                   \n__________________________________________________________________________________________________\nmax_pooling2d_2 (MaxPooling2D)  (None, 16, 16, 64)   0           conv2d_5[0][0]                   \n__________________________________________________________________________________________________\nconv2d_6 (Conv2D)               (None, 16, 16, 128)  73856       max_pooling2d_2[0][0]            \n__________________________________________________________________________________________________\nconv2d_7 (Conv2D)               (None, 16, 16, 128)  147584      conv2d_6[0][0]                   \n__________________________________________________________________________________________________\nmax_pooling2d_3 (MaxPooling2D)  (None, 8, 8, 128)    0           conv2d_7[0][0]                   \n__________________________________________________________________________________________________\nconv2d_8 (Conv2D)               (None, 8, 8, 256)    295168      max_pooling2d_3[0][0]            \n__________________________________________________________________________________________________\nconv2d_9 (Conv2D)               (None, 8, 8, 256)    590080      conv2d_8[0][0]                   \n__________________________________________________________________________________________________\nup_sampling2d (UpSampling2D)    (None, 16, 16, 256)  0           conv2d_9[0][0]                   \n__________________________________________________________________________________________________\nconcatenate (Concatenate)       (None, 16, 16, 384)  0           up_sampling2d[0][0]              \n                                                                 conv2d_7[0][0]                   \n__________________________________________________________________________________________________\nconv2d_10 (Conv2D)              (None, 16, 16, 128)  442496      concatenate[0][0]                \n__________________________________________________________________________________________________\nconv2d_11 (Conv2D)              (None, 16, 16, 128)  147584      conv2d_10[0][0]                  \n__________________________________________________________________________________________________\nup_sampling2d_1 (UpSampling2D)  (None, 32, 32, 128)  0           conv2d_11[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_1 (Concatenate)     (None, 32, 32, 192)  0           up_sampling2d_1[0][0]            \n                                                                 conv2d_5[0][0]                   \n__________________________________________________________________________________________________\nconv2d_12 (Conv2D)              (None, 32, 32, 64)   110656      concatenate_1[0][0]              \n__________________________________________________________________________________________________\nconv2d_13 (Conv2D)              (None, 32, 32, 64)   36928       conv2d_12[0][0]                  \n__________________________________________________________________________________________________\nup_sampling2d_2 (UpSampling2D)  (None, 64, 64, 64)   0           conv2d_13[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_2 (Concatenate)     (None, 64, 64, 96)   0           up_sampling2d_2[0][0]            \n                                                                 conv2d_3[0][0]                   \n__________________________________________________________________________________________________\nconv2d_14 (Conv2D)              (None, 64, 64, 32)   27680       concatenate_2[0][0]              \n__________________________________________________________________________________________________\nconv2d_15 (Conv2D)              (None, 64, 64, 32)   9248        conv2d_14[0][0]                  \n__________________________________________________________________________________________________\nup_sampling2d_3 (UpSampling2D)  (None, 128, 128, 32) 0           conv2d_15[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_3 (Concatenate)     (None, 128, 128, 48) 0           up_sampling2d_3[0][0]            \n                                                                 conv2d_1[0][0]                   \n__________________________________________________________________________________________________\nconv2d_16 (Conv2D)              (None, 128, 128, 16) 6928        concatenate_3[0][0]              \n__________________________________________________________________________________________________\nconv2d_17 (Conv2D)              (None, 128, 128, 16) 2320        conv2d_16[0][0]                  \n__________________________________________________________________________________________________\nconv2d_18 (Conv2D)              (None, 128, 128, 1)  17          conv2d_17[0][0]                  \n==================================================================================================\nTotal params: 1,962,625\nTrainable params: 1,962,625\nNon-trainable params: 0\n__________________________________________________________________________________________________"
  },
  {
    "objectID": "pre-post/code/unet_segmentation.html#training-the-model",
    "href": "pre-post/code/unet_segmentation.html#training-the-model",
    "title": "U-NET",
    "section": "Training the model",
    "text": "Training the model\n\ntrain_gen = DataGen(train_ids, train_path, image_size=image_size, batch_size=batch_size)\nvalid_gen = DataGen(valid_ids, train_path, image_size=image_size, batch_size=batch_size)\n\ntrain_steps = len(train_ids)//batch_size\nvalid_steps = len(valid_ids)//batch_size\n\nmodel.fit_generator(train_gen, validation_data=valid_gen, steps_per_epoch=train_steps,\n                    validation_steps=valid_steps, epochs=epochs)\n\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.cast instead.\nEpoch 1/10\n2/2 [==============================] - 1s 462ms/step - loss: 0.8926 - acc: 0.9346\n83/83 [==============================] - 24s 291ms/step - loss: 2.6685 - acc: 0.8320 - val_loss: 0.8926 - val_acc: 0.9346\nEpoch 2/10\n5/5 [==============================] - 0s 76ms/step - loss: 0.2497 - acc: 0.9107\n83/83 [==============================] - 14s 170ms/step - loss: 0.8620 - acc: 0.8629 - val_loss: 0.2497 - val_acc: 0.9107\nEpoch 3/10\n5/5 [==============================] - 0s 85ms/step - loss: 0.1496 - acc: 0.9668\n83/83 [==============================] - 15s 177ms/step - loss: 0.6539 - acc: 0.9156 - val_loss: 0.1496 - val_acc: 0.9668\nEpoch 4/10\n5/5 [==============================] - 0s 79ms/step - loss: 0.1137 - acc: 0.9709\n83/83 [==============================] - 15s 179ms/step - loss: 0.4919 - acc: 0.9382 - val_loss: 0.1137 - val_acc: 0.9709\nEpoch 5/10\n5/5 [==============================] - 0s 78ms/step - loss: 0.1407 - acc: 0.9667\n83/83 [==============================] - 14s 172ms/step - loss: 0.5270 - acc: 0.9361 - val_loss: 0.1407 - val_acc: 0.9667\nEpoch 6/10\n5/5 [==============================] - 0s 83ms/step - loss: 0.1272 - acc: 0.9680\n83/83 [==============================] - 14s 172ms/step - loss: 0.4950 - acc: 0.9378 - val_loss: 0.1272 - val_acc: 0.9680\nEpoch 7/10\n5/5 [==============================] - 0s 79ms/step - loss: 0.1142 - acc: 0.9684\n83/83 [==============================] - 14s 172ms/step - loss: 0.5161 - acc: 0.9344 - val_loss: 0.1142 - val_acc: 0.9684\nEpoch 8/10\n5/5 [==============================] - 0s 80ms/step - loss: 0.0952 - acc: 0.9717\n83/83 [==============================] - 15s 175ms/step - loss: 0.4100 - acc: 0.9449 - val_loss: 0.0952 - val_acc: 0.9717\nEpoch 9/10\n5/5 [==============================] - 0s 73ms/step - loss: 0.0981 - acc: 0.9712\n83/83 [==============================] - 16s 188ms/step - loss: 0.3795 - acc: 0.9478 - val_loss: 0.0981 - val_acc: 0.9712\nEpoch 10/10\n5/5 [==============================] - 0s 82ms/step - loss: 0.0883 - acc: 0.9726\n83/83 [==============================] - 14s 172ms/step - loss: 0.3781 - acc: 0.9481 - val_loss: 0.0883 - val_acc: 0.9726\n\n\n&lt;tensorflow.python.keras.callbacks.History at 0x7f7f0e253ba8&gt;"
  },
  {
    "objectID": "pre-post/code/unet_segmentation.html#testing-the-model",
    "href": "pre-post/code/unet_segmentation.html#testing-the-model",
    "title": "U-NET",
    "section": "Testing the model",
    "text": "Testing the model\n\n## Save the Weights\nmodel.save_weights(\"UNetW.h5\")\n\n## Dataset for prediction\nx, y = valid_gen.__getitem__(2)\nresult = model.predict(x)\n\nresult = result &gt; 0.5\n\n\nfig = plt.figure()\nfig.subplots_adjust(hspace=0.4, wspace=0.4)\n\nax = fig.add_subplot(1, 2, 1)\nax.imshow(np.reshape(y[0]*255, (image_size, image_size)), cmap=\"gray\")\n\nax = fig.add_subplot(1, 2, 2)\nax.imshow(np.reshape(result[0]*255, (image_size, image_size)), cmap=\"gray\")\n\n\n\n\n\n\n\n\n\nfig = plt.figure()\nfig.subplots_adjust(hspace=0.4, wspace=0.4)\n\nax = fig.add_subplot(1, 2, 1)\nax.imshow(np.reshape(y[1]*255, (image_size, image_size)), cmap=\"gray\")\n\nax = fig.add_subplot(1, 2, 2)\nax.imshow(np.reshape(result[1]*255, (image_size, image_size)), cmap=\"gray\")"
  },
  {
    "objectID": "pre-post/Presentaciones/IntroIM.html#índice",
    "href": "pre-post/Presentaciones/IntroIM.html#índice",
    "title": "Introducción a la imagen médica",
    "section": "Índice",
    "text": "Índice\n\nIntroducción a la imagen digital\nConceptos básicos de imagen\nAdquisición de imágenes médicas\nProcesamiento de imágenes\n\n\nNotes"
  },
  {
    "objectID": "pre-post/Presentaciones/IntroIM.html#definición-de-imagen-digital",
    "href": "pre-post/Presentaciones/IntroIM.html#definición-de-imagen-digital",
    "title": "Introducción a la imagen médica",
    "section": "Definición de imagen digital",
    "text": "Definición de imagen digital\nUna imagen digital o gráfico digital es una representación bidimensional de una imagen a partir de una matriz numérica, frecuentemente en binario (unos y ceros). Dependiendo de si la resolución de la imagen es estática o dinámica, puede tratarse de una imagen matricial (o mapa de bits) o de un gráfico vectorial. El mapa de bits es el formato más utilizado en informática.\n\nNotes"
  },
  {
    "objectID": "pre-post/Presentaciones/IntroIM.html#definición-de-radiología-digital",
    "href": "pre-post/Presentaciones/IntroIM.html#definición-de-radiología-digital",
    "title": "Introducción a la imagen médica",
    "section": "Definición de radiología digital",
    "text": "Definición de radiología digital\nLa radiología digital es el conjunto de técnicas para obtener imágenes radiológicas escaneadas en formato digital\n\nA partir del escaneo de una la película tradicional (analógica) una vez revelada.\nPor escaneo de una Placa fotoestimulable de fósforo reutilizable que se graba con la imagen de la radiografía (CR)\nUtilizando detectores sensibles expuestos directa o indirectamente a los detectores de rayos X.\n\n\nNotes"
  },
  {
    "objectID": "pre-post/Presentaciones/IntroIM.html#breve-historia-y-evolución-de-la-imagen-digital",
    "href": "pre-post/Presentaciones/IntroIM.html#breve-historia-y-evolución-de-la-imagen-digital",
    "title": "Introducción a la imagen médica",
    "section": "Breve historia y evolución de la imagen digital",
    "text": "Breve historia y evolución de la imagen digital\n\n\nNotes"
  },
  {
    "objectID": "pre-post/Presentaciones/IntroIM.html#importancia-de-la-imagen-digital-en-la-medicina",
    "href": "pre-post/Presentaciones/IntroIM.html#importancia-de-la-imagen-digital-en-la-medicina",
    "title": "Introducción a la imagen médica",
    "section": "Importancia de la imagen digital en la medicina",
    "text": "Importancia de la imagen digital en la medicina\n\nDiagnóstico preciso.\nProcedimiento no invasivo\nInvestigación y desarrollo\nReducción de la dosis al paciente\n\n\nNotes"
  },
  {
    "objectID": "pre-post/Presentaciones/IntroIM.html#qué-es-un-pixel",
    "href": "pre-post/Presentaciones/IntroIM.html#qué-es-un-pixel",
    "title": "Introducción a la imagen médica",
    "section": "¿Qué es un pixel?",
    "text": "¿Qué es un pixel?\n\n\n\n\n\n\n\n\nNotes"
  },
  {
    "objectID": "pre-post/Presentaciones/IntroIM.html#resolución",
    "href": "pre-post/Presentaciones/IntroIM.html#resolución",
    "title": "Introducción a la imagen médica",
    "section": "Resolución",
    "text": "Resolución\n\n\nNotes"
  },
  {
    "objectID": "pre-post/Presentaciones/IntroIM.html#profundidad-de-bit",
    "href": "pre-post/Presentaciones/IntroIM.html#profundidad-de-bit",
    "title": "Introducción a la imagen médica",
    "section": "Profundidad de bit",
    "text": "Profundidad de bit"
  },
  {
    "objectID": "pre-post/Presentaciones/IntroIM.html#otras-características",
    "href": "pre-post/Presentaciones/IntroIM.html#otras-características",
    "title": "Introducción a la imagen médica",
    "section": "Otras características",
    "text": "Otras características\n\nEspacio de color (RGB,CMYK)\nRelación de aspecto\n\n\nNotes"
  },
  {
    "objectID": "pre-post/Presentaciones/IntroIM.html#modalidades-de-imagen-médica-radiografías",
    "href": "pre-post/Presentaciones/IntroIM.html#modalidades-de-imagen-médica-radiografías",
    "title": "Introducción a la imagen médica",
    "section": "Modalidades de imagen médica: Radiografías",
    "text": "Modalidades de imagen médica: Radiografías\n\n\n\nTécnica 2D\nRayos X\nExploración anatómica\nGrafía\n\n\n\n\n\n\nNotes"
  },
  {
    "objectID": "pre-post/Presentaciones/IntroIM.html#modalidades-de-imagen-médica-arcos-de-quirófano",
    "href": "pre-post/Presentaciones/IntroIM.html#modalidades-de-imagen-médica-arcos-de-quirófano",
    "title": "Introducción a la imagen médica",
    "section": "Modalidades de imagen médica: Arcos de quirófano",
    "text": "Modalidades de imagen médica: Arcos de quirófano\n\n\n\nTécnica 2D\nRayos X\nExploración anatómica\nEscopia\n\n\n\n\n\n\nNotes"
  },
  {
    "objectID": "pre-post/Presentaciones/IntroIM.html#modalidades-de-imagen-médica-tomografía-computerizada",
    "href": "pre-post/Presentaciones/IntroIM.html#modalidades-de-imagen-médica-tomografía-computerizada",
    "title": "Introducción a la imagen médica",
    "section": "Modalidades de imagen médica: Tomografía computerizada",
    "text": "Modalidades de imagen médica: Tomografía computerizada\n\n\n\nTécnica 3D\nRayos X\nExploración anatómica\nImagen tomográfica\n\n\n\n\n\n\nNotes"
  },
  {
    "objectID": "pre-post/Presentaciones/IntroIM.html#modalidades-de-imagen-médica-imágenes-por-resonancia-magnética-nuclear",
    "href": "pre-post/Presentaciones/IntroIM.html#modalidades-de-imagen-médica-imágenes-por-resonancia-magnética-nuclear",
    "title": "Introducción a la imagen médica",
    "section": "Modalidades de imagen médica: Imágenes por resonancia magnética nuclear",
    "text": "Modalidades de imagen médica: Imágenes por resonancia magnética nuclear\n\n\n\nTécnica 3D\nResonancia magnética nuclear\nExploración anatómica\nImagen tomográfica\n\n\n\n\n\n\nNotes"
  },
  {
    "objectID": "pre-post/Presentaciones/IntroIM.html#modalidades-de-imagen-médica-equipos-de-ultrasonidos",
    "href": "pre-post/Presentaciones/IntroIM.html#modalidades-de-imagen-médica-equipos-de-ultrasonidos",
    "title": "Introducción a la imagen médica",
    "section": "Modalidades de imagen médica: Equipos de ultrasonidos",
    "text": "Modalidades de imagen médica: Equipos de ultrasonidos\n\n\n\nTécnica 2D\nOndas sonoras de alta frecuencia\nExploración anatómica\n\n\n\n\n\n\nNotes"
  },
  {
    "objectID": "pre-post/Presentaciones/IntroIM.html#modalidades-de-imagen-médica-spect",
    "href": "pre-post/Presentaciones/IntroIM.html#modalidades-de-imagen-médica-spect",
    "title": "Introducción a la imagen médica",
    "section": "Modalidades de imagen médica: SPECT",
    "text": "Modalidades de imagen médica: SPECT\n\n\n\nTécnica 3D\nRadiofármacos que emiten radiación gamma (\\(^{99}Tc^{m}\\))\nExploración funcional\n\n\n\n\n\n\nNotes"
  },
  {
    "objectID": "pre-post/Presentaciones/IntroIM.html#modalidades-de-imagen-médica-pet",
    "href": "pre-post/Presentaciones/IntroIM.html#modalidades-de-imagen-médica-pet",
    "title": "Introducción a la imagen médica",
    "section": "Modalidades de imagen médica: PET",
    "text": "Modalidades de imagen médica: PET\n\n\n\nTécnica 3D\nRadiofármacos que emiten positrones (\\(^{18}F\\))\nExploración funcional\n\n\n\n\n\n\nNotes"
  },
  {
    "objectID": "pre-post/Presentaciones/IntroIM.html#segmentación-de-órganos",
    "href": "pre-post/Presentaciones/IntroIM.html#segmentación-de-órganos",
    "title": "Introducción a la imagen médica",
    "section": "Segmentación de órganos",
    "text": "Segmentación de órganos\n\n\nNotes"
  },
  {
    "objectID": "pre-post/Presentaciones/IntroIM.html#segmentación-de-células",
    "href": "pre-post/Presentaciones/IntroIM.html#segmentación-de-células",
    "title": "Introducción a la imagen médica",
    "section": "Segmentación de células",
    "text": "Segmentación de células\n\n\nNotes"
  },
  {
    "objectID": "pre-post/Presentaciones/IntroIM.html#segmentación-de-tumores",
    "href": "pre-post/Presentaciones/IntroIM.html#segmentación-de-tumores",
    "title": "Introducción a la imagen médica",
    "section": "Segmentación de tumores",
    "text": "Segmentación de tumores\n\n\nNotes"
  },
  {
    "objectID": "pre-post/Presentaciones/IntroIM.html#instalar-3dslicer-y-darse-de-alta-en-huggingface",
    "href": "pre-post/Presentaciones/IntroIM.html#instalar-3dslicer-y-darse-de-alta-en-huggingface",
    "title": "Introducción a la imagen médica",
    "section": "Instalar 3DSlicer y darse de alta en Huggingface",
    "text": "Instalar 3DSlicer y darse de alta en Huggingface\n\nDescargar el programa\nSeguir las intrucciones del ejecutable\nIr a la página de Huggingface\nDarse de alta\n\n\n\n\n\nIAMED"
  },
  {
    "objectID": "posts/Presentaciones/IntroIM.html#índice",
    "href": "posts/Presentaciones/IntroIM.html#índice",
    "title": "Introducción a la imagen médica",
    "section": "Índice",
    "text": "Índice\n\nIntroducción a la imagen digital\nConceptos básicos de imagen\nAdquisición de imágenes médicas\nSegmentación de imágenes médicas\nRadiómica (Radiomics)\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/IntroIM.html#definición-de-imagen-digital",
    "href": "posts/Presentaciones/IntroIM.html#definición-de-imagen-digital",
    "title": "Introducción a la imagen médica",
    "section": "Definición de imagen digital",
    "text": "Definición de imagen digital\nUna imagen digital o gráfico digital es una representación bidimensional de una imagen a partir de una matriz numérica, frecuentemente en binario (unos y ceros). Dependiendo de si la resolución de la imagen es estática o dinámica, puede tratarse de una imagen matricial (o mapa de bits) o de un gráfico vectorial. El mapa de bits es el formato más utilizado en informática.\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/IntroIM.html#definición-de-radiología-digital",
    "href": "posts/Presentaciones/IntroIM.html#definición-de-radiología-digital",
    "title": "Introducción a la imagen médica",
    "section": "Definición de radiología digital",
    "text": "Definición de radiología digital\nLa radiología digital es el conjunto de técnicas para obtener imágenes radiológicas escaneadas en formato digital\n\nA partir del escaneo de una la película tradicional (analógica) una vez revelada.\nPor escaneo de una Placa fotoestimulable de fósforo reutilizable que se graba con la imagen de la radiografía (CR)\nUtilizando detectores sensibles expuestos directa o indirectamente a los detectores de rayos X.\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/IntroIM.html#breve-historia-y-evolución-de-la-imagen-digital",
    "href": "posts/Presentaciones/IntroIM.html#breve-historia-y-evolución-de-la-imagen-digital",
    "title": "Introducción a la imagen médica",
    "section": "Breve historia y evolución de la imagen digital",
    "text": "Breve historia y evolución de la imagen digital\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/IntroIM.html#importancia-de-la-imagen-digital-en-la-medicina",
    "href": "posts/Presentaciones/IntroIM.html#importancia-de-la-imagen-digital-en-la-medicina",
    "title": "Introducción a la imagen médica",
    "section": "Importancia de la imagen digital en la medicina",
    "text": "Importancia de la imagen digital en la medicina\n\nDiagnóstico preciso.\nProcedimiento no invasivo\nInvestigación y desarrollo\nReducción de la dosis al paciente\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/IntroIM.html#qué-es-un-pixel",
    "href": "posts/Presentaciones/IntroIM.html#qué-es-un-pixel",
    "title": "Introducción a la imagen médica",
    "section": "¿Qué es un pixel?",
    "text": "¿Qué es un pixel?\n\n\n\n\n\n\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/IntroIM.html#resolución",
    "href": "posts/Presentaciones/IntroIM.html#resolución",
    "title": "Introducción a la imagen médica",
    "section": "Resolución",
    "text": "Resolución\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/IntroIM.html#profundidad-de-bit",
    "href": "posts/Presentaciones/IntroIM.html#profundidad-de-bit",
    "title": "Introducción a la imagen médica",
    "section": "Profundidad de bit",
    "text": "Profundidad de bit"
  },
  {
    "objectID": "posts/Presentaciones/IntroIM.html#otras-características",
    "href": "posts/Presentaciones/IntroIM.html#otras-características",
    "title": "Introducción a la imagen médica",
    "section": "Otras características",
    "text": "Otras características\n\nEspacio de color (RGB,CMYK)\nRelación de aspecto\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/IntroIM.html#modalidades-de-imagen-médica-radiografías",
    "href": "posts/Presentaciones/IntroIM.html#modalidades-de-imagen-médica-radiografías",
    "title": "Introducción a la imagen médica",
    "section": "Modalidades de imagen médica: Radiografías",
    "text": "Modalidades de imagen médica: Radiografías\n\n\n\nTécnica 2D\nRayos X\nExploración anatómica\nGrafía\n\n\n\n\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/IntroIM.html#modalidades-de-imagen-médica-arcos-de-quirófano",
    "href": "posts/Presentaciones/IntroIM.html#modalidades-de-imagen-médica-arcos-de-quirófano",
    "title": "Introducción a la imagen médica",
    "section": "Modalidades de imagen médica: Arcos de quirófano",
    "text": "Modalidades de imagen médica: Arcos de quirófano\n\n\n\nTécnica 2D\nRayos X\nExploración anatómica\nEscopia\n\n\n\n\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/IntroIM.html#modalidades-de-imagen-médica-tomografía-computerizada",
    "href": "posts/Presentaciones/IntroIM.html#modalidades-de-imagen-médica-tomografía-computerizada",
    "title": "Introducción a la imagen médica",
    "section": "Modalidades de imagen médica: Tomografía computerizada",
    "text": "Modalidades de imagen médica: Tomografía computerizada\n\n\n\nTécnica 3D\nRayos X\nExploración anatómica\nImagen tomográfica\n\n\n\n\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/IntroIM.html#modalidades-de-imagen-médica-imágenes-por-resonancia-magnética-nuclear",
    "href": "posts/Presentaciones/IntroIM.html#modalidades-de-imagen-médica-imágenes-por-resonancia-magnética-nuclear",
    "title": "Introducción a la imagen médica",
    "section": "Modalidades de imagen médica: Imágenes por resonancia magnética nuclear",
    "text": "Modalidades de imagen médica: Imágenes por resonancia magnética nuclear\n\n\n\nTécnica 3D\nResonancia magnética nuclear\nExploración anatómica\nImagen tomográfica\n\n\n\n\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/IntroIM.html#modalidades-de-imagen-médica-equipos-de-ultrasonidos",
    "href": "posts/Presentaciones/IntroIM.html#modalidades-de-imagen-médica-equipos-de-ultrasonidos",
    "title": "Introducción a la imagen médica",
    "section": "Modalidades de imagen médica: Equipos de ultrasonidos",
    "text": "Modalidades de imagen médica: Equipos de ultrasonidos\n\n\n\nTécnica 2D\nOndas sonoras de alta frecuencia\nExploración anatómica\n\n\n\n\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/IntroIM.html#modalidades-de-imagen-médica-spect",
    "href": "posts/Presentaciones/IntroIM.html#modalidades-de-imagen-médica-spect",
    "title": "Introducción a la imagen médica",
    "section": "Modalidades de imagen médica: SPECT",
    "text": "Modalidades de imagen médica: SPECT\n\n\n\nTécnica 3D\nRadiofármacos que emiten radiación gamma (\\(^{99}Tc^{m}\\))\nExploración funcional\n\n\n\n\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/IntroIM.html#modalidades-de-imagen-médica-pet",
    "href": "posts/Presentaciones/IntroIM.html#modalidades-de-imagen-médica-pet",
    "title": "Introducción a la imagen médica",
    "section": "Modalidades de imagen médica: PET",
    "text": "Modalidades de imagen médica: PET\n\n\n\nTécnica 3D\nRadiofármacos que emiten positrones (\\(^{18}F\\))\nExploración funcional\n\n\n\n\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/IntroIM.html#segmentación-de-órganos",
    "href": "posts/Presentaciones/IntroIM.html#segmentación-de-órganos",
    "title": "Introducción a la imagen médica",
    "section": "Segmentación de órganos",
    "text": "Segmentación de órganos\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/IntroIM.html#segmentación-de-células",
    "href": "posts/Presentaciones/IntroIM.html#segmentación-de-células",
    "title": "Introducción a la imagen médica",
    "section": "Segmentación de células",
    "text": "Segmentación de células\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/IntroIM.html#segmentación-de-tumores",
    "href": "posts/Presentaciones/IntroIM.html#segmentación-de-tumores",
    "title": "Introducción a la imagen médica",
    "section": "Segmentación de tumores",
    "text": "Segmentación de tumores\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/IntroIM.html#instalar-3dslicer-y-darse-de-alta-en-huggingface",
    "href": "posts/Presentaciones/IntroIM.html#instalar-3dslicer-y-darse-de-alta-en-huggingface",
    "title": "Introducción a la imagen médica",
    "section": "Instalar 3DSlicer y darse de alta en Huggingface",
    "text": "Instalar 3DSlicer y darse de alta en Huggingface\n\nDescargar el programa\nSeguir las intrucciones del ejecutable\nIr a la página de Huggingface\nDarse de alta\n\n\n\n\n\nIAMED"
  },
  {
    "objectID": "posts/Presentaciones/IntroIM.html#la-imagen-médica-más-allá-de-lo-visual",
    "href": "posts/Presentaciones/IntroIM.html#la-imagen-médica-más-allá-de-lo-visual",
    "title": "Introducción a la imagen médica",
    "section": "La Imagen Médica: Más Allá de lo Visual",
    "text": "La Imagen Médica: Más Allá de lo Visual\n\nModalidades tradicionales:\n\nRayos X\nTomografía Computarizada (TC)\nResonancia Magnética (RM)\nMedicina Nuclear (PET/CT)\n\nInterpretación: Principalmente cualitativa y subjetiva.\nLimitaciones:\n\nVariabilidad inter-observador.\nInformación potencialmente oculta."
  },
  {
    "objectID": "posts/Presentaciones/IntroIM.html#qué-es-la-radiómica",
    "href": "posts/Presentaciones/IntroIM.html#qué-es-la-radiómica",
    "title": "Introducción a la imagen médica",
    "section": "¿Qué es la Radiómica?",
    "text": "¿Qué es la Radiómica?\n\nLa radiómica es un enfoque cuantitativo de la imagenología médica, que tiene como objetivo mejorar los datos existentes disponibles para los clínicos mediante análisis matemáticos avanzados. A través de la extracción matemática de la distribución espacial de las intensidades de señal y las interrelaciones de los píxeles, la radiómica cuantifica la información textural utilizando métodos de análisis del campo de la inteligencia artificial.\n\n\nTransforma imágenes médicas en datos cuantitativos.\n“Biopsia virtual”: extraer información sin procedimientos invasivos.\nLa imagen como un gran conjunto de datos a explotar."
  },
  {
    "objectID": "posts/Presentaciones/IntroIM.html#qué-es-la-radiómica-1",
    "href": "posts/Presentaciones/IntroIM.html#qué-es-la-radiómica-1",
    "title": "Introducción a la imagen médica",
    "section": "¿Qué es la Radiómica?",
    "text": "¿Qué es la Radiómica?"
  },
  {
    "objectID": "posts/Presentaciones/IntroIM.html#flujo-de-trabajo-de-la-radiómica-imagen",
    "href": "posts/Presentaciones/IntroIM.html#flujo-de-trabajo-de-la-radiómica-imagen",
    "title": "Introducción a la imagen médica",
    "section": "Flujo de Trabajo de la Radiómica: Imagen",
    "text": "Flujo de Trabajo de la Radiómica: Imagen\n\nAdquisición de Imágenes\n\nImágenes médicas estándar (TC, RM, etc.).\n¡Importancia de la estandarización!\n\nSegmentación (ROI)\n\nDelimitar la región de interés (tumor, órgano…).\nManual, semiautomática o automática."
  },
  {
    "objectID": "posts/Presentaciones/IntroIM.html#flujo-de-trabajo-de-la-radiómica-parte-radiómica",
    "href": "posts/Presentaciones/IntroIM.html#flujo-de-trabajo-de-la-radiómica-parte-radiómica",
    "title": "Introducción a la imagen médica",
    "section": "Flujo de Trabajo de la Radiómica: Parte Radiómica",
    "text": "Flujo de Trabajo de la Radiómica: Parte Radiómica\n\n\n\nExtracción de Características: Los algoritmos extraen miles de características.\nForma: Tamaño, esfericidad…\nIntensidad: Distribución de valores.\nTextura: Patrones espaciales.\n\n\n\n\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/IntroIM.html#flujo-de-trabajo-de-la-radiómica-análisis",
    "href": "posts/Presentaciones/IntroIM.html#flujo-de-trabajo-de-la-radiómica-análisis",
    "title": "Introducción a la imagen médica",
    "section": "Flujo de Trabajo de la Radiómica: Análisis",
    "text": "Flujo de Trabajo de la Radiómica: Análisis\n\nSelección y Modelado\n\nNo todas las características son útiles.\nSelección: quedarnos con las más relevantes.\nModelado: crear algoritmos predictivos (Machine Learning).\n\nValidación del Modelo\n\nProbar con datos “nuevos”.\nAsegurar la generalización (evitar sobreajuste).\nMétricas: Precisión, Sensibilidad, Especificidad."
  },
  {
    "objectID": "posts/Presentaciones/IntroIM.html#aplicaciones-clínicas-oncología",
    "href": "posts/Presentaciones/IntroIM.html#aplicaciones-clínicas-oncología",
    "title": "Introducción a la imagen médica",
    "section": "Aplicaciones Clínicas: Oncología",
    "text": "Aplicaciones Clínicas: Oncología\n\nDetección temprana\nDiferenciación benigno/maligno.\nPredicción de respuesta a tratamientos.\nPronóstico y supervivencia."
  },
  {
    "objectID": "posts/Presentaciones/IntroIM.html#aplicaciones-clínicas-otras-áreas",
    "href": "posts/Presentaciones/IntroIM.html#aplicaciones-clínicas-otras-áreas",
    "title": "Introducción a la imagen médica",
    "section": "Aplicaciones Clínicas: Otras Áreas",
    "text": "Aplicaciones Clínicas: Otras Áreas\n\n\nCardiovasculares\n\nCaracterización de placas.\n\nNeurológicas\n\nAlzheimer.\nEsclerosis múltiple.\n\n\nPulmonares\n\nEPOC.\nFibrosis."
  },
  {
    "objectID": "posts/Presentaciones/IntroIM.html#desafíos-y-limitaciones",
    "href": "posts/Presentaciones/IntroIM.html#desafíos-y-limitaciones",
    "title": "Introducción a la imagen médica",
    "section": "Desafíos y Limitaciones",
    "text": "Desafíos y Limitaciones\n\n\nEstandarización: protocolos consistentes.\n“Caja Negra”: interpretabilidad de los modelos.\nGrandes Datos: necesidad de muchos datos de calidad.\nIntegración Clínica: herramientas fáciles de usar."
  },
  {
    "objectID": "posts/Presentaciones/AI_Image.html#índice",
    "href": "posts/Presentaciones/AI_Image.html#índice",
    "title": "AI e Imagen Médica",
    "section": "Índice",
    "text": "Índice\n\nOCR\nAI en Imagen Médica\nFormatos de Imagen\nSegmentación automática\nLímites de los LLM\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/AI_Image.html#qué-es-ocr-y-por-qué-es-relevante-en-el-entorno-sanitario",
    "href": "posts/Presentaciones/AI_Image.html#qué-es-ocr-y-por-qué-es-relevante-en-el-entorno-sanitario",
    "title": "AI e Imagen Médica",
    "section": "¿Qué es OCR y por qué es relevante en el entorno sanitario?",
    "text": "¿Qué es OCR y por qué es relevante en el entorno sanitario?\n\n¿Has usado alguna vez OCR? (Escanear documentos, apps de escaneo de texto, etc.)\nDefinición sencilla: “Enseñar a un ordenador a leer texto en imágenes.” [Image of Demostración online sencilla de OCR: Imagen de documento médico en papel a la izquierda, texto digitalizado a la derecha]\nAplicaciones cotidianas: Escaneo de documentos con el móvil, reconocimiento de matrículas.\nRelevancia en sanidad: Digitalizar documentos médicos:\n\nEficiencia, menos papel, menos errores.\nAcceso rápido a información."
  },
  {
    "objectID": "posts/Presentaciones/AI_Image.html#ejemplos-concretos-de-uso-de-ocr-en-hospitales-y-clínicas",
    "href": "posts/Presentaciones/AI_Image.html#ejemplos-concretos-de-uso-de-ocr-en-hospitales-y-clínicas",
    "title": "AI e Imagen Médica",
    "section": "Ejemplos concretos de uso de OCR en hospitales y clínicas",
    "text": "Ejemplos concretos de uso de OCR en hospitales y clínicas\n\nDigitalización de historias clínicas en papel.\nProcesamiento de recetas médicas.\nGestión de resultados de laboratorio."
  },
  {
    "objectID": "posts/Presentaciones/AI_Image.html#ampliando-la-perspectiva-ia-no-solo-lee-texto-también-ve-imágenes-médicas.",
    "href": "posts/Presentaciones/AI_Image.html#ampliando-la-perspectiva-ia-no-solo-lee-texto-también-ve-imágenes-médicas.",
    "title": "AI e Imagen Médica",
    "section": "Ampliando la perspectiva: IA no solo lee texto, también “ve” imágenes médicas.",
    "text": "Ampliando la perspectiva: IA no solo lee texto, también “ve” imágenes médicas.\n\nConexión con OCR: Si la IA puede “leer” imágenes con texto…\n… ¿Podría también “analizar” imágenes médicas para ayudarnos en el diagnóstico?\nImportancia de la imagen médica en el diagnóstico y tratamiento (radiografías, RM, TC, etc.).\nAuge de la IA en medicina y su potencial transformador en el análisis de imágenes."
  },
  {
    "objectID": "posts/Presentaciones/AI_Image.html#inteligencia-artificial-e-imagen-médica-qué-ventajas-nos-ofrece",
    "href": "posts/Presentaciones/AI_Image.html#inteligencia-artificial-e-imagen-médica-qué-ventajas-nos-ofrece",
    "title": "AI e Imagen Médica",
    "section": "Inteligencia Artificial e Imagen Médica: ¿Qué ventajas nos ofrece?",
    "text": "Inteligencia Artificial e Imagen Médica: ¿Qué ventajas nos ofrece?\n\nMejora de la precisión diagnóstica: Detectar enfermedades antes y con mayor exactitud. [Image of Radiografía de tórax mostrando un pequeño nódulo pulmonar resaltado por IA]\n\nEjemplo concreto: Detectar pequeños nódulos pulmonares en radiografías.\n\nEficiencia en el flujo de trabajo: Ahorrar tiempo a los médicos en tareas repetitivas.\n\nEjemplo concreto: Segmentar órganos automáticamente.\n\nIdentificación de patrones sutiles: Descubrir información oculta no visible al ojo humano.\n\nEjemplo concreto: Detectar cambios tempranos en resonancias cerebrales.\n\nHacia una medicina más personalizada: Adaptar diagnóstico y tratamiento a cada paciente."
  },
  {
    "objectID": "posts/Presentaciones/AI_Image.html#por-qué-necesitamos-formatos-especiales-para-las-imágenes-médicas",
    "href": "posts/Presentaciones/AI_Image.html#por-qué-necesitamos-formatos-especiales-para-las-imágenes-médicas",
    "title": "AI e Imagen Médica",
    "section": "¿Por qué necesitamos “formatos especiales” para las imágenes médicas?",
    "text": "¿Por qué necesitamos “formatos especiales” para las imágenes médicas?\n\nAnalogía con formatos de texto o música: Necesitamos formatos estandarizados para que los ordenadores “entiendan” las imágenes médicas.\nImportancia de la información adicional (metadatos) en imágenes médicas."
  },
  {
    "objectID": "posts/Presentaciones/AI_Image.html#el-estándar-dicom-el-formato-universal-de-las-imágenes-médicas-clínicas.",
    "href": "posts/Presentaciones/AI_Image.html#el-estándar-dicom-el-formato-universal-de-las-imágenes-médicas-clínicas.",
    "title": "AI e Imagen Médica",
    "section": "El Estándar DICOM: El “formato universal” de las imágenes médicas clínicas.",
    "text": "El Estándar DICOM: El “formato universal” de las imágenes médicas clínicas.\n\nDICOM: Estándar principal para imágenes en hospitales. “Idioma común” de equipos médicos.\nCaracterísticas clave:\n\n“Caja” que contiene imagen y metadatos.\nMetadatos: Información del paciente, estudio, equipo (“etiqueta”).\nInteroperabilidad: Equipos de diferentes marcas “se entienden”.\n\n[Image of Visor DICOM sencillo mostrando imagen y metadatos básicos]\n\nEjemplo visual de un visor DICOM sencillo."
  },
  {
    "objectID": "posts/Presentaciones/AI_Image.html#el-formato-nifti-más-para-investigación-especialmente-en-el-cerebro.",
    "href": "posts/Presentaciones/AI_Image.html#el-formato-nifti-más-para-investigación-especialmente-en-el-cerebro.",
    "title": "AI e Imagen Médica",
    "section": "El Formato NIfTI: Más para investigación, especialmente en el cerebro.",
    "text": "El Formato NIfTI: Más para investigación, especialmente en el cerebro.\n\nNIfTI: Formato más simple, para investigación, sobre todo neuroimagen.\nMás “flexible” para análisis computacional e investigación.\nEjemplo visual de software de neuroimagen (opcional).\nDiferencia clave con DICOM: DICOM para clínica, NIfTI para investigación."
  },
  {
    "objectID": "posts/Presentaciones/AI_Image.html#qué-es-la-segmentación-como-si-la-ia-resaltara-o-dibujara-bordes-sobre-las-estructuras.",
    "href": "posts/Presentaciones/AI_Image.html#qué-es-la-segmentación-como-si-la-ia-resaltara-o-dibujara-bordes-sobre-las-estructuras.",
    "title": "AI e Imagen Médica",
    "section": "¿Qué es la Segmentación? “Como si la IA resaltara o dibujara bordes sobre las estructuras.”",
    "text": "¿Qué es la Segmentación? “Como si la IA resaltara o dibujara bordes sobre las estructuras.”\n\nDefinición sencilla: Identificar y delimitar regiones en imágenes médicas.\nAnalogía: “Resaltar” o “dibujar bordes”.\nSegmentación manual vs. automática: Manual laboriosa y subjetiva. Automática más rápida y precisa."
  },
  {
    "objectID": "posts/Presentaciones/AI_Image.html#por-qué-es-importante-la-segmentación-en-medicina",
    "href": "posts/Presentaciones/AI_Image.html#por-qué-es-importante-la-segmentación-en-medicina",
    "title": "AI e Imagen Médica",
    "section": "¿Por qué es importante la Segmentación en Medicina?",
    "text": "¿Por qué es importante la Segmentación en Medicina?\n\nDiagnóstico: Detectar y medir tumores, lesiones. [Image of Imagen de TC de pulmón mostrando un tumor segmentado y medido por IA]\n\nEjemplo concreto: Medir tamaño de tumor en pulmón.\n\nPlanificación del tratamiento: Planificar cirugías, radioterapia. [Image of Resonancia magnética cerebral mostrando el área a operar delimitada por segmentación de IA]\n\nEjemplo concreto: Delimitar área a operar en cirugía cerebral.\n\nAnálisis cuantitativo: Medir volúmenes de órganos para detectar enfermedades.\n\nEjemplo concreto: Medir volumen del ventrículo izquierdo del corazón."
  },
  {
    "objectID": "posts/Presentaciones/AI_Image.html#cómo-hace-la-ia-la-segmentación-automática-explicación-conceptual-sencilla.",
    "href": "posts/Presentaciones/AI_Image.html#cómo-hace-la-ia-la-segmentación-automática-explicación-conceptual-sencilla.",
    "title": "AI e Imagen Médica",
    "section": "¿Cómo hace la IA la Segmentación Automática? (Explicación conceptual sencilla).",
    "text": "¿Cómo hace la IA la Segmentación Automática? (Explicación conceptual sencilla).\n\nRedes Neuronales Convolucionales (CNNs): “La IA aprende a reconocer patrones en las imágenes.” [Image of Representación muy simplificada de una Red Neuronal Convolucional como una “caja negra” que aprende de ejemplos]\n\nAnalogía: “Como un niño aprende a reconocer caras.”\n\nProceso general simplificado:\n\n“Entrenamiento”: IA aprende con imágenes segmentadas por expertos.\n“Segmentación automática”: IA segmenta nuevas imágenes."
  },
  {
    "objectID": "posts/Presentaciones/AI_Image.html#herramientas-de-segmentación-con-ia-que-ya-se-usan-ejemplos-visuales.",
    "href": "posts/Presentaciones/AI_Image.html#herramientas-de-segmentación-con-ia-que-ya-se-usan-ejemplos-visuales.",
    "title": "AI e Imagen Médica",
    "section": "Herramientas de Segmentación con IA que ya se usan (ejemplos visuales).",
    "text": "Herramientas de Segmentación con IA que ya se usan (ejemplos visuales).\n\nEjemplos de software de segmentación médica con IA (interfaces gráficas). [Image of Interfaz de software de segmentación médica mostrando segmentación de un órgano en 3D]\nAplicaciones clínicas concretas:\n\nDetección de tumores (cerebro, pulmón, mama).\nSegmentación de órganos (cardiología, neurología).\nGuía quirúrgica."
  },
  {
    "objectID": "posts/Presentaciones/AI_Image.html#resumen-rápido-de-lo-aprendido-ocr-dicom-segmentación.",
    "href": "posts/Presentaciones/AI_Image.html#resumen-rápido-de-lo-aprendido-ocr-dicom-segmentación.",
    "title": "AI e Imagen Médica",
    "section": "Resumen rápido de lo aprendido: OCR, DICOM, Segmentación.",
    "text": "Resumen rápido de lo aprendido: OCR, DICOM, Segmentación.\n\nRepaso visual y conciso de los temas principales."
  },
  {
    "objectID": "posts/Presentaciones/AI_Image.html#limitaciones-y-desafíos-actuales-de-la-ia-en-imagen-médica-adaptado-al-perfil-de-primer-año.",
    "href": "posts/Presentaciones/AI_Image.html#limitaciones-y-desafíos-actuales-de-la-ia-en-imagen-médica-adaptado-al-perfil-de-primer-año.",
    "title": "AI e Imagen Médica",
    "section": "Limitaciones y Desafíos actuales de la IA en Imagen Médica (Adaptado al perfil de primer año).",
    "text": "Limitaciones y Desafíos actuales de la IA en Imagen Médica (Adaptado al perfil de primer año).\n\n“La IA es una herramienta, no un sustituto del médico.” Juicio clínico humano esencial.\n“La calidad de la IA depende de los datos con los que se la entrena.” Datos de calidad cruciales.\n“La IA necesita validación constante en la práctica clínica.” Seguridad y eficacia deben probarse."
  },
  {
    "objectID": "posts/Presentaciones/AI_Image.html#ejemplos-clínicos-impactantes-y-casos-de-uso-actual-recapitulando-y-reforzando-ejemplos.",
    "href": "posts/Presentaciones/AI_Image.html#ejemplos-clínicos-impactantes-y-casos-de-uso-actual-recapitulando-y-reforzando-ejemplos.",
    "title": "AI e Imagen Médica",
    "section": "Ejemplos Clínicos Impactantes y Casos de Uso Actual (Recapitulando y reforzando ejemplos).",
    "text": "Ejemplos Clínicos Impactantes y Casos de Uso Actual (Recapitulando y reforzando ejemplos).\n\nRadiología: Detección de nódulos pulmonares, fracturas, hemorragias cerebrales. [Image of Radiografía de tórax con detección de nódulo pulmonar resaltado por IA]\nCardiología: Análisis de ecocardiogramas, segmentación cardíaca en RM. [Image of Resonancia magnética cardíaca con segmentación del ventrículo izquierdo por IA]\nOftalmología: Detección de retinopatía diabética, degeneración macular. [Image of Imagen de retina con detección de retinopatía diabética por IA]\nPatología digital: Diagnóstico de cáncer a partir de biopsias. [Image of Imagen de histopatología con detección de células cancerosas por IA]\nTeledermatología: Pre-evaluación de riesgo de cáncer de piel con imágenes de smartphone. [Image of Imagen de lesión cutánea analizada por IA para detección de cáncer de piel]"
  },
  {
    "objectID": "posts/Presentaciones/AI_Image.html#pregunta-abierta-a-los-alumnos",
    "href": "posts/Presentaciones/AI_Image.html#pregunta-abierta-a-los-alumnos",
    "title": "AI e Imagen Médica",
    "section": "Pregunta abierta a los alumnos",
    "text": "Pregunta abierta a los alumnos\n\n¿Dónde ven ustedes el mayor potencial de la IA en imagen médica en su futuro como médicos?"
  },
  {
    "objectID": "posts/Presentaciones/AI_Image.html#recursos-adicionales-amigables-para-principiantes",
    "href": "posts/Presentaciones/AI_Image.html#recursos-adicionales-amigables-para-principiantes",
    "title": "AI e Imagen Médica",
    "section": "Recursos Adicionales “amigables” para principiantes",
    "text": "Recursos Adicionales “amigables” para principiantes\n\nSugerencias de recursos de divulgación científica accesibles, webs sencillas sobre IA en medicina, vídeos cortos explicativos."
  },
  {
    "objectID": "posts/Presentaciones/Procesamiento_datos.html#repositorio-de-github",
    "href": "posts/Presentaciones/Procesamiento_datos.html#repositorio-de-github",
    "title": "Procesamiento de prospectos",
    "section": "Repositorio de github",
    "text": "Repositorio de github\n\n\n\n\nEl enlace al repositorio aquí"
  },
  {
    "objectID": "posts/Presentaciones/Procesamiento_datos.html#pinecone-base-de-datos-vectorial",
    "href": "posts/Presentaciones/Procesamiento_datos.html#pinecone-base-de-datos-vectorial",
    "title": "Procesamiento de prospectos",
    "section": "Pinecone: Base de datos vectorial",
    "text": "Pinecone: Base de datos vectorial\n\n\n\n¿Qué es Pinecone?\n\nBase de datos vectorial diseñada para aplicaciones de IA.\nOptimizado para búsqueda por similitud de alta velocidad.\nAlmacena y consulta incrustaciones (embeddings).\n\nCasos de uso clave:\n\nSistemas de recomendación.\nBúsqueda semántica.\nChatbots y asistentes virtuales.\nDetección de anomalías."
  },
  {
    "objectID": "posts/Presentaciones/Procesamiento_datos.html#características-principales-de-pinecone",
    "href": "posts/Presentaciones/Procesamiento_datos.html#características-principales-de-pinecone",
    "title": "Procesamiento de prospectos",
    "section": "Características principales de Pinecone",
    "text": "Características principales de Pinecone\n\nAlta velocidad y escalabilidad:\n\nÍndices optimizados para búsqueda de vecinos más cercanos (ANN).\nEscalado horizontal para manejar grandes volúmenes de datos.\n\nFlexibilidad:\n\nSoporta diferentes tipos de metricas de distancia (coseno, euclidiana, etc).\nSoporta metadatos."
  },
  {
    "objectID": "posts/Presentaciones/Procesamiento_datos.html#adaptando-datos-a-pinecone",
    "href": "posts/Presentaciones/Procesamiento_datos.html#adaptando-datos-a-pinecone",
    "title": "Procesamiento de prospectos",
    "section": "Adaptando datos a pinecone",
    "text": "Adaptando datos a pinecone\n\n\n\n\nDiálogo 5 con Gemini\n\n\nEnlace a pinecone"
  },
  {
    "objectID": "posts/Presentaciones/Procesamiento_datos.html#section",
    "href": "posts/Presentaciones/Procesamiento_datos.html#section",
    "title": "Procesamiento de prospectos",
    "section": "",
    "text": "Quiero codificar en una base de datos de pinecone el siguiente json que consiste en una lista de tres diccionarios que corresponden a 3 analgesicos. Solamente es una muestra de una lista mucho más grande. La lista es:[…] Me gustaría que encontrases la manera más eficiente de codificarla en pinecone como base de una aplicación rag que mediante lenguaje natural haga preguntas y reciba también respuestas en lenguaje natural lo más exactas posibles. Es un proyecto de un chat para médicos que les ayude a encontrar el analgésico más adecuado para un determinado paciente con una determinada dolencia. Por lo tanto, las respuestas deben ser en un lenguaje de experto médico."
  },
  {
    "objectID": "posts/Presentaciones/Procesamiento_datos.html#promt-para-pinecone",
    "href": "posts/Presentaciones/Procesamiento_datos.html#promt-para-pinecone",
    "title": "Procesamiento de prospectos",
    "section": "Promt para pinecone",
    "text": "Promt para pinecone\n\nQuiero codificar en una base de datos de pinecone el siguiente json que consiste en una lista de tres diccionarios que corresponden a 3 analgesicos. Solamente es una muestra de una lista mucho más grande. La lista es:[…] Me gustaría que encontrases la manera más eficiente de codificarla en pinecone como base de una aplicación rag que mediante lenguaje natural haga preguntas y reciba también respuestas en lenguaje natural lo más exactas posibles. Es un proyecto de un chat para médicos que les ayude a encontrar el analgésico más adecuado para un determinado paciente con una determinada dolencia. Por lo tanto, las respuestas deben ser en un lenguaje de experto médico."
  },
  {
    "objectID": "posts/Presentaciones/Procesamiento_datos.html#promt-para-diseño-del-colab",
    "href": "posts/Presentaciones/Procesamiento_datos.html#promt-para-diseño-del-colab",
    "title": "Procesamiento de prospectos",
    "section": "Promt para diseño del colab",
    "text": "Promt para diseño del colab\n\nQuiero que busques un medicamento por nombre, por ejemplo paracetamol utilizando la información contenida en el archivo adjunto. Ten en cuenta que cada búsqueda puede tener varias páginas. Hay un campo llamados totalFilas que indica el número de medicamentos encontrados y otro llamado tamanioPagina que indica el número de medicamentos por página. Me gustaría que almacenases los ids de cada uno en un array. Hazlo mediante una estructura de funciones de python, comentadas y lo más concisas y a su vez que formen una estructura robusta y escalable."
  },
  {
    "objectID": "posts/Presentaciones/Procesamiento_datos.html#promt-para-buscar-la-estructura",
    "href": "posts/Presentaciones/Procesamiento_datos.html#promt-para-buscar-la-estructura",
    "title": "Procesamiento de prospectos",
    "section": "Promt para buscar la estructura",
    "text": "Promt para buscar la estructura\n\nTe he pasado un archivo JSON en el que los campos son los IDs de unos medicamentos y el alor el prospecto de cada uno de los medicamentos. Puedes buscar campos comunes dentro del texto de los prospectos?"
  },
  {
    "objectID": "posts/Presentaciones/Procesamiento_datos.html#promt-para-buscar-la-estructura-1",
    "href": "posts/Presentaciones/Procesamiento_datos.html#promt-para-buscar-la-estructura-1",
    "title": "Procesamiento de prospectos",
    "section": "Promt para buscar la estructura",
    "text": "Promt para buscar la estructura\n\nTe he pasado un archivo JSON en el que los campos son los IDs de unos medicamentos y el alor el prospecto de cada uno de los medicamentos. Puedes buscar campos comunes dentro del texto de los prospectos?"
  },
  {
    "objectID": "posts/Presentaciones/Procesamiento_datos.html#promt-para-definir-las-clases",
    "href": "posts/Presentaciones/Procesamiento_datos.html#promt-para-definir-las-clases",
    "title": "Procesamiento de prospectos",
    "section": "Promt para definir las clases",
    "text": "Promt para definir las clases\n\nTengo una lista de posibles valores: Comprimidos Comprimidos efervescentes Comprimidos gastrorresistentes Comprimidos recubiertos con película Cápsulas blandas Granulado para solución oral Suspensión oral Solución oral Solución para perfusión Concentrado para solución para perfusión Y una estructura en un lenguage llamado baml: enum MyEnum {   Value1 @alias(“complete_summary”) @description(“Answer in 2 sentences”)   Value2   Value3 @skip   Value4 @description(#”     This is a long description that spans multiple lines.     It can be useful for providing more context to the value.   “#) } Puedes adaptarme la lista a la estructura?"
  },
  {
    "objectID": "posts/Presentaciones/Procesamiento_datos.html#promt-creando-el-chatbot",
    "href": "posts/Presentaciones/Procesamiento_datos.html#promt-creando-el-chatbot",
    "title": "Procesamiento de prospectos",
    "section": "Promt: Creando el chatbot",
    "text": "Promt: Creando el chatbot\n\nEstoy creando una aplicación rag sobre medicamentos analgesicos. El fichero de partida es un json con una estructura de diccionarios como la siguiente: […] Existen tres ficheros python. En primer lugar, data_utils.py: […] En segundo lugar transform_json.py: […] Y por último el fichero que carga los datos en pinecone upload_pinecone.py […] Todo esto funciona. A partir de ahora me gustraía crear una aplicación chatbot que al hacerle preguntas en lenguaje natural sobre medicamento respondiese como un experto médico en analgesicos con la información contenida en la base de datos de pinecone recomendando los medicamentos optimos en cada situación."
  },
  {
    "objectID": "posts/Presentaciones/Procesamiento_datos.html#creando-el-chatbot",
    "href": "posts/Presentaciones/Procesamiento_datos.html#creando-el-chatbot",
    "title": "Procesamiento de prospectos",
    "section": "Creando el chatbot",
    "text": "Creando el chatbot\n\n\n\n\nDiálogo 5 con Gemini\n\n\nEnlace a pinecone"
  },
  {
    "objectID": "posts/Presentaciones/Procesamiento_datos.html#neo4j-y-aura",
    "href": "posts/Presentaciones/Procesamiento_datos.html#neo4j-y-aura",
    "title": "Procesamiento de prospectos",
    "section": "Neo4j y Aura",
    "text": "Neo4j y Aura\n\nNeo4j es la base de datos de grafos líder en el mundo.\nAlmacena y consulta datos como nodos y relaciones.\nIdeal para datos altamente conectados.\nCasos de uso: redes sociales, detección de fraudes, recomendaciones, etc.\nAura la solución en la nube neo4j."
  },
  {
    "objectID": "posts/Presentaciones/Procesamiento_datos.html#por-qué-grafos",
    "href": "posts/Presentaciones/Procesamiento_datos.html#por-qué-grafos",
    "title": "Procesamiento de prospectos",
    "section": "¿Por qué grafos?",
    "text": "¿Por qué grafos?\n\nLos datos están conectados. Los grafos permiten modelar estas conexiones de forma natural.\nLas consultas en grafos son más eficientes para relaciones complejas.\nLos grafos permiten descubrir patrones ocultos en los datos.\n\nEjemplo de código Cypher\nMATCH (usuario:Usuario)-[:SIGUE]-&gt;(otroUsuario:Usuario)-[:PUBLICÓ]-&gt;(articulo:Articulo)\nWHERE usuario.nombre = 'TuNombre'\nRETURN articulo.titulo"
  },
  {
    "objectID": "posts/code/Radiomics_training_masmodelos.html",
    "href": "posts/code/Radiomics_training_masmodelos.html",
    "title": "Radiomics training",
    "section": "",
    "text": "Colab\n\n\n# @title Instalación de Librerías (si es necesario)\n# %pip install pandas numpy scikit-learn matplotlib seaborn\n!pip install catboost\n!pip install dask\n\nRequirement already satisfied: catboost in /usr/local/lib/python3.11/dist-packages (1.2.7)\nRequirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from catboost) (0.20.3)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from catboost) (3.10.0)\nRequirement already satisfied: numpy&lt;2.0,&gt;=1.16.0 in /usr/local/lib/python3.11/dist-packages (from catboost) (1.26.4)\nRequirement already satisfied: pandas&gt;=0.24 in /usr/local/lib/python3.11/dist-packages (from catboost) (2.2.2)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from catboost) (1.13.1)\nRequirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from catboost) (5.24.1)\nRequirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from catboost) (1.17.0)\nRequirement already satisfied: python-dateutil&gt;=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas&gt;=0.24-&gt;catboost) (2.8.2)\nRequirement already satisfied: pytz&gt;=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas&gt;=0.24-&gt;catboost) (2025.1)\nRequirement already satisfied: tzdata&gt;=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas&gt;=0.24-&gt;catboost) (2025.1)\nRequirement already satisfied: contourpy&gt;=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib-&gt;catboost) (1.3.1)\nRequirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib-&gt;catboost) (0.12.1)\nRequirement already satisfied: fonttools&gt;=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib-&gt;catboost) (4.56.0)\nRequirement already satisfied: kiwisolver&gt;=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib-&gt;catboost) (1.4.8)\nRequirement already satisfied: packaging&gt;=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib-&gt;catboost) (24.2)\nRequirement already satisfied: pillow&gt;=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib-&gt;catboost) (11.1.0)\nRequirement already satisfied: pyparsing&gt;=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib-&gt;catboost) (3.2.1)\nRequirement already satisfied: tenacity&gt;=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly-&gt;catboost) (9.0.0)\nRequirement already satisfied: dask in /usr/local/lib/python3.11/dist-packages (2024.10.0)\nRequirement already satisfied: click&gt;=8.1 in /usr/local/lib/python3.11/dist-packages (from dask) (8.1.8)\nRequirement already satisfied: cloudpickle&gt;=3.0.0 in /usr/local/lib/python3.11/dist-packages (from dask) (3.1.1)\nRequirement already satisfied: fsspec&gt;=2021.09.0 in /usr/local/lib/python3.11/dist-packages (from dask) (2024.10.0)\nRequirement already satisfied: packaging&gt;=20.0 in /usr/local/lib/python3.11/dist-packages (from dask) (24.2)\nRequirement already satisfied: partd&gt;=1.4.0 in /usr/local/lib/python3.11/dist-packages (from dask) (1.4.2)\nRequirement already satisfied: pyyaml&gt;=5.3.1 in /usr/local/lib/python3.11/dist-packages (from dask) (6.0.2)\nRequirement already satisfied: toolz&gt;=0.10.0 in /usr/local/lib/python3.11/dist-packages (from dask) (0.12.1)\nRequirement already satisfied: importlib-metadata&gt;=4.13.0 in /usr/local/lib/python3.11/dist-packages (from dask) (8.6.1)\nRequirement already satisfied: zipp&gt;=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata&gt;=4.13.0-&gt;dask) (3.21.0)\nRequirement already satisfied: locket in /usr/local/lib/python3.11/dist-packages (from partd&gt;=1.4.0-&gt;dask) (1.0.0)\n\n\n\n# @title Importar Librerías\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.feature_selection import VarianceThreshold, RFE\nfrom sklearn.impute import SimpleImputer, KNNImputer  # Import KNNImputer\nfrom sklearn.decomposition import PCA, FastICA\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.base import clone\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\n\n\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\n\n\n# @title Definición de la Función de Preprocesamiento y Selección de Características (Modificada)\n\ndef preprocess_and_feature_selection(X_train, y_train_cs, y_train_gleason, X_test=None, y_test_cs=None, y_test_gleason=None,\n                                     variance_threshold=0.01, correlation_threshold=0.95,\n                                     use_sklearn_variance_threshold=False, n_components_pca=None,\n                                     pca_explained_variance=0.95, n_components_ica=None,\n                                     use_rfe=False, n_features_to_select=None, rfe_estimator=None, rfe_step=1,\n                                     gleason_zero_strategy='separate_category', gleason_imputation_strategy='mean'):\n    \"\"\"\n    Preprocesses, selects features, and optionally applies PCA/ICA/RFE.  Handles Gleason Grade Group.\n\n    Args:\n        X_train (pd.DataFrame): Training features.\n        y_train_cs (pd.Series): Clinically Significant training target.\n        y_train_gleason (pd.Series): Gleason Grade Group training target.\n        X_test (pd.DataFrame, optional): Test features.\n        y_test_cs (pd.Series, optional):  Clinically Significant test target.\n        y_test_gleason (pd.Series, optional): Gleason Grade Group test target.\n        variance_threshold (float): Variance threshold.\n        correlation_threshold (float): Correlation threshold.\n        use_sklearn_variance_threshold (bool): Use sklearn's VarianceThreshold.\n        n_components_pca (int/None): PCA components.\n        pca_explained_variance (float): Explained variance for PCA.\n        n_components_ica (int/None): ICA components.\n        use_rfe (bool): Use RFE.\n        n_features_to_select (int/None): Features to select with RFE.\n        rfe_estimator (estimator): Estimator for RFE.\n        rfe_step (int/float): Features to remove at each RFE step.\n        gleason_zero_strategy (str): 'separate_category', 'impute', or 'drop'.\n        gleason_imputation_strategy (str): 'mean', 'median', 'most_frequent', or 'knn'.\n\n    Returns:\n        tuple: (X_train_processed, X_test_processed, dropped_df, results, feature_names)\n               or (X_train, X_test, dropped_df, results, feature_names) if no processing\n    \"\"\"\n    results = {}\n    # --- 0. Handle Gleason Grade Group ---\n    # Make copies to avoid modifying original data\n    X_train = X_train.copy()\n    y_train_gleason = y_train_gleason.copy()\n    if X_test is not None:\n        X_test = X_test.copy()\n        y_test_gleason = y_test_gleason.copy()\n    print(f\"BEFORE Gleason Handling - y_train_gleason dtype: {y_train_gleason.dtype}, unique values: {np.unique(y_train_gleason)}\") #añadido\n    if X_test is not None:\n      print(f\"BEFORE Gleason Handling - y_test_gleason dtype: {y_test_gleason.dtype}, unique values: {np.unique(y_test_gleason)}\")\n\n\n    if gleason_zero_strategy == 'separate_category':\n        # Already handled by making it a separate category.  No action needed here.\n        pass\n    elif gleason_zero_strategy == 'impute':\n        if gleason_imputation_strategy == 'knn':\n            imputer = KNNImputer(n_neighbors=5)  # Or another number of neighbors\n        else:\n            imputer = SimpleImputer(strategy=gleason_imputation_strategy)\n\n        # Impute *only* the Gleason Grade Group column, and only where it's 0.\n        gleason_train_to_impute = y_train_gleason.values.reshape(-1, 1)\n        mask_train = (gleason_train_to_impute == 0)\n        if np.any(mask_train):  # Only impute if there are 0s\n            gleason_train_to_impute = imputer.fit_transform(gleason_train_to_impute)\n        y_train_gleason = gleason_train_to_impute.flatten().astype(int)  # Ensure integer type!\n\n        if X_test is not None:\n            gleason_test_to_impute = y_test_gleason.values.reshape(-1, 1)\n            mask_test = (gleason_test_to_impute == 0)\n            if np.any(mask_test): # Only impute if there are 0s.\n                gleason_test_to_impute = imputer.transform(gleason_test_to_impute) # Use .transform on test!\n            y_test_gleason = gleason_test_to_impute.flatten().astype(int) # Ensure integer type!\n    elif gleason_zero_strategy == 'drop':\n        # Drop rows where Gleason Grade Group is 0.  Least recommended.\n        drop_indices_train = y_train_gleason[y_train_gleason == 0].index\n        X_train = X_train.drop(index=drop_indices_train)\n        y_train_cs = y_train_cs.drop(index=drop_indices_train)\n        y_train_gleason = y_train_gleason.drop(index=drop_indices_train)\n\n        if X_test is not None:\n            drop_indices_test = y_test_gleason[y_test_gleason == 0].index\n            X_test = X_test.drop(index=drop_indices_test)\n            y_test_cs = y_test_cs.drop(index=drop_indices_test)\n            y_test_gleason = y_test_gleason.drop(index=drop_indices_test)\n\n    else:\n        raise ValueError(\"Invalid gleason_zero_strategy.\")\n\n\n    # --- 1. Handle Object Columns ---\n    object_columns = X_train.select_dtypes(include=['object']).columns\n    X_train = X_train.drop(columns=object_columns)\n    if X_test is not None:\n        X_test = X_test.drop(columns=object_columns)\n\n    # --- 2. Handle Missing Values & Imputation (Numerical Features) ---\n    numerical_cols = X_train.select_dtypes(include=np.number).columns\n    imputer_numerical = SimpleImputer(strategy='mean')  # Or 'median', etc.\n\n    if not X_train[numerical_cols].isnull().all().all():\n        X_train[numerical_cols] = imputer_numerical.fit_transform(X_train[numerical_cols])\n        if X_test is not None:\n            X_test[numerical_cols] = imputer_numerical.transform(X_test[numerical_cols])\n    df_numerical = X_train[numerical_cols]\n\n    # --- 3. Variance and Standard Deviation (Optional) ---\n    variances = df_numerical.var()  # For reporting\n\n    # --- 4. Remove Low-Variance Features ---\n    if use_sklearn_variance_threshold:\n        selector = VarianceThreshold(threshold=variance_threshold)\n        X_train_no_low_variance = pd.DataFrame(selector.fit_transform(X_train[numerical_cols]),\n                                               columns=selector.get_feature_names_out(),\n                                               index=X_train.index)\n        if X_test is not None:\n            X_test_no_low_variance = pd.DataFrame(selector.transform(X_test[numerical_cols]),\n                                                  columns=selector.get_feature_names_out(),\n                                                  index=X_test.index)\n        low_variance_cols_dropped = [col for col in X_train.columns if col not in X_train_no_low_variance.columns]\n\n    else:\n        low_variance_cols = variances[variances &lt; variance_threshold].index.tolist()\n        X_train_no_low_variance = X_train.drop(columns=low_variance_cols)\n        if X_test is not None:\n            X_test_no_low_variance = X_test.drop(columns=low_variance_cols)\n        low_variance_cols_dropped = low_variance_cols\n\n    # --- 5. Correlation-Based Feature Selection ---\n    corr_matrix = X_train_no_low_variance.corr().abs()\n    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n\n    to_drop = []\n    dropped_info = {}\n\n    for column in upper.columns:\n        correlated_cols = upper[column][upper[column] &gt; correlation_threshold].index.tolist()\n        if correlated_cols:\n            representative = column\n            for correlated_col in correlated_cols:\n                if correlated_col not in to_drop:\n                    to_drop.append(correlated_col)\n                    dropped_info[correlated_col] = {\n                        'correlation': upper[column][correlated_col],\n                        'representative': representative\n                    }\n    X_train_uncorrelated = X_train_no_low_variance.drop(columns=to_drop)\n    if X_test is not None:\n        X_test_uncorrelated = X_test_no_low_variance.drop(columns=to_drop)\n\n    # --- 6. Feature Scaling ---\n    scaler = StandardScaler()\n    X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train_uncorrelated),\n                                    columns=X_train_uncorrelated.columns,\n                                    index=X_train_uncorrelated.index)\n    if X_test is not None:\n        X_test_scaled = pd.DataFrame(scaler.transform(X_test_uncorrelated),\n                                     columns=X_test_uncorrelated.columns,\n                                     index=X_test_uncorrelated.index)\n    else:\n        X_test_scaled = None\n\n\n    results = {}\n\n    # --- 7. PCA (Optional) ---\n    if n_components_pca is not None or pca_explained_variance is not None:\n        if n_components_pca is None:\n            pca = PCA(n_components=pca_explained_variance, svd_solver='full')\n        else:\n            pca = PCA(n_components=n_components_pca)\n        X_train_pca = pd.DataFrame(pca.fit_transform(X_train_scaled), index=X_train_scaled.index).add_prefix('PC')\n        if X_test is not None:\n            X_test_pca = pd.DataFrame(pca.transform(X_test_scaled), index=X_test_scaled.index).add_prefix('PC')\n        else:\n            X_test_pca = None\n        results['pca'] = {\n            'explained_variance_ratio': pca.explained_variance_ratio_,\n            'cumulative_explained_variance': np.cumsum(pca.explained_variance_ratio_),\n            'n_components': pca.n_components_,\n            'components': pca.components_,\n            'singular_values': pca.singular_values_\n        }\n    else:\n        X_train_pca = None\n        X_test_pca = None\n        results['pca'] = None\n\n    # --- 8. ICA (Optional) ---\n    if n_components_ica is not None:\n        ica = FastICA(n_components=n_components_ica, random_state=42, whiten='unit-variance')\n        X_train_ica = pd.DataFrame(ica.fit_transform(X_train_scaled), index=X_train_scaled.index).add_prefix('IC')\n        if X_test is not None:\n            X_test_ica = pd.DataFrame(ica.transform(X_test_scaled), index=X_test_scaled.index).add_prefix('IC')\n        else:\n            X_test_ica = None\n\n        results['ica'] = {\n            'components': ica.components_,\n            'mixing_matrix': ica.mixing_,\n        }\n    else:\n        X_train_ica = None\n        X_test_ica = None\n        results['ica'] = None\n\n    # --- 9. RFE (Optional) ---\n    if use_rfe:\n        if y_train_cs is None or y_train_gleason is None:\n            raise ValueError(\"RFE requires target variables (y_train_cs and y_train_gleason).\")\n\n        #RFE for Clinically Significant\n        if rfe_estimator is None:\n            rfe_estimator_cs = LogisticRegression(solver='liblinear', random_state=42)\n        else:\n            rfe_estimator_cs = clone(rfe_estimator) # Use a clone of the provided estimator\n        if n_features_to_select is None:\n            n_features_to_select_cs = X_train_scaled.shape[1] // 2\n        else:\n            n_features_to_select_cs = n_features_to_select\n\n        rfe_cs = RFE(estimator=rfe_estimator_cs, n_features_to_select=n_features_to_select_cs, step=rfe_step)\n        X_train_rfe_cs = pd.DataFrame(rfe_cs.fit_transform(X_train_scaled, y_train_cs),\n                                      columns=X_train_scaled.columns[rfe_cs.support_],\n                                      index=X_train_scaled.index)\n        if X_test is not None:\n            X_test_rfe_cs = pd.DataFrame(rfe_cs.transform(X_test_scaled),\n                                         columns=X_train_scaled.columns[rfe_cs.support_],\n                                         index=X_test_scaled.index)\n        else:\n            X_test_rfe_cs = None\n        results['rfe_cs'] = {\n            'support': rfe_cs.support_,\n            'ranking': rfe_cs.ranking_,\n            'estimator': rfe_cs.estimator_,\n        }\n\n        # RFE for Gleason Grade Group\n        if rfe_estimator is None:\n            rfe_estimator_gleason = LogisticRegression(solver='liblinear', multi_class='ovr', random_state=42) #For multiclass\n        else:\n            #Ensure multi_class is appropiate\n            rfe_estimator_gleason = clone(rfe_estimator)\n            if isinstance(rfe_estimator_gleason, LogisticRegression):\n                rfe_estimator_gleason.multi_class = 'ovr'  # Or 'multinomial', depending on your needs\n\n        if n_features_to_select is None:\n          n_features_to_select_gleason = X_train_scaled.shape[1] // 2\n        else:\n            n_features_to_select_gleason = n_features_to_select\n\n        rfe_gleason = RFE(estimator=rfe_estimator_gleason, n_features_to_select=n_features_to_select_gleason, step=rfe_step)\n        X_train_rfe_gleason = pd.DataFrame(rfe_gleason.fit_transform(X_train_scaled, y_train_gleason),\n                                            columns=X_train_scaled.columns[rfe_gleason.support_],\n                                            index=X_train_scaled.index)\n\n        if X_test is not None:\n            X_test_rfe_gleason = pd.DataFrame(rfe_gleason.transform(X_test_scaled),\n                                              columns=X_train_scaled.columns[rfe_gleason.support_],\n                                              index=X_test_scaled.index)\n        else:\n            X_test_rfe_gleason = None\n        results['rfe_gleason'] = {\n            'support': rfe_gleason.support_,\n            'ranking': rfe_gleason.ranking_,\n            'estimator': rfe_gleason.estimator_,\n        }\n\n    else:\n        X_train_rfe_cs = None\n        X_test_rfe_cs = None\n        X_train_rfe_gleason = None\n        X_test_rfe_gleason = None\n        results['rfe_cs'] = None\n        results['rfe_gleason'] = None\n\n    # --- Choose which DataFrame to return ---\n    # Prioritize RFE, then PCA, then ICA, then scaled data\n    if use_rfe:\n        X_train_processed_cs = X_train_rfe_cs\n        X_test_processed_cs = X_test_rfe_cs\n        X_train_processed_gleason = X_train_rfe_gleason\n        X_test_processed_gleason = X_test_rfe_gleason\n    elif X_train_pca is not None:\n        X_train_processed_cs = X_train_pca\n        X_test_processed_cs = X_test_pca\n        X_train_processed_gleason = X_train_pca  # Use PCA for Gleason too if RFE not used\n        X_test_processed_gleason = X_test_pca\n    elif X_train_ica is not None:\n        X_train_processed_cs = X_train_ica\n        X_test_processed_cs = X_test_ica\n        X_train_processed_gleason = X_train_ica  # Use ICA for Gleason too if RFE and PCA not used\n        X_test_processed_gleason = X_test_ica\n    else:\n        X_train_processed_cs = X_train_scaled\n        X_test_processed_cs = X_test_scaled\n        X_train_processed_gleason = X_train_scaled  #Use scaled for Gleason\n        X_test_processed_gleason = X_test_scaled\n\n\n    # --- 10. Create Dropped Columns DataFrame ---\n    dropped_df = pd.DataFrame.from_dict(dropped_info, orient='index')\n    dropped_df = dropped_df.rename_axis('dropped_column').reset_index()\n    low_variance_df = pd.DataFrame({'dropped_column': low_variance_cols_dropped,\n                                     'reason': 'low_variance',\n                                     'variance': [variances[col] for col in low_variance_cols_dropped]})\n    dropped_df = pd.concat([dropped_df, low_variance_df], ignore_index=True)\n\n    # Get feature names *after* all preprocessing steps\n    final_features = X_train_processed_cs.columns.tolist()\n\n    return (X_train_processed_cs, X_test_processed_cs,\n        X_train_processed_gleason, X_test_processed_gleason,\n        dropped_df, results, final_features, X_train_scaled)  # Add X_train_scaled\n\n\n# @title Carga de Datos y Fusión\n# Load the original CSV\ndf = pd.read_csv('output_t2w.csv')\ndf_prostatex_classes = pd.read_csv(\"PROSTATEx_Classes.csv\")\ndf['ClinicallySignificant']=df_prostatex_classes['Clinically Significant'].astype(int)\ndf['GleasonGradeGroup'] = df_prostatex_classes['Gleason Grade Group'].replace('No biopsy information', 0).astype(int)\n\n\n# --- Prepare Target Variables ---\n# 1. Clinically Significant\ny_cs = df['ClinicallySignificant']  # Replace 'ClinicallySignificant'\n\n# 2. Gleason Grade Group\ny_gleason = df['GleasonGradeGroup']  # Replace 'GleasonGradeGroup'\n\n# Drop target variables from the feature matrix\nX = df.drop(columns=['ClinicallySignificant', 'GleasonGradeGroup'])\n\n\n# --- Train/Test Split ---\n# Split *before* preprocessing, but *after* merging.\n# Split BOTH targets.\nX_train, X_test, y_train_cs, y_test_cs, y_train_gleason, y_test_gleason = train_test_split(\n    X, y_cs, y_gleason, test_size=0.2, random_state=42, stratify=y_cs\n) #Added stratify\n\nprint(\"Tamaño de X_train:\", X_train.shape)\nprint(\"Tamaño de y_train_cs:\", y_train_cs.shape)\nprint(\"Tamaño de y_train_gleason:\", y_train_gleason.shape)\nif X_test is not None:\n    print(\"Tamaño de X_test:\", X_test.shape)\n    print(\"Tamaño de y_test_cs:\", y_test_cs.shape if y_test_cs is not None else \"No 'y_test_cs'\")\n    print(\"Tamaño de y_test_gleason:\", y_test_gleason.shape if y_test_gleason is not None else \"No y_test_gleason'\")\n\nTamaño de X_train: (239, 1449)\nTamaño de y_train_cs: (239,)\nTamaño de y_train_gleason: (239,)\nTamaño de X_test: (60, 1449)\nTamaño de y_test_cs: (60,)\nTamaño de y_test_gleason: (60,)\n\n\n\n# @title Ejecución del Preprocesamiento y Selección de Características (con Dos Objetivos)\n# --- Parameters ---\nparams = {\n    'variance_threshold': 0.01,\n    'correlation_threshold': 0.9,\n    'use_sklearn_variance_threshold': False,\n    'n_components_pca': None,\n    'pca_explained_variance': 0.95,\n    'n_components_ica': None,\n    'use_rfe': True,\n    'n_features_to_select': 10,\n    'rfe_estimator': LogisticRegression(solver='liblinear', random_state=42),\n    'rfe_step': 1,\n    'gleason_zero_strategy': 'separate_category',  # 'separate_category', 'impute', 'drop'\n    'gleason_imputation_strategy': 'mean'  # If impute: 'mean', 'median', 'most_frequent', 'knn'\n}\n\n# --- Call the function ---\n(X_train_processed_cs, X_test_processed_cs,\n X_train_processed_gleason, X_test_processed_gleason,\n dropped_df, results, final_features, X_train_scaled) = preprocess_and_feature_selection( #Added X_train_scaled\n    X_train, y_train_cs, y_train_gleason, X_test, y_test_cs, y_test_gleason, **params\n)\n\nprint(\"\\nProcessed Training Data (Clinically Significant - First 5 rows):\")\nprint(X_train_processed_cs.head().to_markdown(index=False, numalign=\"left\", stralign=\"left\"))\nprint(\"\\nProcessed Training Data Info (Clinically Significant):\")\nprint(X_train_processed_cs.info())\n\nif X_test_processed_cs is not None:\n    print(\"\\nProcessed Test Data (Clinically Significant - First 5 rows):\")\n    print(X_test_processed_cs.head().to_markdown(index=False, numalign=\"left\", stralign=\"left\"))\n    print(\"\\nProcessed Test Data Info (Clinically Significant):\")\n    print(X_test_processed_cs.info())\n\nprint(\"\\nProcessed Training Data (Gleason - First 5 rows):\")\nprint(X_train_processed_gleason.head().to_markdown(index=False, numalign=\"left\", stralign=\"left\"))\nprint(\"\\nProcessed Training Data Info (Gleason):\")\nprint(X_train_processed_gleason.info())\n\n\nif X_test_processed_gleason is not None:\n    print(\"\\nProcessed Test Data (Gleason - First 5 rows):\")\n    print(X_test_processed_gleason.head().to_markdown(index=False, numalign=\"left\", stralign=\"left\"))\n    print(\"\\nProcessed Test Data Info (Gleason):\")\n    print(X_test_processed_gleason.info())\n\nprint(\"\\nDropped Columns Information:\")\nprint(dropped_df.to_markdown(index=False, numalign=\"left\", stralign=\"left\"))\n\nprint(\"\\nFinal Features:\", final_features) # Print the final feature names.\n\nBEFORE Gleason Handling - y_train_gleason dtype: int64, unique values: [0 1 2 3 4 5]\nBEFORE Gleason Handling - y_test_gleason dtype: int64, unique values: [0 1 2 3 5]\n\n\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n\n\n\nProcessed Training Data (Clinically Significant - First 5 rows):\n| diagnostics_Image-interpolated_Mean   | original_shape_Maximum3DDiameter   | log-sigma-2-mm-3D_firstorder_Range   | log-sigma-2-mm-3D_ngtdm_Strength   | log-sigma-3-mm-3D_firstorder_Maximum   | log-sigma-3-mm-3D_glcm_DifferenceVariance   | wavelet-HL_gldm_SmallDependenceHighGrayLevelEmphasis   | wavelet-HH_firstorder_RootMeanSquared   | wavelet2-HL_gldm_SmallDependenceHighGrayLevelEmphasis   | wavelet2-LL_gldm_GrayLevelVariance   |\n|:--------------------------------------|:-----------------------------------|:-------------------------------------|:-----------------------------------|:---------------------------------------|:--------------------------------------------|:-------------------------------------------------------|:----------------------------------------|:--------------------------------------------------------|:-------------------------------------|\n| 2.59357                               | -0.150849                          | 0.163608                             | 0.135526                           | 2.24305                                | 2.18493                                     | 0.997                                                  | -0.890941                               | 2.26941                                                 | 0.375179                             |\n| 0.177958                              | -0.656051                          | 0.186288                             | 0.485276                           | 0.638513                               | 0.213254                                    | -0.57791                                               | 0.569297                                | -0.505443                                               | 0.218141                             |\n| 0.110206                              | -0.0973034                         | -0.235787                            | 0.220735                           | 1.72725                                | 0.19854                                     | -0.62962                                               | 0.0321385                               | -0.917162                                               | -0.306996                            |\n| -0.700036                             | 0.23133                            | 1.30006                              | 0.812319                           | -0.0975779                             | 0.973168                                    | 1.08411                                                | 0.0416702                               | 1.23685                                                 | 0.450426                             |\n| -0.847845                             | -0.305254                          | -0.617779                            | -0.409522                          | -0.623337                              | -0.502123                                   | 1.66348                                                | 0.314083                                | 0.670069                                                | -0.51297                             |\n\nProcessed Training Data Info (Clinically Significant):\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 239 entries, 83 to 135\nData columns (total 10 columns):\n #   Column                                                 Non-Null Count  Dtype  \n---  ------                                                 --------------  -----  \n 0   diagnostics_Image-interpolated_Mean                    239 non-null    float64\n 1   original_shape_Maximum3DDiameter                       239 non-null    float64\n 2   log-sigma-2-mm-3D_firstorder_Range                     239 non-null    float64\n 3   log-sigma-2-mm-3D_ngtdm_Strength                       239 non-null    float64\n 4   log-sigma-3-mm-3D_firstorder_Maximum                   239 non-null    float64\n 5   log-sigma-3-mm-3D_glcm_DifferenceVariance              239 non-null    float64\n 6   wavelet-HL_gldm_SmallDependenceHighGrayLevelEmphasis   239 non-null    float64\n 7   wavelet-HH_firstorder_RootMeanSquared                  239 non-null    float64\n 8   wavelet2-HL_gldm_SmallDependenceHighGrayLevelEmphasis  239 non-null    float64\n 9   wavelet2-LL_gldm_GrayLevelVariance                     239 non-null    float64\ndtypes: float64(10)\nmemory usage: 20.5 KB\nNone\n\nProcessed Test Data (Clinically Significant - First 5 rows):\n| diagnostics_Image-interpolated_Mean   | original_shape_Maximum3DDiameter   | log-sigma-2-mm-3D_firstorder_Range   | log-sigma-2-mm-3D_ngtdm_Strength   | log-sigma-3-mm-3D_firstorder_Maximum   | log-sigma-3-mm-3D_glcm_DifferenceVariance   | wavelet-HL_gldm_SmallDependenceHighGrayLevelEmphasis   | wavelet-HH_firstorder_RootMeanSquared   | wavelet2-HL_gldm_SmallDependenceHighGrayLevelEmphasis   | wavelet2-LL_gldm_GrayLevelVariance   |\n|:--------------------------------------|:-----------------------------------|:-------------------------------------|:-----------------------------------|:---------------------------------------|:--------------------------------------------|:-------------------------------------------------------|:----------------------------------------|:--------------------------------------------------------|:-------------------------------------|\n| -0.87431                              | -0.288883                          | -0.551323                            | -0.446649                          | 0.0866886                              | 0.356294                                    | 0.458743                                               | 0.229865                                | 0.500469                                                | -0.202586                            |\n| -1.28946                              | 0.173745                           | 0.415029                             | -0.25487                           | -1.23426                               | -0.274687                                   | -0.396298                                              | 0.369611                                | -0.612893                                               | -0.491164                            |\n| 0.275166                              | 0.478483                           | 1.49084                              | -0.49434                           | -0.318244                              | -0.252272                                   | 1.9672                                                 | 0.145817                                | 2.1662                                                  | -0.423101                            |\n| 0.433288                              | -0.201488                          | -0.636307                            | -0.682271                          | -1.0306                                | 0.196828                                    | -0.852058                                              | -0.494681                               | -0.994781                                               | -0.0447167                           |\n| -0.257166                             | 0.261831                           | 0.456643                             | 0.630335                           | 0.501688                               | -0.56689                                    | 0.533304                                               | 0.315848                                | 0.534895                                                | -0.436182                            |\n\nProcessed Test Data Info (Clinically Significant):\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 60 entries, 224 to 3\nData columns (total 10 columns):\n #   Column                                                 Non-Null Count  Dtype  \n---  ------                                                 --------------  -----  \n 0   diagnostics_Image-interpolated_Mean                    60 non-null     float64\n 1   original_shape_Maximum3DDiameter                       60 non-null     float64\n 2   log-sigma-2-mm-3D_firstorder_Range                     60 non-null     float64\n 3   log-sigma-2-mm-3D_ngtdm_Strength                       60 non-null     float64\n 4   log-sigma-3-mm-3D_firstorder_Maximum                   60 non-null     float64\n 5   log-sigma-3-mm-3D_glcm_DifferenceVariance              60 non-null     float64\n 6   wavelet-HL_gldm_SmallDependenceHighGrayLevelEmphasis   60 non-null     float64\n 7   wavelet-HH_firstorder_RootMeanSquared                  60 non-null     float64\n 8   wavelet2-HL_gldm_SmallDependenceHighGrayLevelEmphasis  60 non-null     float64\n 9   wavelet2-LL_gldm_GrayLevelVariance                     60 non-null     float64\ndtypes: float64(10)\nmemory usage: 5.2 KB\nNone\n\nProcessed Training Data (Gleason - First 5 rows):\n| square_glcm_JointEntropy   | square_glcm_MCC   | square_glcm_SumAverage   | logarithm_glcm_Correlation   | exponential_glcm_Imc2   | exponential_glszm_GrayLevelNonUniformity   | exponential_glszm_ZoneVariance   | exponential_ngtdm_Busyness   | exponential_gldm_GrayLevelNonUniformity   | exponential_gldm_LargeDependenceEmphasis   |\n|:---------------------------|:------------------|:-------------------------|:-----------------------------|:------------------------|:-------------------------------------------|:---------------------------------|:-----------------------------|:------------------------------------------|:-------------------------------------------|\n| -0.0338258                 | 0.590586          | -0.84                    | 2.02808                      | 0.856202                | -0.448497                                  | -0.232784                        | -0.497158                    | -0.345865                                 | 0.266479                                   |\n| 0.0298385                  | 1.03707           | 1.83834                  | 0.0246712                    | -0.0511263              | -0.684313                                  | -0.349829                        | -0.394237                    | -0.530825                                 | 0.412659                                   |\n| 0.848769                   | 0.939159          | 2.96263                  | 0.438299                     | -0.457702               | -0.495558                                  | 0.947389                         | 1.24831                      | 0.0604506                                 | 2.27271                                    |\n| 0.555694                   | 0.779582          | -0.0459777               | 0.633297                     | 0.635065                | -0.337855                                  | -0.418733                        | -0.579732                    | -0.290732                                 | -0.0910274                                 |\n| -0.0648107                 | 0.302929          | -0.484855                | -0.9528                      | -0.540555               | -0.26114                                   | -0.483809                        | -0.196956                    | -0.483699                                 | -0.503282                                  |\n\nProcessed Training Data Info (Gleason):\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 239 entries, 83 to 135\nData columns (total 10 columns):\n #   Column                                    Non-Null Count  Dtype  \n---  ------                                    --------------  -----  \n 0   square_glcm_JointEntropy                  239 non-null    float64\n 1   square_glcm_MCC                           239 non-null    float64\n 2   square_glcm_SumAverage                    239 non-null    float64\n 3   logarithm_glcm_Correlation                239 non-null    float64\n 4   exponential_glcm_Imc2                     239 non-null    float64\n 5   exponential_glszm_GrayLevelNonUniformity  239 non-null    float64\n 6   exponential_glszm_ZoneVariance            239 non-null    float64\n 7   exponential_ngtdm_Busyness                239 non-null    float64\n 8   exponential_gldm_GrayLevelNonUniformity   239 non-null    float64\n 9   exponential_gldm_LargeDependenceEmphasis  239 non-null    float64\ndtypes: float64(10)\nmemory usage: 20.5 KB\nNone\n\nProcessed Test Data (Gleason - First 5 rows):\n| square_glcm_JointEntropy   | square_glcm_MCC   | square_glcm_SumAverage   | logarithm_glcm_Correlation   | exponential_glcm_Imc2   | exponential_glszm_GrayLevelNonUniformity   | exponential_glszm_ZoneVariance   | exponential_ngtdm_Busyness   | exponential_gldm_GrayLevelNonUniformity   | exponential_gldm_LargeDependenceEmphasis   |\n|:---------------------------|:------------------|:-------------------------|:-----------------------------|:------------------------|:-------------------------------------------|:---------------------------------|:-----------------------------|:------------------------------------------|:-------------------------------------------|\n| -0.887459                  | -0.898396         | -0.93918                 | -0.310247                    | -0.00768085             | -0.42583                                   | -0.519758                        | -0.426601                    | -0.563325                                 | -0.639787                                  |\n| -1.10778                   | 0.855466          | -1.08291                 | -1.21409                     | -0.258768               | 0.139898                                   | -0.527216                        | -0.513559                    | -0.452587                                 | -0.919128                                  |\n| -0.832634                  | -1.52239          | -1.10551                 | 0.0373886                    | -0.363856               | 0.863803                                   | -0.391082                        | -0.190766                    | 0.226225                                  | -0.30727                                   |\n| -1.22075                   | 0.470366          | -0.900271                | 0.647113                     | 1.079                   | -0.78285                                   | -0.535617                        | -0.665774                    | -0.721991                                 | -0.891123                                  |\n| -0.718603                  | -0.16469          | -1.09698                 | 0.209374                     | -0.430115               | 0.744166                                   | -0.437632                        | 0.134222                     | 0.0495357                                 | -0.297468                                  |\n\nProcessed Test Data Info (Gleason):\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 60 entries, 224 to 3\nData columns (total 10 columns):\n #   Column                                    Non-Null Count  Dtype  \n---  ------                                    --------------  -----  \n 0   square_glcm_JointEntropy                  60 non-null     float64\n 1   square_glcm_MCC                           60 non-null     float64\n 2   square_glcm_SumAverage                    60 non-null     float64\n 3   logarithm_glcm_Correlation                60 non-null     float64\n 4   exponential_glcm_Imc2                     60 non-null     float64\n 5   exponential_glszm_GrayLevelNonUniformity  60 non-null     float64\n 6   exponential_glszm_ZoneVariance            60 non-null     float64\n 7   exponential_ngtdm_Busyness                60 non-null     float64\n 8   exponential_gldm_GrayLevelNonUniformity   60 non-null     float64\n 9   exponential_gldm_LargeDependenceEmphasis  60 non-null     float64\ndtypes: float64(10)\nmemory usage: 5.2 KB\nNone\n\nDropped Columns Information:\n| dropped_column                                                | correlation   | representative                                                | reason       | variance    |\n|:--------------------------------------------------------------|:--------------|:--------------------------------------------------------------|:-------------|:------------|\n| diagnostics_Mask-original_VoxelNum                            | 0.966508      | diagnostics_Mask-interpolated_VoxelNum                        | nan          | nan         |\n| diagnostics_Mask-original_VolumeNum                           | 0.961168      | diagnostics_Mask-interpolated_VolumeNum                       | nan          | nan         |\n| original_shape_MajorAxisLength                                | 0.916841      | original_shape_Maximum2DDiameterColumn                        | nan          | nan         |\n| original_shape_Maximum2DDiameterSlice                         | 0.927712      | original_shape_Maximum3DDiameter                              | nan          | nan         |\n| diagnostics_Mask-interpolated_VoxelNum                        | 0.991962      | original_shape_MeshVolume                                     | nan          | nan         |\n| original_shape_MeshVolume                                     | 0.979766      | original_shape_SurfaceArea                                    | nan          | nan         |\n| original_shape_MinorAxisLength                                | 0.926681      | original_shape_SurfaceArea                                    | nan          | nan         |\n| original_shape_SurfaceArea                                    | 0.980459      | original_shape_VoxelVolume                                    | nan          | nan         |\n| diagnostics_Mask-interpolated_Mean                            | 0.936552      | original_firstorder_10Percentile                              | nan          | nan         |\n| original_shape_VoxelVolume                                    | 0.966481      | original_firstorder_Energy                                    | nan          | nan         |\n| diagnostics_Mask-interpolated_Maximum                         | 1             | original_firstorder_Maximum                                   | nan          | nan         |\n| original_firstorder_InterquartileRange                        | 0.981335      | original_firstorder_MeanAbsoluteDeviation                     | nan          | nan         |\n| original_firstorder_10Percentile                              | 0.936552      | original_firstorder_Mean                                      | nan          | nan         |\n| original_firstorder_90Percentile                              | 0.959433      | original_firstorder_Mean                                      | nan          | nan         |\n| original_firstorder_Mean                                      | 0.995761      | original_firstorder_Median                                    | nan          | nan         |\n| diagnostics_Mask-interpolated_Minimum                         | 1             | original_firstorder_Minimum                                   | nan          | nan         |\n| original_firstorder_MeanAbsoluteDeviation                     | 0.98802       | original_firstorder_RobustMeanAbsoluteDeviation               | nan          | nan         |\n| original_firstorder_Median                                    | 0.993754      | original_firstorder_RootMeanSquared                           | nan          | nan         |\n| original_firstorder_Energy                                    | 0.989627      | original_firstorder_TotalEnergy                               | nan          | nan         |\n| original_firstorder_RobustMeanAbsoluteDeviation               | 0.953581      | original_firstorder_Variance                                  | nan          | nan         |\n| original_firstorder_Variance                                  | 0.90126       | original_glcm_ClusterProminence                               | nan          | nan         |\n| original_glcm_ClusterProminence                               | 0.907851      | original_glcm_ClusterTendency                                 | nan          | nan         |\n| original_glcm_Contrast                                        | 0.968414      | original_glcm_DifferenceAverage                               | nan          | nan         |\n| original_firstorder_Entropy                                   | 0.971562      | original_glcm_DifferenceEntropy                               | nan          | nan         |\n| original_glcm_DifferenceAverage                               | 0.928101      | original_glcm_DifferenceVariance                              | nan          | nan         |\n| original_glcm_Autocorrelation                                 | 0.974414      | original_glcm_JointAverage                                    | nan          | nan         |\n| original_shape_LeastAxisLength                                | 0.913478      | original_glcm_JointEntropy                                    | nan          | nan         |\n| original_shape_SurfaceVolumeRatio                             | 0.943731      | original_glcm_JointEntropy                                    | nan          | nan         |\n| original_glcm_JointAverage                                    | 1             | original_glcm_SumAverage                                      | nan          | nan         |\n| original_glcm_DifferenceEntropy                               | 0.941672      | original_glcm_SumEntropy                                      | nan          | nan         |\n| original_glcm_JointEntropy                                    | 0.94654       | original_glcm_SumEntropy                                      | nan          | nan         |\n| original_glcm_ClusterTendency                                 | 0.993419      | original_glcm_SumSquares                                      | nan          | nan         |\n| original_glcm_Imc1                                            | 0.934305      | original_glrlm_GrayLevelNonUniformity                         | nan          | nan         |\n| original_glcm_SumSquares                                      | 0.997396      | original_glrlm_GrayLevelVariance                              | nan          | nan         |\n| original_glcm_SumAverage                                      | 0.965423      | original_glrlm_HighGrayLevelRunEmphasis                       | nan          | nan         |\n| original_glrlm_HighGrayLevelRunEmphasis                       | 0.999939      | original_glrlm_LongRunHighGrayLevelEmphasis                   | nan          | nan         |\n| original_glcm_SumEntropy                                      | 0.978986      | original_glrlm_RunEntropy                                     | nan          | nan         |\n| original_firstorder_TotalEnergy                               | 0.953379      | original_glrlm_RunLengthNonUniformity                         | nan          | nan         |\n| original_glrlm_LongRunHighGrayLevelEmphasis                   | 0.999905      | original_glrlm_ShortRunHighGrayLevelEmphasis                  | nan          | nan         |\n| original_glrlm_GrayLevelNonUniformity                         | 0.999762      | original_glszm_GrayLevelNonUniformity                         | nan          | nan         |\n| original_glrlm_GrayLevelVariance                              | 0.999984      | original_glszm_GrayLevelVariance                              | nan          | nan         |\n| original_glrlm_ShortRunHighGrayLevelEmphasis                  | 0.999991      | original_glszm_HighGrayLevelZoneEmphasis                      | nan          | nan         |\n| original_glszm_HighGrayLevelZoneEmphasis                      | 0.998967      | original_glszm_LargeAreaHighGrayLevelEmphasis                 | nan          | nan         |\n| original_glrlm_RunLengthNonUniformity                         | 0.999736      | original_glszm_SizeZoneNonUniformity                          | nan          | nan         |\n| original_glszm_LargeAreaHighGrayLevelEmphasis                 | 0.99841       | original_glszm_SmallAreaHighGrayLevelEmphasis                 | nan          | nan         |\n| original_glrlm_RunEntropy                                     | 0.999417      | original_glszm_ZoneEntropy                                    | nan          | nan         |\n| original_glszm_ZoneEntropy                                    | 0.999703      | original_gldm_DependenceEntropy                               | nan          | nan         |\n| original_glszm_SizeZoneNonUniformity                          | 0.999965      | original_gldm_DependenceNonUniformity                         | nan          | nan         |\n| original_glszm_GrayLevelNonUniformity                         | 0.999575      | original_gldm_GrayLevelNonUniformity                          | nan          | nan         |\n| original_glszm_GrayLevelVariance                              | 0.999971      | original_gldm_GrayLevelVariance                               | nan          | nan         |\n| original_glszm_SmallAreaHighGrayLevelEmphasis                 | 0.999923      | original_gldm_HighGrayLevelEmphasis                           | nan          | nan         |\n| original_gldm_HighGrayLevelEmphasis                           | 0.996417      | original_gldm_LargeDependenceHighGrayLevelEmphasis            | nan          | nan         |\n| original_gldm_LargeDependenceHighGrayLevelEmphasis            | 0.994369      | original_gldm_SmallDependenceHighGrayLevelEmphasis            | nan          | nan         |\n| original_gldm_DependenceNonUniformity                         | 0.999229      | log-sigma-0-6-mm-3D_firstorder_Energy                         | nan          | nan         |\n| original_gldm_DependenceEntropy                               | 0.914611      | log-sigma-0-6-mm-3D_firstorder_Entropy                        | nan          | nan         |\n| original_glcm_DifferenceVariance                              | 0.908336      | log-sigma-0-6-mm-3D_firstorder_MeanAbsoluteDeviation          | nan          | nan         |\n| log-sigma-0-6-mm-3D_firstorder_90Percentile                   | 0.903086      | log-sigma-0-6-mm-3D_firstorder_MeanAbsoluteDeviation          | nan          | nan         |\n| log-sigma-0-6-mm-3D_firstorder_InterquartileRange             | 0.984481      | log-sigma-0-6-mm-3D_firstorder_MeanAbsoluteDeviation          | nan          | nan         |\n| log-sigma-0-6-mm-3D_firstorder_Mean                           | 0.978146      | log-sigma-0-6-mm-3D_firstorder_Median                         | nan          | nan         |\n| log-sigma-0-6-mm-3D_firstorder_Minimum                        | 0.916205      | log-sigma-0-6-mm-3D_firstorder_Range                          | nan          | nan         |\n| log-sigma-0-6-mm-3D_firstorder_MeanAbsoluteDeviation          | 0.990527      | log-sigma-0-6-mm-3D_firstorder_RobustMeanAbsoluteDeviation    | nan          | nan         |\n| log-sigma-0-6-mm-3D_firstorder_Median                         | 0.977922      | log-sigma-0-6-mm-3D_firstorder_RootMeanSquared                | nan          | nan         |\n| log-sigma-0-6-mm-3D_firstorder_Energy                         | 0.991951      | log-sigma-0-6-mm-3D_firstorder_TotalEnergy                    | nan          | nan         |\n| log-sigma-0-6-mm-3D_firstorder_RobustMeanAbsoluteDeviation    | 0.949705      | log-sigma-0-6-mm-3D_firstorder_Variance                       | nan          | nan         |\n| log-sigma-0-6-mm-3D_firstorder_Range                          | 0.916807      | log-sigma-0-6-mm-3D_glcm_Autocorrelation                      | nan          | nan         |\n| log-sigma-0-6-mm-3D_firstorder_Variance                       | 0.988447      | log-sigma-0-6-mm-3D_glcm_ClusterTendency                      | nan          | nan         |\n| log-sigma-0-6-mm-3D_glcm_ClusterTendency                      | 0.903064      | log-sigma-0-6-mm-3D_glcm_DifferenceAverage                    | nan          | nan         |\n| log-sigma-0-6-mm-3D_glcm_Contrast                             | 0.96167       | log-sigma-0-6-mm-3D_glcm_DifferenceAverage                    | nan          | nan         |\n| log-sigma-0-6-mm-3D_firstorder_Entropy                        | 0.979846      | log-sigma-0-6-mm-3D_glcm_DifferenceEntropy                    | nan          | nan         |\n| log-sigma-0-6-mm-3D_glcm_DifferenceAverage                    | 0.940353      | log-sigma-0-6-mm-3D_glcm_DifferenceVariance                   | nan          | nan         |\n| log-sigma-0-6-mm-3D_glcm_Autocorrelation                      | 0.978892      | log-sigma-0-6-mm-3D_glcm_JointAverage                         | nan          | nan         |\n| log-sigma-0-6-mm-3D_glcm_Imc1                                 | 0.903853      | log-sigma-0-6-mm-3D_glcm_JointEntropy                         | nan          | nan         |\n| log-sigma-0-6-mm-3D_glcm_JointAverage                         | 1             | log-sigma-0-6-mm-3D_glcm_SumAverage                           | nan          | nan         |\n| log-sigma-0-6-mm-3D_glcm_DifferenceEntropy                    | 0.92477       | log-sigma-0-6-mm-3D_glcm_SumEntropy                           | nan          | nan         |\n| log-sigma-0-6-mm-3D_glcm_DifferenceVariance                   | 0.968751      | log-sigma-0-6-mm-3D_glcm_SumSquares                           | nan          | nan         |\n| log-sigma-0-6-mm-3D_firstorder_TotalEnergy                    | 0.959727      | log-sigma-0-6-mm-3D_glrlm_GrayLevelNonUniformity              | nan          | nan         |\n| log-sigma-0-6-mm-3D_glcm_SumSquares                           | 0.998151      | log-sigma-0-6-mm-3D_glrlm_GrayLevelVariance                   | nan          | nan         |\n| log-sigma-0-6-mm-3D_glcm_SumAverage                           | 0.978541      | log-sigma-0-6-mm-3D_glrlm_HighGrayLevelRunEmphasis            | nan          | nan         |\n| log-sigma-0-6-mm-3D_glrlm_HighGrayLevelRunEmphasis            | 0.999946      | log-sigma-0-6-mm-3D_glrlm_LongRunHighGrayLevelEmphasis        | nan          | nan         |\n| log-sigma-0-6-mm-3D_glcm_SumEntropy                           | 0.975849      | log-sigma-0-6-mm-3D_glrlm_RunEntropy                          | nan          | nan         |\n| log-sigma-0-6-mm-3D_glrlm_GrayLevelNonUniformity              | 0.96234       | log-sigma-0-6-mm-3D_glrlm_RunLengthNonUniformity              | nan          | nan         |\n| log-sigma-0-6-mm-3D_glrlm_LongRunHighGrayLevelEmphasis        | 0.999916      | log-sigma-0-6-mm-3D_glrlm_ShortRunHighGrayLevelEmphasis       | nan          | nan         |\n| log-sigma-0-6-mm-3D_glrlm_RunLengthNonUniformity              | 0.964307      | log-sigma-0-6-mm-3D_glszm_GrayLevelNonUniformity              | nan          | nan         |\n| log-sigma-0-6-mm-3D_glrlm_GrayLevelVariance                   | 0.99995       | log-sigma-0-6-mm-3D_glszm_GrayLevelVariance                   | nan          | nan         |\n| log-sigma-0-6-mm-3D_glrlm_ShortRunHighGrayLevelEmphasis       | 0.999992      | log-sigma-0-6-mm-3D_glszm_HighGrayLevelZoneEmphasis           | nan          | nan         |\n| log-sigma-0-6-mm-3D_glszm_HighGrayLevelZoneEmphasis           | 0.999152      | log-sigma-0-6-mm-3D_glszm_LargeAreaHighGrayLevelEmphasis      | nan          | nan         |\n| log-sigma-0-6-mm-3D_glszm_GrayLevelNonUniformity              | 0.959159      | log-sigma-0-6-mm-3D_glszm_SizeZoneNonUniformity               | nan          | nan         |\n| log-sigma-0-6-mm-3D_glszm_LargeAreaHighGrayLevelEmphasis      | 0.998673      | log-sigma-0-6-mm-3D_glszm_SmallAreaHighGrayLevelEmphasis      | nan          | nan         |\n| log-sigma-0-6-mm-3D_glrlm_RunEntropy                          | 0.99783       | log-sigma-0-6-mm-3D_glszm_ZoneEntropy                         | nan          | nan         |\n| original_ngtdm_Contrast                                       | 0.996652      | log-sigma-0-6-mm-3D_ngtdm_Contrast                            | nan          | nan         |\n| log-sigma-0-6-mm-3D_glszm_ZoneEntropy                         | 0.998811      | log-sigma-0-6-mm-3D_gldm_DependenceEntropy                    | nan          | nan         |\n| log-sigma-0-6-mm-3D_glszm_SizeZoneNonUniformity               | 0.999967      | log-sigma-0-6-mm-3D_gldm_DependenceNonUniformity              | nan          | nan         |\n| log-sigma-0-6-mm-3D_gldm_DependenceNonUniformity              | 0.954164      | log-sigma-0-6-mm-3D_gldm_GrayLevelNonUniformity               | nan          | nan         |\n| log-sigma-0-6-mm-3D_glszm_GrayLevelVariance                   | 0.999912      | log-sigma-0-6-mm-3D_gldm_GrayLevelVariance                    | nan          | nan         |\n| log-sigma-0-6-mm-3D_glszm_SmallAreaHighGrayLevelEmphasis      | 0.999925      | log-sigma-0-6-mm-3D_gldm_HighGrayLevelEmphasis                | nan          | nan         |\n| log-sigma-0-6-mm-3D_gldm_HighGrayLevelEmphasis                | 0.997275      | log-sigma-0-6-mm-3D_gldm_LargeDependenceHighGrayLevelEmphasis | nan          | nan         |\n| log-sigma-0-6-mm-3D_gldm_LargeDependenceHighGrayLevelEmphasis | 0.99555       | log-sigma-0-6-mm-3D_gldm_SmallDependenceHighGrayLevelEmphasis | nan          | nan         |\n| log-sigma-0-6-mm-3D_gldm_GrayLevelNonUniformity               | 0.960056      | log-sigma-2-mm-3D_firstorder_Energy                           | nan          | nan         |\n| log-sigma-0-6-mm-3D_gldm_DependenceEntropy                    | 0.942983      | log-sigma-2-mm-3D_firstorder_Entropy                          | nan          | nan         |\n| log-sigma-2-mm-3D_firstorder_InterquartileRange               | 0.970367      | log-sigma-2-mm-3D_firstorder_MeanAbsoluteDeviation            | nan          | nan         |\n| log-sigma-2-mm-3D_firstorder_Mean                             | 0.983187      | log-sigma-2-mm-3D_firstorder_Median                           | nan          | nan         |\n| log-sigma-2-mm-3D_firstorder_MeanAbsoluteDeviation            | 0.983906      | log-sigma-2-mm-3D_firstorder_RobustMeanAbsoluteDeviation      | nan          | nan         |\n| log-sigma-2-mm-3D_firstorder_Median                           | 0.983367      | log-sigma-2-mm-3D_firstorder_RootMeanSquared                  | nan          | nan         |\n| log-sigma-2-mm-3D_firstorder_Energy                           | 0.992364      | log-sigma-2-mm-3D_firstorder_TotalEnergy                      | nan          | nan         |\n| log-sigma-2-mm-3D_firstorder_RobustMeanAbsoluteDeviation      | 0.92871       | log-sigma-2-mm-3D_firstorder_Variance                         | nan          | nan         |\n| log-sigma-2-mm-3D_firstorder_Variance                         | 0.996295      | log-sigma-2-mm-3D_glcm_ClusterTendency                        | nan          | nan         |\n| log-sigma-2-mm-3D_glcm_Contrast                               | 0.964431      | log-sigma-2-mm-3D_glcm_DifferenceAverage                      | nan          | nan         |\n| log-sigma-2-mm-3D_glcm_DifferenceAverage                      | 0.915392      | log-sigma-2-mm-3D_glcm_DifferenceVariance                     | nan          | nan         |\n| log-sigma-2-mm-3D_glcm_Autocorrelation                        | 0.960234      | log-sigma-2-mm-3D_glcm_JointAverage                           | nan          | nan         |\n| log-sigma-0-6-mm-3D_glcm_JointEntropy                         | 0.998154      | log-sigma-2-mm-3D_glcm_JointEntropy                           | nan          | nan         |\n| log-sigma-2-mm-3D_glcm_JointAverage                           | 1             | log-sigma-2-mm-3D_glcm_SumAverage                             | nan          | nan         |\n| log-sigma-2-mm-3D_firstorder_Entropy                          | 0.961535      | log-sigma-2-mm-3D_glcm_SumEntropy                             | nan          | nan         |\n| log-sigma-2-mm-3D_glcm_JointEntropy                           | 0.93664       | log-sigma-2-mm-3D_glcm_SumEntropy                             | nan          | nan         |\n| log-sigma-2-mm-3D_glcm_ClusterTendency                        | 0.999355      | log-sigma-2-mm-3D_glcm_SumSquares                             | nan          | nan         |\n| log-sigma-2-mm-3D_firstorder_TotalEnergy                      | 0.943642      | log-sigma-2-mm-3D_glrlm_GrayLevelNonUniformity                | nan          | nan         |\n| log-sigma-2-mm-3D_glcm_SumSquares                             | 0.997895      | log-sigma-2-mm-3D_glrlm_GrayLevelVariance                     | nan          | nan         |\n| log-sigma-2-mm-3D_glcm_SumAverage                             | 0.959924      | log-sigma-2-mm-3D_glrlm_HighGrayLevelRunEmphasis              | nan          | nan         |\n| log-sigma-2-mm-3D_glrlm_HighGrayLevelRunEmphasis              | 0.999536      | log-sigma-2-mm-3D_glrlm_LongRunHighGrayLevelEmphasis          | nan          | nan         |\n| log-sigma-2-mm-3D_glcm_SumEntropy                             | 0.974878      | log-sigma-2-mm-3D_glrlm_RunEntropy                            | nan          | nan         |\n| log-sigma-2-mm-3D_glrlm_GrayLevelNonUniformity                | 0.946704      | log-sigma-2-mm-3D_glrlm_RunLengthNonUniformity                | nan          | nan         |\n| log-sigma-2-mm-3D_glrlm_LongRunHighGrayLevelEmphasis          | 0.999307      | log-sigma-2-mm-3D_glrlm_ShortRunHighGrayLevelEmphasis         | nan          | nan         |\n| log-sigma-2-mm-3D_glrlm_RunLengthNonUniformity                | 0.953989      | log-sigma-2-mm-3D_glszm_GrayLevelNonUniformity                | nan          | nan         |\n| log-sigma-2-mm-3D_glrlm_GrayLevelVariance                     | 0.999864      | log-sigma-2-mm-3D_glszm_GrayLevelVariance                     | nan          | nan         |\n| log-sigma-2-mm-3D_glrlm_ShortRunHighGrayLevelEmphasis         | 0.999963      | log-sigma-2-mm-3D_glszm_HighGrayLevelZoneEmphasis             | nan          | nan         |\n| log-sigma-2-mm-3D_glszm_HighGrayLevelZoneEmphasis             | 0.991665      | log-sigma-2-mm-3D_glszm_LargeAreaHighGrayLevelEmphasis        | nan          | nan         |\n| log-sigma-2-mm-3D_glszm_GrayLevelNonUniformity                | 0.936716      | log-sigma-2-mm-3D_glszm_SizeZoneNonUniformity                 | nan          | nan         |\n| log-sigma-2-mm-3D_glszm_LargeAreaHighGrayLevelEmphasis        | 0.98813       | log-sigma-2-mm-3D_glszm_SmallAreaHighGrayLevelEmphasis        | nan          | nan         |\n| log-sigma-2-mm-3D_glrlm_RunEntropy                            | 0.992768      | log-sigma-2-mm-3D_glszm_ZoneEntropy                           | nan          | nan         |\n| log-sigma-2-mm-3D_glszm_ZoneEntropy                           | 0.996719      | log-sigma-2-mm-3D_gldm_DependenceEntropy                      | nan          | nan         |\n| log-sigma-2-mm-3D_glszm_SizeZoneNonUniformity                 | 0.999821      | log-sigma-2-mm-3D_gldm_DependenceNonUniformity                | nan          | nan         |\n| log-sigma-2-mm-3D_gldm_DependenceNonUniformity                | 0.917737      | log-sigma-2-mm-3D_gldm_GrayLevelNonUniformity                 | nan          | nan         |\n| log-sigma-2-mm-3D_glszm_GrayLevelVariance                     | 0.99976       | log-sigma-2-mm-3D_gldm_GrayLevelVariance                      | nan          | nan         |\n| log-sigma-2-mm-3D_glszm_SmallAreaHighGrayLevelEmphasis        | 0.999516      | log-sigma-2-mm-3D_gldm_HighGrayLevelEmphasis                  | nan          | nan         |\n| log-sigma-2-mm-3D_glszm_LargeAreaEmphasis                     | 0.994914      | log-sigma-2-mm-3D_gldm_LargeDependenceEmphasis                | nan          | nan         |\n| log-sigma-2-mm-3D_gldm_HighGrayLevelEmphasis                  | 0.979813      | log-sigma-2-mm-3D_gldm_LargeDependenceHighGrayLevelEmphasis   | nan          | nan         |\n| log-sigma-2-mm-3D_gldm_LargeDependenceHighGrayLevelEmphasis   | 0.96816       | log-sigma-2-mm-3D_gldm_SmallDependenceHighGrayLevelEmphasis   | nan          | nan         |\n| log-sigma-2-mm-3D_gldm_GrayLevelNonUniformity                 | 0.943562      | log-sigma-3-mm-3D_firstorder_Energy                           | nan          | nan         |\n| log-sigma-2-mm-3D_gldm_DependenceEntropy                      | 0.954633      | log-sigma-3-mm-3D_firstorder_Entropy                          | nan          | nan         |\n| log-sigma-3-mm-3D_firstorder_InterquartileRange               | 0.972919      | log-sigma-3-mm-3D_firstorder_MeanAbsoluteDeviation            | nan          | nan         |\n| log-sigma-3-mm-3D_firstorder_10Percentile                     | 0.901888      | log-sigma-3-mm-3D_firstorder_Mean                             | nan          | nan         |\n| log-sigma-3-mm-3D_firstorder_90Percentile                     | 0.90952       | log-sigma-3-mm-3D_firstorder_Mean                             | nan          | nan         |\n| log-sigma-3-mm-3D_firstorder_Mean                             | 0.991088      | log-sigma-3-mm-3D_firstorder_Median                           | nan          | nan         |\n| log-sigma-3-mm-3D_firstorder_MeanAbsoluteDeviation            | 0.984811      | log-sigma-3-mm-3D_firstorder_RobustMeanAbsoluteDeviation      | nan          | nan         |\n| log-sigma-3-mm-3D_firstorder_Median                           | 0.991131      | log-sigma-3-mm-3D_firstorder_RootMeanSquared                  | nan          | nan         |\n| log-sigma-3-mm-3D_firstorder_Energy                           | 0.992723      | log-sigma-3-mm-3D_firstorder_TotalEnergy                      | nan          | nan         |\n| log-sigma-3-mm-3D_firstorder_RobustMeanAbsoluteDeviation      | 0.93751       | log-sigma-3-mm-3D_firstorder_Variance                         | nan          | nan         |\n| log-sigma-3-mm-3D_firstorder_Variance                         | 0.997401      | log-sigma-3-mm-3D_glcm_ClusterTendency                        | nan          | nan         |\n| log-sigma-3-mm-3D_glcm_Contrast                               | 0.978117      | log-sigma-3-mm-3D_glcm_DifferenceAverage                      | nan          | nan         |\n| log-sigma-2-mm-3D_glcm_DifferenceEntropy                      | 0.90735       | log-sigma-3-mm-3D_glcm_DifferenceEntropy                      | nan          | nan         |\n| log-sigma-2-mm-3D_glcm_Imc1                                   | 0.967664      | log-sigma-3-mm-3D_glcm_Imc1                                   | nan          | nan         |\n| log-sigma-3-mm-3D_glcm_Autocorrelation                        | 0.960812      | log-sigma-3-mm-3D_glcm_JointAverage                           | nan          | nan         |\n| log-sigma-3-mm-3D_glcm_JointAverage                           | 1             | log-sigma-3-mm-3D_glcm_SumAverage                             | nan          | nan         |\n| log-sigma-3-mm-3D_firstorder_Entropy                          | 0.970659      | log-sigma-3-mm-3D_glcm_SumEntropy                             | nan          | nan         |\n| log-sigma-3-mm-3D_glcm_JointEntropy                           | 0.94977       | log-sigma-3-mm-3D_glcm_SumEntropy                             | nan          | nan         |\n| log-sigma-3-mm-3D_glcm_ClusterTendency                        | 0.999794      | log-sigma-3-mm-3D_glcm_SumSquares                             | nan          | nan         |\n| log-sigma-3-mm-3D_firstorder_TotalEnergy                      | 0.908876      | log-sigma-3-mm-3D_glrlm_GrayLevelNonUniformity                | nan          | nan         |\n| log-sigma-3-mm-3D_glcm_SumSquares                             | 0.998345      | log-sigma-3-mm-3D_glrlm_GrayLevelVariance                     | nan          | nan         |\n| log-sigma-3-mm-3D_glcm_SumAverage                             | 0.95846       | log-sigma-3-mm-3D_glrlm_HighGrayLevelRunEmphasis              | nan          | nan         |\n| log-sigma-3-mm-3D_glrlm_HighGrayLevelRunEmphasis              | 0.999133      | log-sigma-3-mm-3D_glrlm_LongRunHighGrayLevelEmphasis          | nan          | nan         |\n| log-sigma-3-mm-3D_glcm_SumEntropy                             | 0.982282      | log-sigma-3-mm-3D_glrlm_RunEntropy                            | nan          | nan         |\n| log-sigma-3-mm-3D_glrlm_GrayLevelNonUniformity                | 0.911714      | log-sigma-3-mm-3D_glrlm_RunLengthNonUniformity                | nan          | nan         |\n| log-sigma-3-mm-3D_glrlm_LongRunHighGrayLevelEmphasis          | 0.998759      | log-sigma-3-mm-3D_glrlm_ShortRunHighGrayLevelEmphasis         | nan          | nan         |\n| log-sigma-3-mm-3D_glrlm_RunLengthNonUniformity                | 0.925724      | log-sigma-3-mm-3D_glszm_GrayLevelNonUniformity                | nan          | nan         |\n| log-sigma-3-mm-3D_glrlm_GrayLevelVariance                     | 0.999799      | log-sigma-3-mm-3D_glszm_GrayLevelVariance                     | nan          | nan         |\n| log-sigma-3-mm-3D_glrlm_ShortRunHighGrayLevelEmphasis         | 0.999932      | log-sigma-3-mm-3D_glszm_HighGrayLevelZoneEmphasis             | nan          | nan         |\n| log-sigma-3-mm-3D_glszm_HighGrayLevelZoneEmphasis             | 0.983591      | log-sigma-3-mm-3D_glszm_LargeAreaHighGrayLevelEmphasis        | nan          | nan         |\n| log-sigma-3-mm-3D_glszm_LargeAreaHighGrayLevelEmphasis        | 0.977871      | log-sigma-3-mm-3D_glszm_SmallAreaHighGrayLevelEmphasis        | nan          | nan         |\n| log-sigma-3-mm-3D_glrlm_RunEntropy                            | 0.992514      | log-sigma-3-mm-3D_glszm_ZoneEntropy                           | nan          | nan         |\n| log-sigma-3-mm-3D_glszm_LargeAreaEmphasis                     | 0.987092      | log-sigma-3-mm-3D_glszm_ZoneVariance                          | nan          | nan         |\n| log-sigma-0-6-mm-3D_ngtdm_Contrast                            | 0.939714      | log-sigma-3-mm-3D_ngtdm_Contrast                              | nan          | nan         |\n| log-sigma-2-mm-3D_ngtdm_Contrast                              | 0.971027      | log-sigma-3-mm-3D_ngtdm_Contrast                              | nan          | nan         |\n| log-sigma-3-mm-3D_glszm_ZoneEntropy                           | 0.996609      | log-sigma-3-mm-3D_gldm_DependenceEntropy                      | nan          | nan         |\n| log-sigma-3-mm-3D_glszm_SizeZoneNonUniformity                 | 0.999727      | log-sigma-3-mm-3D_gldm_DependenceNonUniformity                | nan          | nan         |\n| log-sigma-3-mm-3D_glszm_ZoneVariance                          | 0.949606      | log-sigma-3-mm-3D_gldm_DependenceVariance                     | nan          | nan         |\n| log-sigma-3-mm-3D_glszm_GrayLevelNonUniformity                | 0.99732       | log-sigma-3-mm-3D_gldm_GrayLevelNonUniformity                 | nan          | nan         |\n| log-sigma-3-mm-3D_glszm_GrayLevelVariance                     | 0.999646      | log-sigma-3-mm-3D_gldm_GrayLevelVariance                      | nan          | nan         |\n| log-sigma-3-mm-3D_glszm_SmallAreaHighGrayLevelEmphasis        | 0.999264      | log-sigma-3-mm-3D_gldm_HighGrayLevelEmphasis                  | nan          | nan         |\n| log-sigma-3-mm-3D_gldm_DependenceVariance                     | 0.990571      | log-sigma-3-mm-3D_gldm_LargeDependenceEmphasis                | nan          | nan         |\n| log-sigma-3-mm-3D_gldm_HighGrayLevelEmphasis                  | 0.970314      | log-sigma-3-mm-3D_gldm_LargeDependenceHighGrayLevelEmphasis   | nan          | nan         |\n| log-sigma-3-mm-3D_gldm_LargeDependenceHighGrayLevelEmphasis   | 0.953043      | log-sigma-3-mm-3D_gldm_SmallDependenceHighGrayLevelEmphasis   | nan          | nan         |\n| log-sigma-3-mm-3D_gldm_DependenceNonUniformity                | 0.990887      | wavelet-LH_firstorder_Energy                                  | nan          | nan         |\n| log-sigma-3-mm-3D_gldm_GrayLevelNonUniformity                 | 0.915982      | wavelet-LH_firstorder_Energy                                  | nan          | nan         |\n| log-sigma-3-mm-3D_gldm_DependenceEntropy                      | 0.930366      | wavelet-LH_firstorder_Entropy                                 | nan          | nan         |\n| wavelet-LH_firstorder_90Percentile                            | 0.932647      | wavelet-LH_firstorder_InterquartileRange                      | nan          | nan         |\n| wavelet-LH_firstorder_InterquartileRange                      | 0.977096      | wavelet-LH_firstorder_MeanAbsoluteDeviation                   | nan          | nan         |\n| wavelet-LH_firstorder_Mean                                    | 0.937816      | wavelet-LH_firstorder_Median                                  | nan          | nan         |\n| wavelet-LH_firstorder_Minimum                                 | 0.917795      | wavelet-LH_firstorder_Range                                   | nan          | nan         |\n| wavelet-LH_firstorder_MeanAbsoluteDeviation                   | 0.987425      | wavelet-LH_firstorder_RobustMeanAbsoluteDeviation             | nan          | nan         |\n| wavelet-LH_firstorder_Median                                  | 0.942026      | wavelet-LH_firstorder_RootMeanSquared                         | nan          | nan         |\n| wavelet-LH_firstorder_Energy                                  | 0.991914      | wavelet-LH_firstorder_TotalEnergy                             | nan          | nan         |\n| wavelet-LH_firstorder_RobustMeanAbsoluteDeviation             | 0.936803      | wavelet-LH_firstorder_Variance                                | nan          | nan         |\n| wavelet-LH_firstorder_Variance                                | 0.976315      | wavelet-LH_glcm_ClusterTendency                               | nan          | nan         |\n| wavelet-LH_firstorder_10Percentile                            | 0.902455      | wavelet-LH_glcm_Contrast                                      | nan          | nan         |\n| wavelet-LH_glcm_ClusterTendency                               | 0.905159      | wavelet-LH_glcm_Contrast                                      | nan          | nan         |\n| wavelet-LH_glcm_Contrast                                      | 0.972884      | wavelet-LH_glcm_DifferenceAverage                             | nan          | nan         |\n| wavelet-LH_firstorder_Entropy                                 | 0.993941      | wavelet-LH_glcm_DifferenceEntropy                             | nan          | nan         |\n| wavelet-LH_glcm_DifferenceAverage                             | 0.910172      | wavelet-LH_glcm_DifferenceVariance                            | nan          | nan         |\n| log-sigma-3-mm-3D_glcm_Imc1                                   | 0.907592      | wavelet-LH_glcm_Imc1                                          | nan          | nan         |\n| wavelet-LH_firstorder_Range                                   | 0.920061      | wavelet-LH_glcm_JointAverage                                  | nan          | nan         |\n| wavelet-LH_glcm_Autocorrelation                               | 0.977781      | wavelet-LH_glcm_JointAverage                                  | nan          | nan         |\n| wavelet-LH_glcm_Imc1                                          | 0.911424      | wavelet-LH_glcm_JointEntropy                                  | nan          | nan         |\n| wavelet-LH_glcm_JointAverage                                  | 1             | wavelet-LH_glcm_SumAverage                                    | nan          | nan         |\n| wavelet-LH_glcm_DifferenceEntropy                             | 0.983322      | wavelet-LH_glcm_SumEntropy                                    | nan          | nan         |\n| wavelet-LH_glcm_JointEntropy                                  | 0.911906      | wavelet-LH_glcm_SumEntropy                                    | nan          | nan         |\n| wavelet-LH_glcm_DifferenceVariance                            | 0.955586      | wavelet-LH_glcm_SumSquares                                    | nan          | nan         |\n| wavelet-LH_firstorder_TotalEnergy                             | 0.957235      | wavelet-LH_glrlm_GrayLevelNonUniformity                       | nan          | nan         |\n| wavelet-LH_glcm_SumSquares                                    | 0.995669      | wavelet-LH_glrlm_GrayLevelVariance                            | nan          | nan         |\n| wavelet-LH_glcm_SumAverage                                    | 0.978951      | wavelet-LH_glrlm_HighGrayLevelRunEmphasis                     | nan          | nan         |\n| wavelet-LH_glrlm_HighGrayLevelRunEmphasis                     | 0.99998       | wavelet-LH_glrlm_LongRunHighGrayLevelEmphasis                 | nan          | nan         |\n| wavelet-LH_glcm_SumEntropy                                    | 0.992353      | wavelet-LH_glrlm_RunEntropy                                   | nan          | nan         |\n| wavelet-LH_glrlm_GrayLevelNonUniformity                       | 0.96393       | wavelet-LH_glrlm_RunLengthNonUniformity                       | nan          | nan         |\n| wavelet-LH_glrlm_LongRunHighGrayLevelEmphasis                 | 0.999968      | wavelet-LH_glrlm_ShortRunHighGrayLevelEmphasis                | nan          | nan         |\n| wavelet-LH_glrlm_RunLengthNonUniformity                       | 0.965025      | wavelet-LH_glszm_GrayLevelNonUniformity                       | nan          | nan         |\n| wavelet-LH_glrlm_GrayLevelVariance                            | 0.999965      | wavelet-LH_glszm_GrayLevelVariance                            | nan          | nan         |\n| wavelet-LH_glrlm_ShortRunHighGrayLevelEmphasis                | 0.999997      | wavelet-LH_glszm_HighGrayLevelZoneEmphasis                    | nan          | nan         |\n| wavelet-LH_glszm_HighGrayLevelZoneEmphasis                    | 0.999673      | wavelet-LH_glszm_LargeAreaHighGrayLevelEmphasis               | nan          | nan         |\n| wavelet-LH_glszm_GrayLevelNonUniformity                       | 0.962751      | wavelet-LH_glszm_SizeZoneNonUniformity                        | nan          | nan         |\n| wavelet-LH_glszm_LargeAreaHighGrayLevelEmphasis               | 0.999489      | wavelet-LH_glszm_SmallAreaHighGrayLevelEmphasis               | nan          | nan         |\n| wavelet-LH_glrlm_RunEntropy                                   | 0.99963       | wavelet-LH_glszm_ZoneEntropy                                  | nan          | nan         |\n| wavelet-LH_glszm_ZoneEntropy                                  | 0.999812      | wavelet-LH_gldm_DependenceEntropy                             | nan          | nan         |\n| wavelet-LH_glszm_SizeZoneNonUniformity                        | 0.99999       | wavelet-LH_gldm_DependenceNonUniformity                       | nan          | nan         |\n| wavelet-LH_gldm_DependenceNonUniformity                       | 0.960174      | wavelet-LH_gldm_GrayLevelNonUniformity                        | nan          | nan         |\n| wavelet-LH_glszm_GrayLevelVariance                            | 0.999937      | wavelet-LH_gldm_GrayLevelVariance                             | nan          | nan         |\n| wavelet-LH_glszm_SmallAreaHighGrayLevelEmphasis               | 0.999975      | wavelet-LH_gldm_HighGrayLevelEmphasis                         | nan          | nan         |\n| wavelet-LH_gldm_HighGrayLevelEmphasis                         | 0.998822      | wavelet-LH_gldm_LargeDependenceHighGrayLevelEmphasis          | nan          | nan         |\n| wavelet-LH_gldm_LargeDependenceHighGrayLevelEmphasis          | 0.998126      | wavelet-LH_gldm_SmallDependenceHighGrayLevelEmphasis          | nan          | nan         |\n| wavelet-LH_gldm_GrayLevelNonUniformity                        | 0.963641      | wavelet-HL_firstorder_Energy                                  | nan          | nan         |\n| wavelet-LH_gldm_DependenceEntropy                             | 0.982399      | wavelet-HL_firstorder_Entropy                                 | nan          | nan         |\n| wavelet-HL_firstorder_90Percentile                            | 0.904101      | wavelet-HL_firstorder_InterquartileRange                      | nan          | nan         |\n| wavelet-HL_firstorder_10Percentile                            | 0.916847      | wavelet-HL_firstorder_MeanAbsoluteDeviation                   | nan          | nan         |\n| wavelet-HL_firstorder_InterquartileRange                      | 0.960682      | wavelet-HL_firstorder_MeanAbsoluteDeviation                   | nan          | nan         |\n| wavelet-HL_firstorder_Maximum                                 | 0.902714      | wavelet-HL_firstorder_Range                                   | nan          | nan         |\n| wavelet-HL_firstorder_Minimum                                 | 0.940254      | wavelet-HL_firstorder_Range                                   | nan          | nan         |\n| wavelet-HL_firstorder_MeanAbsoluteDeviation                   | 0.965319      | wavelet-HL_firstorder_RobustMeanAbsoluteDeviation             | nan          | nan         |\n| wavelet-HL_firstorder_Mean                                    | 0.976751      | wavelet-HL_firstorder_RootMeanSquared                         | nan          | nan         |\n| wavelet-HL_firstorder_Energy                                  | 0.99192       | wavelet-HL_firstorder_TotalEnergy                             | nan          | nan         |\n| wavelet-HL_firstorder_RobustMeanAbsoluteDeviation             | 0.90548       | wavelet-HL_firstorder_Variance                                | nan          | nan         |\n| wavelet-HL_firstorder_Variance                                | 0.975961      | wavelet-HL_glcm_ClusterTendency                               | nan          | nan         |\n| wavelet-HL_glcm_Contrast                                      | 0.96836       | wavelet-HL_glcm_DifferenceAverage                             | nan          | nan         |\n| wavelet-HL_firstorder_Entropy                                 | 0.99537       | wavelet-HL_glcm_DifferenceEntropy                             | nan          | nan         |\n| wavelet-HL_glcm_DifferenceAverage                             | 0.909517      | wavelet-HL_glcm_DifferenceVariance                            | nan          | nan         |\n| wavelet-HL_firstorder_Range                                   | 0.942265      | wavelet-HL_glcm_JointAverage                                  | nan          | nan         |\n| wavelet-HL_glcm_Autocorrelation                               | 0.952803      | wavelet-HL_glcm_JointAverage                                  | nan          | nan         |\n| wavelet-HL_glcm_Imc1                                          | 0.911013      | wavelet-HL_glcm_JointEntropy                                  | nan          | nan         |\n| wavelet-HL_glcm_JointAverage                                  | 1             | wavelet-HL_glcm_SumAverage                                    | nan          | nan         |\n| wavelet-HL_glcm_DifferenceEntropy                             | 0.986172      | wavelet-HL_glcm_SumEntropy                                    | nan          | nan         |\n| wavelet-HL_glcm_JointEntropy                                  | 0.919776      | wavelet-HL_glcm_SumEntropy                                    | nan          | nan         |\n| wavelet-HL_glcm_ClusterTendency                               | 0.97077       | wavelet-HL_glcm_SumSquares                                    | nan          | nan         |\n| wavelet-HL_glcm_DifferenceVariance                            | 0.924518      | wavelet-HL_glcm_SumSquares                                    | nan          | nan         |\n| wavelet-HL_firstorder_TotalEnergy                             | 0.960234      | wavelet-HL_glrlm_GrayLevelNonUniformity                       | nan          | nan         |\n| wavelet-HL_glcm_SumSquares                                    | 0.994763      | wavelet-HL_glrlm_GrayLevelVariance                            | nan          | nan         |\n| wavelet-HL_glcm_SumAverage                                    | 0.955092      | wavelet-HL_glrlm_HighGrayLevelRunEmphasis                     | nan          | nan         |\n| wavelet-HL_glrlm_HighGrayLevelRunEmphasis                     | 0.999985      | wavelet-HL_glrlm_LongRunHighGrayLevelEmphasis                 | nan          | nan         |\n| wavelet-HL_glcm_SumEntropy                                    | 0.993747      | wavelet-HL_glrlm_RunEntropy                                   | nan          | nan         |\n| wavelet-HL_glrlm_GrayLevelNonUniformity                       | 0.964946      | wavelet-HL_glrlm_RunLengthNonUniformity                       | nan          | nan         |\n| wavelet-HL_glrlm_LongRunHighGrayLevelEmphasis                 | 0.999977      | wavelet-HL_glrlm_ShortRunHighGrayLevelEmphasis                | nan          | nan         |\n| wavelet-HL_glrlm_RunLengthNonUniformity                       | 0.966013      | wavelet-HL_glszm_GrayLevelNonUniformity                       | nan          | nan         |\n| wavelet-HL_glrlm_GrayLevelVariance                            | 0.999972      | wavelet-HL_glszm_GrayLevelVariance                            | nan          | nan         |\n| wavelet-HL_glrlm_ShortRunHighGrayLevelEmphasis                | 0.999999      | wavelet-HL_glszm_HighGrayLevelZoneEmphasis                    | nan          | nan         |\n| wavelet-HL_glszm_HighGrayLevelZoneEmphasis                    | 0.999764      | wavelet-HL_glszm_LargeAreaHighGrayLevelEmphasis               | nan          | nan         |\n| wavelet-HL_glszm_GrayLevelNonUniformity                       | 0.963643      | wavelet-HL_glszm_SizeZoneNonUniformity                        | nan          | nan         |\n| wavelet-HL_glszm_LargeAreaHighGrayLevelEmphasis               | 0.999635      | wavelet-HL_glszm_SmallAreaHighGrayLevelEmphasis               | nan          | nan         |\n| wavelet-HL_glrlm_RunEntropy                                   | 0.999664      | wavelet-HL_glszm_ZoneEntropy                                  | nan          | nan         |\n| log-sigma-3-mm-3D_ngtdm_Contrast                              | 0.957573      | wavelet-HL_ngtdm_Contrast                                     | nan          | nan         |\n| wavelet-HL_glszm_ZoneEntropy                                  | 0.999844      | wavelet-HL_gldm_DependenceEntropy                             | nan          | nan         |\n| wavelet-HL_glszm_SizeZoneNonUniformity                        | 0.999988      | wavelet-HL_gldm_DependenceNonUniformity                       | nan          | nan         |\n| wavelet-HL_gldm_DependenceNonUniformity                       | 0.961073      | wavelet-HL_gldm_GrayLevelNonUniformity                        | nan          | nan         |\n| wavelet-HL_glszm_GrayLevelVariance                            | 0.99995       | wavelet-HL_gldm_GrayLevelVariance                             | nan          | nan         |\n| wavelet-HL_glszm_SmallAreaHighGrayLevelEmphasis               | 0.999982      | wavelet-HL_gldm_HighGrayLevelEmphasis                         | nan          | nan         |\n| wavelet-HL_gldm_HighGrayLevelEmphasis                         | 0.999164      | wavelet-HL_gldm_LargeDependenceHighGrayLevelEmphasis          | nan          | nan         |\n| wavelet-HL_gldm_LargeDependenceHighGrayLevelEmphasis          | 0.998669      | wavelet-HL_gldm_SmallDependenceHighGrayLevelEmphasis          | nan          | nan         |\n| wavelet-HH_firstorder_10Percentile                            | 0.945388      | wavelet-HH_firstorder_90Percentile                            | nan          | nan         |\n| wavelet-HL_gldm_GrayLevelNonUniformity                        | 0.96517       | wavelet-HH_firstorder_Energy                                  | nan          | nan         |\n| wavelet-HL_gldm_DependenceEntropy                             | 0.971264      | wavelet-HH_firstorder_Entropy                                 | nan          | nan         |\n| wavelet-HH_firstorder_90Percentile                            | 0.955012      | wavelet-HH_firstorder_InterquartileRange                      | nan          | nan         |\n| wavelet-HH_firstorder_InterquartileRange                      | 0.983246      | wavelet-HH_firstorder_MeanAbsoluteDeviation                   | nan          | nan         |\n| wavelet-HH_firstorder_Maximum                                 | 0.934877      | wavelet-HH_firstorder_Range                                   | nan          | nan         |\n| wavelet-HH_firstorder_Minimum                                 | 0.926613      | wavelet-HH_firstorder_Range                                   | nan          | nan         |\n| wavelet-HH_firstorder_MeanAbsoluteDeviation                   | 0.993638      | wavelet-HH_firstorder_RobustMeanAbsoluteDeviation             | nan          | nan         |\n| wavelet-HH_firstorder_Mean                                    | 0.902567      | wavelet-HH_firstorder_RootMeanSquared                         | nan          | nan         |\n| wavelet-HH_firstorder_Energy                                  | 0.991892      | wavelet-HH_firstorder_TotalEnergy                             | nan          | nan         |\n| wavelet-HH_firstorder_RobustMeanAbsoluteDeviation             | 0.974107      | wavelet-HH_firstorder_Variance                                | nan          | nan         |\n| wavelet-HH_firstorder_Range                                   | 0.901867      | wavelet-HH_glcm_Autocorrelation                               | nan          | nan         |\n| wavelet-HH_firstorder_Variance                                | 0.949532      | wavelet-HH_glcm_ClusterProminence                             | nan          | nan         |\n| wavelet-HH_glcm_ClusterProminence                             | 0.955941      | wavelet-HH_glcm_ClusterTendency                               | nan          | nan         |\n| wavelet-HH_glcm_ClusterTendency                               | 0.993673      | wavelet-HH_glcm_Contrast                                      | nan          | nan         |\n| wavelet-HH_glcm_Contrast                                      | 0.988094      | wavelet-HH_glcm_DifferenceAverage                             | nan          | nan         |\n| wavelet-HH_firstorder_Entropy                                 | 0.996687      | wavelet-HH_glcm_DifferenceEntropy                             | nan          | nan         |\n| wavelet-HH_glcm_DifferenceAverage                             | 0.978682      | wavelet-HH_glcm_DifferenceVariance                            | nan          | nan         |\n| wavelet-HH_glcm_Autocorrelation                               | 0.985482      | wavelet-HH_glcm_JointAverage                                  | nan          | nan         |\n| wavelet-HH_glcm_Imc1                                          | 0.920292      | wavelet-HH_glcm_JointEntropy                                  | nan          | nan         |\n| wavelet-HH_glcm_JointAverage                                  | 1             | wavelet-HH_glcm_SumAverage                                    | nan          | nan         |\n| wavelet-HH_glcm_DifferenceEntropy                             | 0.98788       | wavelet-HH_glcm_SumEntropy                                    | nan          | nan         |\n| wavelet-HH_glcm_DifferenceVariance                            | 0.991545      | wavelet-HH_glcm_SumSquares                                    | nan          | nan         |\n| original_gldm_GrayLevelNonUniformity                          | 0.905275      | wavelet-HH_glrlm_GrayLevelNonUniformity                       | nan          | nan         |\n| wavelet-HH_firstorder_TotalEnergy                             | 0.949237      | wavelet-HH_glrlm_GrayLevelNonUniformity                       | nan          | nan         |\n| wavelet-HH_glcm_SumSquares                                    | 0.998177      | wavelet-HH_glrlm_GrayLevelVariance                            | nan          | nan         |\n| wavelet-HH_glcm_SumAverage                                    | 0.98604       | wavelet-HH_glrlm_HighGrayLevelRunEmphasis                     | nan          | nan         |\n| wavelet-HH_glrlm_HighGrayLevelRunEmphasis                     | 0.999961      | wavelet-HH_glrlm_LongRunHighGrayLevelEmphasis                 | nan          | nan         |\n| wavelet-HH_glcm_SumEntropy                                    | 0.991206      | wavelet-HH_glrlm_RunEntropy                                   | nan          | nan         |\n| wavelet-HH_glrlm_GrayLevelNonUniformity                       | 0.957259      | wavelet-HH_glrlm_RunLengthNonUniformity                       | nan          | nan         |\n| wavelet-HH_glrlm_LongRunHighGrayLevelEmphasis                 | 0.999939      | wavelet-HH_glrlm_ShortRunHighGrayLevelEmphasis                | nan          | nan         |\n| wavelet-HH_glrlm_RunLengthNonUniformity                       | 0.958884      | wavelet-HH_glszm_GrayLevelNonUniformity                       | nan          | nan         |\n| wavelet-HH_glrlm_GrayLevelVariance                            | 0.999961      | wavelet-HH_glszm_GrayLevelVariance                            | nan          | nan         |\n| wavelet-HH_glrlm_ShortRunHighGrayLevelEmphasis                | 0.999996      | wavelet-HH_glszm_HighGrayLevelZoneEmphasis                    | nan          | nan         |\n| wavelet-HH_glszm_HighGrayLevelZoneEmphasis                    | 0.999364      | wavelet-HH_glszm_LargeAreaHighGrayLevelEmphasis               | nan          | nan         |\n| wavelet-HH_glszm_GrayLevelNonUniformity                       | 0.95582       | wavelet-HH_glszm_SizeZoneNonUniformity                        | nan          | nan         |\n| wavelet-HH_glszm_LargeAreaHighGrayLevelEmphasis               | 0.999013      | wavelet-HH_glszm_SmallAreaHighGrayLevelEmphasis               | nan          | nan         |\n| wavelet-HH_glrlm_RunEntropy                                   | 0.999307      | wavelet-HH_glszm_ZoneEntropy                                  | nan          | nan         |\n| wavelet-HH_glszm_ZoneEntropy                                  | 0.999631      | wavelet-HH_gldm_DependenceEntropy                             | nan          | nan         |\n| wavelet-HH_glszm_SizeZoneNonUniformity                        | 0.999986      | wavelet-HH_gldm_DependenceNonUniformity                       | nan          | nan         |\n| wavelet-HH_gldm_DependenceNonUniformity                       | 0.952175      | wavelet-HH_gldm_GrayLevelNonUniformity                        | nan          | nan         |\n| wavelet-HH_glszm_GrayLevelVariance                            | 0.999932      | wavelet-HH_gldm_GrayLevelVariance                             | nan          | nan         |\n| wavelet-HH_glszm_SmallAreaHighGrayLevelEmphasis               | 0.999947      | wavelet-HH_gldm_HighGrayLevelEmphasis                         | nan          | nan         |\n| wavelet-HH_gldm_HighGrayLevelEmphasis                         | 0.99778       | wavelet-HH_gldm_LargeDependenceHighGrayLevelEmphasis          | nan          | nan         |\n| wavelet-HH_gldm_LargeDependenceHighGrayLevelEmphasis          | 0.996459      | wavelet-HH_gldm_SmallDependenceHighGrayLevelEmphasis          | nan          | nan         |\n| wavelet-HH_gldm_GrayLevelNonUniformity                        | 0.956978      | wavelet2-LH_firstorder_Energy                                 | nan          | nan         |\n| wavelet-HH_gldm_DependenceEntropy                             | 0.95066       | wavelet2-LH_firstorder_Entropy                                | nan          | nan         |\n| wavelet2-LH_firstorder_90Percentile                           | 0.925988      | wavelet2-LH_firstorder_InterquartileRange                     | nan          | nan         |\n| wavelet2-LH_firstorder_InterquartileRange                     | 0.976183      | wavelet2-LH_firstorder_MeanAbsoluteDeviation                  | nan          | nan         |\n| wavelet-LH_firstorder_RootMeanSquared                         | 0.972931      | wavelet2-LH_firstorder_Mean                                   | nan          | nan         |\n| wavelet2-LH_firstorder_Mean                                   | 0.952964      | wavelet2-LH_firstorder_Median                                 | nan          | nan         |\n| wavelet2-LH_firstorder_Minimum                                | 0.913387      | wavelet2-LH_firstorder_Range                                  | nan          | nan         |\n| wavelet2-LH_firstorder_MeanAbsoluteDeviation                  | 0.985641      | wavelet2-LH_firstorder_RobustMeanAbsoluteDeviation            | nan          | nan         |\n| wavelet2-LH_firstorder_Median                                 | 0.945042      | wavelet2-LH_firstorder_RootMeanSquared                        | nan          | nan         |\n| wavelet2-LH_firstorder_Energy                                 | 0.991936      | wavelet2-LH_firstorder_TotalEnergy                            | nan          | nan         |\n| wavelet-LH_gldm_GrayLevelVariance                             | 0.908502      | wavelet2-LH_firstorder_Variance                               | nan          | nan         |\n| wavelet2-LH_firstorder_RobustMeanAbsoluteDeviation            | 0.930027      | wavelet2-LH_firstorder_Variance                               | nan          | nan         |\n| wavelet-LH_glcm_ClusterProminence                             | 0.931014      | wavelet2-LH_glcm_ClusterProminence                            | nan          | nan         |\n| wavelet2-LH_firstorder_Variance                               | 0.991426      | wavelet2-LH_glcm_ClusterTendency                              | nan          | nan         |\n| wavelet2-LH_glcm_ClusterTendency                              | 0.963181      | wavelet2-LH_glcm_Contrast                                     | nan          | nan         |\n| wavelet2-LH_glcm_Contrast                                     | 0.962759      | wavelet2-LH_glcm_DifferenceAverage                            | nan          | nan         |\n| wavelet2-LH_firstorder_Entropy                                | 0.989578      | wavelet2-LH_glcm_DifferenceEntropy                            | nan          | nan         |\n| wavelet2-LH_glcm_DifferenceAverage                            | 0.922139      | wavelet2-LH_glcm_DifferenceVariance                           | nan          | nan         |\n| wavelet-HH_glcm_MCC                                           | 0.901494      | wavelet2-LH_glcm_Imc1                                         | nan          | nan         |\n| wavelet2-LH_firstorder_Range                                  | 0.91125       | wavelet2-LH_glcm_JointAverage                                 | nan          | nan         |\n| wavelet2-LH_glcm_Autocorrelation                              | 0.973585      | wavelet2-LH_glcm_JointAverage                                 | nan          | nan         |\n| wavelet-HH_glcm_JointEntropy                                  | 0.999855      | wavelet2-LH_glcm_JointEntropy                                 | nan          | nan         |\n| wavelet2-LH_glcm_JointAverage                                 | 1             | wavelet2-LH_glcm_SumAverage                                   | nan          | nan         |\n| wavelet2-LH_glcm_DifferenceEntropy                            | 0.958675      | wavelet2-LH_glcm_SumEntropy                                   | nan          | nan         |\n| wavelet2-LH_glcm_JointEntropy                                 | 0.917492      | wavelet2-LH_glcm_SumEntropy                                   | nan          | nan         |\n| wavelet2-LH_glcm_DifferenceVariance                           | 0.970564      | wavelet2-LH_glcm_SumSquares                                   | nan          | nan         |\n| wavelet2-LH_firstorder_TotalEnergy                            | 0.957389      | wavelet2-LH_glrlm_GrayLevelNonUniformity                      | nan          | nan         |\n| wavelet2-LH_glcm_Imc1                                         | 0.916496      | wavelet2-LH_glrlm_GrayLevelNonUniformity                      | nan          | nan         |\n| wavelet2-LH_glcm_SumSquares                                   | 0.996419      | wavelet2-LH_glrlm_GrayLevelVariance                           | nan          | nan         |\n| wavelet2-LH_glcm_SumAverage                                   | 0.973651      | wavelet2-LH_glrlm_HighGrayLevelRunEmphasis                    | nan          | nan         |\n| wavelet2-LH_glrlm_HighGrayLevelRunEmphasis                    | 0.99997       | wavelet2-LH_glrlm_LongRunHighGrayLevelEmphasis                | nan          | nan         |\n| wavelet2-LH_glcm_SumEntropy                                   | 0.984278      | wavelet2-LH_glrlm_RunEntropy                                  | nan          | nan         |\n| wavelet2-LH_glrlm_GrayLevelNonUniformity                      | 0.962544      | wavelet2-LH_glrlm_RunLengthNonUniformity                      | nan          | nan         |\n| wavelet2-LH_glrlm_LongRunHighGrayLevelEmphasis                | 0.999953      | wavelet2-LH_glrlm_ShortRunHighGrayLevelEmphasis               | nan          | nan         |\n| wavelet2-LH_glrlm_RunLengthNonUniformity                      | 0.963908      | wavelet2-LH_glszm_GrayLevelNonUniformity                      | nan          | nan         |\n| wavelet2-LH_glrlm_GrayLevelVariance                           | 0.999974      | wavelet2-LH_glszm_GrayLevelVariance                           | nan          | nan         |\n| wavelet2-LH_glrlm_ShortRunHighGrayLevelEmphasis               | 0.999997      | wavelet2-LH_glszm_HighGrayLevelZoneEmphasis                   | nan          | nan         |\n| wavelet2-LH_glszm_HighGrayLevelZoneEmphasis                   | 0.999519      | wavelet2-LH_glszm_LargeAreaHighGrayLevelEmphasis              | nan          | nan         |\n| wavelet2-LH_glszm_GrayLevelNonUniformity                      | 0.96069       | wavelet2-LH_glszm_SizeZoneNonUniformity                       | nan          | nan         |\n| wavelet2-LH_glszm_LargeAreaHighGrayLevelEmphasis              | 0.999251      | wavelet2-LH_glszm_SmallAreaHighGrayLevelEmphasis              | nan          | nan         |\n| wavelet2-LH_glrlm_RunEntropy                                  | 0.999468      | wavelet2-LH_glszm_ZoneEntropy                                 | nan          | nan         |\n| log-sigma-0-6-mm-3D_ngtdm_Complexity                          | 0.907882      | wavelet2-LH_ngtdm_Complexity                                  | nan          | nan         |\n| wavelet-HL_ngtdm_Contrast                                     | 0.978277      | wavelet2-LH_ngtdm_Contrast                                    | nan          | nan         |\n| log-sigma-0-6-mm-3D_ngtdm_Strength                            | 0.904442      | wavelet2-LH_ngtdm_Strength                                    | nan          | nan         |\n| wavelet-LH_ngtdm_Strength                                     | 0.957265      | wavelet2-LH_ngtdm_Strength                                    | nan          | nan         |\n| wavelet2-LH_glszm_GrayLevelVariance                           | 0.906746      | wavelet2-LH_ngtdm_Strength                                    | nan          | nan         |\n| wavelet2-LH_glszm_ZoneEntropy                                 | 0.999714      | wavelet2-LH_gldm_DependenceEntropy                            | nan          | nan         |\n| wavelet2-LH_glszm_SizeZoneNonUniformity                       | 0.999983      | wavelet2-LH_gldm_DependenceNonUniformity                      | nan          | nan         |\n| wavelet2-LH_gldm_DependenceNonUniformity                      | 0.957305      | wavelet2-LH_gldm_GrayLevelNonUniformity                       | nan          | nan         |\n| wavelet2-LH_ngtdm_Strength                                    | 0.906944      | wavelet2-LH_gldm_GrayLevelVariance                            | nan          | nan         |\n| wavelet2-LH_glszm_SmallAreaHighGrayLevelEmphasis              | 0.999965      | wavelet2-LH_gldm_HighGrayLevelEmphasis                        | nan          | nan         |\n| wavelet2-LH_gldm_HighGrayLevelEmphasis                        | 0.998284      | wavelet2-LH_gldm_LargeDependenceHighGrayLevelEmphasis         | nan          | nan         |\n| wavelet2-LH_gldm_LargeDependenceHighGrayLevelEmphasis         | 0.997268      | wavelet2-LH_gldm_SmallDependenceHighGrayLevelEmphasis         | nan          | nan         |\n| wavelet2-LH_gldm_GrayLevelNonUniformity                       | 0.96206       | wavelet2-HL_firstorder_Energy                                 | nan          | nan         |\n| wavelet2-LH_gldm_DependenceEntropy                            | 0.971974      | wavelet2-HL_firstorder_Entropy                                | nan          | nan         |\n| wavelet-HL_ngtdm_Strength                                     | 0.908075      | wavelet2-HL_firstorder_InterquartileRange                     | nan          | nan         |\n| wavelet-HL_gldm_GrayLevelVariance                             | 0.920399      | wavelet2-HL_firstorder_MeanAbsoluteDeviation                  | nan          | nan         |\n| wavelet2-HL_firstorder_10Percentile                           | 0.919185      | wavelet2-HL_firstorder_MeanAbsoluteDeviation                  | nan          | nan         |\n| wavelet2-HL_firstorder_90Percentile                           | 0.943689      | wavelet2-HL_firstorder_MeanAbsoluteDeviation                  | nan          | nan         |\n| wavelet2-HL_firstorder_InterquartileRange                     | 0.963458      | wavelet2-HL_firstorder_MeanAbsoluteDeviation                  | nan          | nan         |\n| wavelet-HL_firstorder_RootMeanSquared                         | 0.94958       | wavelet2-HL_firstorder_Mean                                   | nan          | nan         |\n| wavelet2-HL_firstorder_Minimum                                | 0.930787      | wavelet2-HL_firstorder_Range                                  | nan          | nan         |\n| wavelet2-HL_firstorder_MeanAbsoluteDeviation                  | 0.970991      | wavelet2-HL_firstorder_RobustMeanAbsoluteDeviation            | nan          | nan         |\n| wavelet2-HL_firstorder_Mean                                   | 0.967631      | wavelet2-HL_firstorder_RootMeanSquared                        | nan          | nan         |\n| wavelet2-HL_firstorder_Energy                                 | 0.991935      | wavelet2-HL_firstorder_TotalEnergy                            | nan          | nan         |\n| wavelet2-HL_firstorder_RobustMeanAbsoluteDeviation            | 0.946828      | wavelet2-HL_firstorder_Variance                               | nan          | nan         |\n| wavelet2-HL_firstorder_Range                                  | 0.905262      | wavelet2-HL_glcm_Autocorrelation                              | nan          | nan         |\n| wavelet2-HL_firstorder_Variance                               | 0.932957      | wavelet2-HL_glcm_ClusterProminence                            | nan          | nan         |\n| wavelet2-HL_glcm_ClusterProminence                            | 0.940373      | wavelet2-HL_glcm_ClusterTendency                              | nan          | nan         |\n| wavelet2-HL_glcm_ClusterTendency                              | 0.947737      | wavelet2-HL_glcm_Contrast                                     | nan          | nan         |\n| wavelet2-HL_glcm_Contrast                                     | 0.96204       | wavelet2-HL_glcm_DifferenceAverage                            | nan          | nan         |\n| wavelet2-HL_firstorder_Entropy                                | 0.990531      | wavelet2-HL_glcm_DifferenceEntropy                            | nan          | nan         |\n| wavelet-HL_glcm_ClusterProminence                             | 0.912818      | wavelet2-HL_glcm_DifferenceVariance                           | nan          | nan         |\n| wavelet2-HL_glcm_DifferenceAverage                            | 0.902345      | wavelet2-HL_glcm_DifferenceVariance                           | nan          | nan         |\n| wavelet2-HL_glcm_Autocorrelation                              | 0.972455      | wavelet2-HL_glcm_JointAverage                                 | nan          | nan         |\n| wavelet2-HL_glcm_Imc1                                         | 0.902464      | wavelet2-HL_glcm_JointEntropy                                 | nan          | nan         |\n| wavelet2-HL_glcm_JointAverage                                 | 1             | wavelet2-HL_glcm_SumAverage                                   | nan          | nan         |\n| wavelet2-HL_glcm_DifferenceEntropy                            | 0.964188      | wavelet2-HL_glcm_SumEntropy                                   | nan          | nan         |\n| wavelet2-HL_glcm_JointEntropy                                 | 0.926732      | wavelet2-HL_glcm_SumEntropy                                   | nan          | nan         |\n| wavelet2-HL_firstorder_TotalEnergy                            | 0.959668      | wavelet2-HL_glrlm_GrayLevelNonUniformity                      | nan          | nan         |\n| wavelet2-HL_glcm_SumSquares                                   | 0.998785      | wavelet2-HL_glrlm_GrayLevelVariance                           | nan          | nan         |\n| wavelet2-HL_glcm_SumAverage                                   | 0.972092      | wavelet2-HL_glrlm_HighGrayLevelRunEmphasis                    | nan          | nan         |\n| wavelet2-HL_glrlm_HighGrayLevelRunEmphasis                    | 0.999975      | wavelet2-HL_glrlm_LongRunHighGrayLevelEmphasis                | nan          | nan         |\n| wavelet2-HL_glcm_SumEntropy                                   | 0.987239      | wavelet2-HL_glrlm_RunEntropy                                  | nan          | nan         |\n| wavelet2-HL_glrlm_GrayLevelNonUniformity                      | 0.963003      | wavelet2-HL_glrlm_RunLengthNonUniformity                      | nan          | nan         |\n| wavelet2-HL_glrlm_LongRunHighGrayLevelEmphasis                | 0.999961      | wavelet2-HL_glrlm_ShortRunHighGrayLevelEmphasis               | nan          | nan         |\n| wavelet2-HL_glrlm_RunLengthNonUniformity                      | 0.964299      | wavelet2-HL_glszm_GrayLevelNonUniformity                      | nan          | nan         |\n| wavelet2-HL_glrlm_GrayLevelVariance                           | 0.999982      | wavelet2-HL_glszm_GrayLevelVariance                           | nan          | nan         |\n| wavelet2-HL_glrlm_ShortRunHighGrayLevelEmphasis               | 0.999998      | wavelet2-HL_glszm_HighGrayLevelZoneEmphasis                   | nan          | nan         |\n| wavelet2-HL_glszm_HighGrayLevelZoneEmphasis                   | 0.999605      | wavelet2-HL_glszm_LargeAreaHighGrayLevelEmphasis              | nan          | nan         |\n| wavelet2-HL_glszm_GrayLevelNonUniformity                      | 0.96138       | wavelet2-HL_glszm_SizeZoneNonUniformity                       | nan          | nan         |\n| wavelet2-HL_glszm_LargeAreaHighGrayLevelEmphasis              | 0.999387      | wavelet2-HL_glszm_SmallAreaHighGrayLevelEmphasis              | nan          | nan         |\n| wavelet2-HL_glrlm_RunEntropy                                  | 0.999435      | wavelet2-HL_glszm_ZoneEntropy                                 | nan          | nan         |\n| wavelet-HL_ngtdm_Complexity                                   | 0.920715      | wavelet2-HL_ngtdm_Complexity                                  | nan          | nan         |\n| wavelet2-LH_ngtdm_Contrast                                    | 0.963361      | wavelet2-HL_ngtdm_Contrast                                    | nan          | nan         |\n| wavelet2-HL_glszm_GrayLevelVariance                           | 0.929468      | wavelet2-HL_ngtdm_Strength                                    | nan          | nan         |\n| wavelet2-HL_glszm_ZoneEntropy                                 | 0.999731      | wavelet2-HL_gldm_DependenceEntropy                            | nan          | nan         |\n| wavelet2-HL_glszm_SizeZoneNonUniformity                       | 0.999984      | wavelet2-HL_gldm_DependenceNonUniformity                      | nan          | nan         |\n| wavelet2-HL_gldm_DependenceNonUniformity                      | 0.958271      | wavelet2-HL_gldm_GrayLevelNonUniformity                       | nan          | nan         |\n| wavelet2-HL_ngtdm_Strength                                    | 0.929349      | wavelet2-HL_gldm_GrayLevelVariance                            | nan          | nan         |\n| wavelet2-HL_glszm_SmallAreaHighGrayLevelEmphasis              | 0.99997       | wavelet2-HL_gldm_HighGrayLevelEmphasis                        | nan          | nan         |\n| wavelet2-HL_gldm_HighGrayLevelEmphasis                        | 0.998619      | wavelet2-HL_gldm_LargeDependenceHighGrayLevelEmphasis         | nan          | nan         |\n| wavelet2-HL_gldm_LargeDependenceHighGrayLevelEmphasis         | 0.997798      | wavelet2-HL_gldm_SmallDependenceHighGrayLevelEmphasis         | nan          | nan         |\n| wavelet2-HH_firstorder_10Percentile                           | 0.905897      | wavelet2-HH_firstorder_90Percentile                           | nan          | nan         |\n| wavelet2-HL_gldm_GrayLevelNonUniformity                       | 0.96352       | wavelet2-HH_firstorder_Energy                                 | nan          | nan         |\n| wavelet2-HL_gldm_DependenceEntropy                            | 0.912436      | wavelet2-HH_firstorder_Entropy                                | nan          | nan         |\n| wavelet2-HH_firstorder_90Percentile                           | 0.941146      | wavelet2-HH_firstorder_InterquartileRange                     | nan          | nan         |\n| wavelet2-HH_firstorder_InterquartileRange                     | 0.977354      | wavelet2-HH_firstorder_MeanAbsoluteDeviation                  | nan          | nan         |\n| wavelet2-HH_firstorder_Maximum                                | 0.93245       | wavelet2-HH_firstorder_Range                                  | nan          | nan         |\n| wavelet2-HH_firstorder_Minimum                                | 0.939277      | wavelet2-HH_firstorder_Range                                  | nan          | nan         |\n| wavelet2-HH_firstorder_MeanAbsoluteDeviation                  | 0.988977      | wavelet2-HH_firstorder_RobustMeanAbsoluteDeviation            | nan          | nan         |\n| wavelet2-HH_firstorder_Mean                                   | 0.984293      | wavelet2-HH_firstorder_RootMeanSquared                        | nan          | nan         |\n| wavelet2-HH_firstorder_Energy                                 | 0.991893      | wavelet2-HH_firstorder_TotalEnergy                            | nan          | nan         |\n| wavelet2-HH_firstorder_RobustMeanAbsoluteDeviation            | 0.945561      | wavelet2-HH_firstorder_Variance                               | nan          | nan         |\n| wavelet2-HH_firstorder_Range                                  | 0.919255      | wavelet2-HH_glcm_Autocorrelation                              | nan          | nan         |\n| wavelet2-HH_firstorder_Variance                               | 0.993931      | wavelet2-HH_glcm_ClusterTendency                              | nan          | nan         |\n| wavelet2-HH_glcm_ClusterProminence                            | 0.901876      | wavelet2-HH_glcm_ClusterTendency                              | nan          | nan         |\n| wavelet2-HH_glcm_ClusterTendency                              | 0.977269      | wavelet2-HH_glcm_Contrast                                     | nan          | nan         |\n| wavelet2-HH_glcm_Contrast                                     | 0.969383      | wavelet2-HH_glcm_DifferenceAverage                            | nan          | nan         |\n| wavelet2-HH_firstorder_Entropy                                | 0.98997       | wavelet2-HH_glcm_DifferenceEntropy                            | nan          | nan         |\n| wavelet2-HH_glcm_DifferenceAverage                            | 0.947707      | wavelet2-HH_glcm_DifferenceVariance                           | nan          | nan         |\n| wavelet2-HH_glcm_Autocorrelation                              | 0.980519      | wavelet2-HH_glcm_JointAverage                                 | nan          | nan         |\n| wavelet2-HH_glcm_Imc1                                         | 0.930604      | wavelet2-HH_glcm_JointEntropy                                 | nan          | nan         |\n| wavelet2-HH_glcm_JointAverage                                 | 1             | wavelet2-HH_glcm_SumAverage                                   | nan          | nan         |\n| wavelet2-HH_glcm_DifferenceEntropy                            | 0.967448      | wavelet2-HH_glcm_SumEntropy                                   | nan          | nan         |\n| wavelet2-HH_glcm_DifferenceVariance                           | 0.988686      | wavelet2-HH_glcm_SumSquares                                   | nan          | nan         |\n| wavelet2-HH_firstorder_TotalEnergy                            | 0.960136      | wavelet2-HH_glrlm_GrayLevelNonUniformity                      | nan          | nan         |\n| wavelet2-HH_glcm_SumSquares                                   | 0.997509      | wavelet2-HH_glrlm_GrayLevelVariance                           | nan          | nan         |\n| wavelet2-HH_glcm_SumAverage                                   | 0.98028       | wavelet2-HH_glrlm_HighGrayLevelRunEmphasis                    | nan          | nan         |\n| wavelet2-HH_glrlm_HighGrayLevelRunEmphasis                    | 0.999887      | wavelet2-HH_glrlm_LongRunHighGrayLevelEmphasis                | nan          | nan         |\n| wavelet2-HH_glcm_SumEntropy                                   | 0.980488      | wavelet2-HH_glrlm_RunEntropy                                  | nan          | nan         |\n| wavelet2-HH_glrlm_GrayLevelNonUniformity                      | 0.963887      | wavelet2-HH_glrlm_RunLengthNonUniformity                      | nan          | nan         |\n| wavelet2-HH_glrlm_LongRunHighGrayLevelEmphasis                | 0.999824      | wavelet2-HH_glrlm_ShortRunHighGrayLevelEmphasis               | nan          | nan         |\n| wavelet2-HH_glrlm_RunLengthNonUniformity                      | 0.966496      | wavelet2-HH_glszm_GrayLevelNonUniformity                      | nan          | nan         |\n| wavelet2-HH_glrlm_GrayLevelVariance                           | 0.999919      | wavelet2-HH_glszm_GrayLevelVariance                           | nan          | nan         |\n| wavelet2-HH_glrlm_ShortRunHighGrayLevelEmphasis               | 0.999989      | wavelet2-HH_glszm_HighGrayLevelZoneEmphasis                   | nan          | nan         |\n| wavelet2-HH_glszm_HighGrayLevelZoneEmphasis                   | 0.998171      | wavelet2-HH_glszm_LargeAreaHighGrayLevelEmphasis              | nan          | nan         |\n| wavelet2-HH_glszm_GrayLevelNonUniformity                      | 0.960605      | wavelet2-HH_glszm_SizeZoneNonUniformity                       | nan          | nan         |\n| wavelet2-HH_glszm_LargeAreaHighGrayLevelEmphasis              | 0.997185      | wavelet2-HH_glszm_SmallAreaHighGrayLevelEmphasis              | nan          | nan         |\n| wavelet2-HH_glrlm_RunEntropy                                  | 0.99498       | wavelet2-HH_glszm_ZoneEntropy                                 | nan          | nan         |\n| wavelet-LH_ngtdm_Contrast                                     | 0.95626       | wavelet2-HH_ngtdm_Contrast                                    | nan          | nan         |\n| wavelet-HH_ngtdm_Strength                                     | 0.910862      | wavelet2-HH_ngtdm_Strength                                    | nan          | nan         |\n| wavelet2-HH_glszm_ZoneEntropy                                 | 0.99756       | wavelet2-HH_gldm_DependenceEntropy                            | nan          | nan         |\n| wavelet2-HH_glszm_SizeZoneNonUniformity                       | 0.999959      | wavelet2-HH_gldm_DependenceNonUniformity                      | nan          | nan         |\n| wavelet2-HH_gldm_DependenceNonUniformity                      | 0.954293      | wavelet2-HH_gldm_GrayLevelNonUniformity                       | nan          | nan         |\n| wavelet2-HH_glszm_GrayLevelVariance                           | 0.999857      | wavelet2-HH_gldm_GrayLevelVariance                            | nan          | nan         |\n| wavelet2-HH_glszm_SmallAreaHighGrayLevelEmphasis              | 0.999875      | wavelet2-HH_gldm_HighGrayLevelEmphasis                        | nan          | nan         |\n| wavelet2-HH_gldm_HighGrayLevelEmphasis                        | 0.994188      | wavelet2-HH_gldm_LargeDependenceHighGrayLevelEmphasis         | nan          | nan         |\n| wavelet2-HH_gldm_LargeDependenceHighGrayLevelEmphasis         | 0.990626      | wavelet2-HH_gldm_SmallDependenceHighGrayLevelEmphasis         | nan          | nan         |\n| original_firstorder_RootMeanSquared                           | 0.925514      | wavelet2-LL_firstorder_10Percentile                           | nan          | nan         |\n| wavelet2-HH_glcm_JointEntropy                                 | 0.98391       | wavelet2-LL_firstorder_Entropy                                | nan          | nan         |\n| original_gldm_GrayLevelVariance                               | 0.917417      | wavelet2-LL_firstorder_InterquartileRange                     | nan          | nan         |\n| original_firstorder_Maximum                                   | 0.963676      | wavelet2-LL_firstorder_Maximum                                | nan          | nan         |\n| original_ngtdm_Strength                                       | 0.912887      | wavelet2-LL_firstorder_MeanAbsoluteDeviation                  | nan          | nan         |\n| wavelet2-LL_firstorder_InterquartileRange                     | 0.976487      | wavelet2-LL_firstorder_MeanAbsoluteDeviation                  | nan          | nan         |\n| wavelet2-LL_firstorder_10Percentile                           | 0.934174      | wavelet2-LL_firstorder_Mean                                   | nan          | nan         |\n| wavelet2-LL_firstorder_90Percentile                           | 0.960485      | wavelet2-LL_firstorder_Mean                                   | nan          | nan         |\n| wavelet2-LL_firstorder_Mean                                   | 0.994933      | wavelet2-LL_firstorder_Median                                 | nan          | nan         |\n| original_firstorder_Minimum                                   | 0.949158      | wavelet2-LL_firstorder_Minimum                                | nan          | nan         |\n| original_firstorder_Range                                     | 0.953911      | wavelet2-LL_firstorder_Range                                  | nan          | nan         |\n| wavelet2-LL_firstorder_MeanAbsoluteDeviation                  | 0.988393      | wavelet2-LL_firstorder_RobustMeanAbsoluteDeviation            | nan          | nan         |\n| wavelet2-LL_firstorder_Median                                 | 0.983797      | wavelet2-LL_firstorder_RootMeanSquared                        | nan          | nan         |\n| original_firstorder_Skewness                                  | 0.910389      | wavelet2-LL_firstorder_Skewness                               | nan          | nan         |\n| wavelet2-LL_firstorder_Energy                                 | 0.989381      | wavelet2-LL_firstorder_TotalEnergy                            | nan          | nan         |\n| wavelet2-LL_firstorder_RobustMeanAbsoluteDeviation            | 0.950985      | wavelet2-LL_firstorder_Variance                               | nan          | nan         |\n| original_gldm_SmallDependenceHighGrayLevelEmphasis            | 0.936386      | wavelet2-LL_glcm_Autocorrelation                              | nan          | nan         |\n| wavelet2-LL_firstorder_Variance                               | 0.910342      | wavelet2-LL_glcm_ClusterProminence                            | nan          | nan         |\n| original_glcm_ClusterShade                                    | 0.986095      | wavelet2-LL_glcm_ClusterShade                                 | nan          | nan         |\n| wavelet2-LL_glcm_ClusterProminence                            | 0.919959      | wavelet2-LL_glcm_ClusterTendency                              | nan          | nan         |\n| log-sigma-0-6-mm-3D_gldm_GrayLevelVariance                    | 0.948215      | wavelet2-LL_glcm_Contrast                                     | nan          | nan         |\n| wavelet2-LL_glcm_Contrast                                     | 0.959489      | wavelet2-LL_glcm_DifferenceAverage                            | nan          | nan         |\n| wavelet2-HH_gldm_DependenceEntropy                            | 0.956457      | wavelet2-LL_glcm_DifferenceEntropy                            | nan          | nan         |\n| wavelet2-LL_firstorder_Entropy                                | 0.958738      | wavelet2-LL_glcm_DifferenceEntropy                            | nan          | nan         |\n| wavelet2-LL_glcm_DifferenceAverage                            | 0.928457      | wavelet2-LL_glcm_DifferenceVariance                           | nan          | nan         |\n| wavelet2-LL_glcm_Autocorrelation                              | 0.96915       | wavelet2-LL_glcm_JointAverage                                 | nan          | nan         |\n| wavelet2-LL_glcm_JointAverage                                 | 1             | wavelet2-LL_glcm_SumAverage                                   | nan          | nan         |\n| wavelet2-LL_glcm_DifferenceEntropy                            | 0.933083      | wavelet2-LL_glcm_SumEntropy                                   | nan          | nan         |\n| wavelet2-LL_glcm_JointEntropy                                 | 0.990171      | wavelet2-LL_glcm_SumEntropy                                   | nan          | nan         |\n| wavelet2-LL_glcm_ClusterTendency                              | 0.996781      | wavelet2-LL_glcm_SumSquares                                   | nan          | nan         |\n| wavelet2-HH_gldm_GrayLevelNonUniformity                       | 0.956034      | wavelet2-LL_glrlm_GrayLevelNonUniformity                      | nan          | nan         |\n| wavelet2-LL_glcm_SumSquares                                   | 0.99755       | wavelet2-LL_glrlm_GrayLevelVariance                           | nan          | nan         |\n| wavelet2-LL_glcm_SumAverage                                   | 0.96195       | wavelet2-LL_glrlm_HighGrayLevelRunEmphasis                    | nan          | nan         |\n| wavelet2-LL_glrlm_HighGrayLevelRunEmphasis                    | 0.999989      | wavelet2-LL_glrlm_LongRunHighGrayLevelEmphasis                | nan          | nan         |\n| wavelet2-LL_glcm_SumEntropy                                   | 0.994899      | wavelet2-LL_glrlm_RunEntropy                                  | nan          | nan         |\n| wavelet2-LL_glrlm_GrayLevelNonUniformity                      | 0.949592      | wavelet2-LL_glrlm_RunLengthNonUniformity                      | nan          | nan         |\n| wavelet2-LL_glrlm_LongRunHighGrayLevelEmphasis                | 0.999983      | wavelet2-LL_glrlm_ShortRunHighGrayLevelEmphasis               | nan          | nan         |\n| wavelet2-LL_glrlm_RunLengthNonUniformity                      | 0.950472      | wavelet2-LL_glszm_GrayLevelNonUniformity                      | nan          | nan         |\n| wavelet2-LL_glrlm_GrayLevelVariance                           | 0.999992      | wavelet2-LL_glszm_GrayLevelVariance                           | nan          | nan         |\n| wavelet2-LL_glrlm_ShortRunHighGrayLevelEmphasis               | 0.999998      | wavelet2-LL_glszm_HighGrayLevelZoneEmphasis                   | nan          | nan         |\n| wavelet2-LL_glszm_HighGrayLevelZoneEmphasis                   | 0.999821      | wavelet2-LL_glszm_LargeAreaHighGrayLevelEmphasis              | nan          | nan         |\n| wavelet2-LL_glszm_GrayLevelNonUniformity                      | 0.948883      | wavelet2-LL_glszm_SizeZoneNonUniformity                       | nan          | nan         |\n| wavelet2-LL_glszm_LargeAreaHighGrayLevelEmphasis              | 0.999723      | wavelet2-LL_glszm_SmallAreaHighGrayLevelEmphasis              | nan          | nan         |\n| wavelet2-LL_glrlm_RunEntropy                                  | 0.999981      | wavelet2-LL_glszm_ZoneEntropy                                 | nan          | nan         |\n| original_ngtdm_Complexity                                     | 0.942036      | wavelet2-LL_ngtdm_Complexity                                  | nan          | nan         |\n| wavelet2-HL_ngtdm_Contrast                                    | 0.997838      | wavelet2-LL_ngtdm_Contrast                                    | nan          | nan         |\n| wavelet2-LL_glszm_ZoneEntropy                                 | 0.999994      | wavelet2-LL_gldm_DependenceEntropy                            | nan          | nan         |\n| wavelet2-LL_glszm_SizeZoneNonUniformity                       | 0.999994      | wavelet2-LL_gldm_DependenceNonUniformity                      | nan          | nan         |\n| wavelet2-LL_gldm_DependenceNonUniformity                      | 0.9469        | wavelet2-LL_gldm_GrayLevelNonUniformity                       | nan          | nan         |\n| wavelet2-LL_glszm_GrayLevelVariance                           | 0.999986      | wavelet2-LL_gldm_GrayLevelVariance                            | nan          | nan         |\n| wavelet2-LL_glszm_SmallAreaHighGrayLevelEmphasis              | 0.999981      | wavelet2-LL_gldm_HighGrayLevelEmphasis                        | nan          | nan         |\n| wavelet2-LL_gldm_HighGrayLevelEmphasis                        | 0.999312      | wavelet2-LL_gldm_LargeDependenceHighGrayLevelEmphasis         | nan          | nan         |\n| wavelet2-LL_gldm_LargeDependenceHighGrayLevelEmphasis         | 0.998926      | wavelet2-LL_gldm_SmallDependenceHighGrayLevelEmphasis         | nan          | nan         |\n| wavelet2-LL_gldm_GrayLevelNonUniformity                       | 0.950357      | square_firstorder_Energy                                      | nan          | nan         |\n| square_firstorder_90Percentile                                | 0.965453      | square_firstorder_InterquartileRange                          | nan          | nan         |\n| square_firstorder_InterquartileRange                          | 0.930458      | square_firstorder_MeanAbsoluteDeviation                       | nan          | nan         |\n| square_firstorder_Mean                                        | 0.929395      | square_firstorder_Median                                      | nan          | nan         |\n| square_firstorder_Maximum                                     | 0.999998      | square_firstorder_Range                                       | nan          | nan         |\n| square_firstorder_MeanAbsoluteDeviation                       | 0.961314      | square_firstorder_RobustMeanAbsoluteDeviation                 | nan          | nan         |\n| square_firstorder_Median                                      | 0.918365      | square_firstorder_RootMeanSquared                             | nan          | nan         |\n| square_firstorder_RobustMeanAbsoluteDeviation                 | 0.907896      | square_firstorder_RootMeanSquared                             | nan          | nan         |\n| square_firstorder_Kurtosis                                    | 0.928208      | square_firstorder_Skewness                                    | nan          | nan         |\n| square_firstorder_Energy                                      | 0.992169      | square_firstorder_TotalEnergy                                 | nan          | nan         |\n| square_glcm_ClusterProminence                                 | 0.981313      | square_glcm_ClusterShade                                      | nan          | nan         |\n| square_firstorder_Variance                                    | 0.971724      | square_glcm_ClusterTendency                                   | nan          | nan         |\n| square_firstorder_Entropy                                     | 0.948633      | square_glcm_DifferenceEntropy                                 | nan          | nan         |\n| square_glcm_Contrast                                          | 0.98233       | square_glcm_DifferenceVariance                                | nan          | nan         |\n| square_firstorder_RootMeanSquared                             | 0.990987      | square_glcm_JointAverage                                      | nan          | nan         |\n| square_glcm_JointAverage                                      | 1             | square_glcm_SumAverage                                        | nan          | nan         |\n| square_glcm_DifferenceEntropy                                 | 0.939246      | square_glcm_SumEntropy                                        | nan          | nan         |\n| square_glcm_ClusterTendency                                   | 0.992089      | square_glcm_SumSquares                                        | nan          | nan         |\n| square_glcm_DifferenceVariance                                | 0.900673      | square_glcm_SumSquares                                        | nan          | nan         |\n| square_glcm_SumSquares                                        | 0.990794      | square_glrlm_GrayLevelVariance                                | nan          | nan         |\n| square_glcm_Autocorrelation                                   | 0.935576      | square_glrlm_HighGrayLevelRunEmphasis                         | nan          | nan         |\n| square_glrlm_GrayLevelVariance                                | 0.928886      | square_glrlm_HighGrayLevelRunEmphasis                         | nan          | nan         |\n| square_glrlm_HighGrayLevelRunEmphasis                         | 0.999948      | square_glrlm_LongRunHighGrayLevelEmphasis                     | nan          | nan         |\n| square_glrlm_LongRunEmphasis                                  | 0.977243      | square_glrlm_LongRunLowGrayLevelEmphasis                      | nan          | nan         |\n| square_glcm_SumEntropy                                        | 0.978013      | square_glrlm_RunEntropy                                       | nan          | nan         |\n| square_firstorder_TotalEnergy                                 | 0.981716      | square_glrlm_RunLengthNonUniformity                           | nan          | nan         |\n| square_glrlm_LongRunHighGrayLevelEmphasis                     | 0.999919      | square_glrlm_ShortRunHighGrayLevelEmphasis                    | nan          | nan         |\n| square_glrlm_GrayLevelNonUniformity                           | 0.961046      | square_glszm_GrayLevelNonUniformity                           | nan          | nan         |\n| square_glrlm_ShortRunHighGrayLevelEmphasis                    | 0.923164      | square_glszm_GrayLevelVariance                                | nan          | nan         |\n| square_glszm_GrayLevelVariance                                | 0.934128      | square_glszm_HighGrayLevelZoneEmphasis                        | nan          | nan         |\n| square_glszm_HighGrayLevelZoneEmphasis                        | 0.999182      | square_glszm_LargeAreaHighGrayLevelEmphasis                   | nan          | nan         |\n| square_glszm_LargeAreaEmphasis                                | 0.999691      | square_glszm_LargeAreaLowGrayLevelEmphasis                    | nan          | nan         |\n| square_glrlm_RunLengthNonUniformity                           | 0.9903        | square_glszm_SizeZoneNonUniformity                            | nan          | nan         |\n| square_glszm_LargeAreaHighGrayLevelEmphasis                   | 0.998769      | square_glszm_SmallAreaHighGrayLevelEmphasis                   | nan          | nan         |\n| square_glrlm_RunEntropy                                       | 0.971058      | square_glszm_ZoneEntropy                                      | nan          | nan         |\n| square_glszm_LargeAreaLowGrayLevelEmphasis                    | 0.999099      | square_glszm_ZoneVariance                                     | nan          | nan         |\n| wavelet2-LL_ngtdm_Contrast                                    | 0.998066      | square_ngtdm_Contrast                                         | nan          | nan         |\n| square_glszm_ZoneEntropy                                      | 0.993029      | square_gldm_DependenceEntropy                                 | nan          | nan         |\n| square_glszm_SizeZoneNonUniformity                            | 0.997007      | square_gldm_DependenceNonUniformity                           | nan          | nan         |\n| square_glrlm_LongRunLowGrayLevelEmphasis                      | 0.922718      | square_gldm_DependenceNonUniformityNormalized                 | nan          | nan         |\n| square_glszm_GrayLevelNonUniformity                           | 0.916453      | square_gldm_GrayLevelNonUniformity                            | nan          | nan         |\n| square_glszm_SmallAreaHighGrayLevelEmphasis                   | 0.944132      | square_gldm_GrayLevelVariance                                 | nan          | nan         |\n| square_ngtdm_Strength                                         | 0.943988      | square_gldm_GrayLevelVariance                                 | nan          | nan         |\n| square_gldm_GrayLevelVariance                                 | 0.926457      | square_gldm_HighGrayLevelEmphasis                             | nan          | nan         |\n| square_gldm_DependenceNonUniformityNormalized                 | 0.913037      | square_gldm_LargeDependenceEmphasis                           | nan          | nan         |\n| square_gldm_DependenceVariance                                | 0.99437       | square_gldm_LargeDependenceEmphasis                           | nan          | nan         |\n| square_gldm_HighGrayLevelEmphasis                             | 0.996666      | square_gldm_LargeDependenceHighGrayLevelEmphasis              | nan          | nan         |\n| square_gldm_LargeDependenceEmphasis                           | 0.992817      | square_gldm_LargeDependenceLowGrayLevelEmphasis               | nan          | nan         |\n| square_gldm_LargeDependenceLowGrayLevelEmphasis               | 0.926203      | square_gldm_SmallDependenceEmphasis                           | nan          | nan         |\n| square_gldm_LargeDependenceHighGrayLevelEmphasis              | 0.994916      | square_gldm_SmallDependenceHighGrayLevelEmphasis              | nan          | nan         |\n| wavelet2-LL_firstorder_RootMeanSquared                        | 0.934465      | squareroot_firstorder_90Percentile                            | nan          | nan         |\n| wavelet2-LL_firstorder_TotalEnergy                            | 0.952159      | squareroot_firstorder_Energy                                  | nan          | nan         |\n| wavelet2-LL_gldm_DependenceEntropy                            | 0.922294      | squareroot_firstorder_Entropy                                 | nan          | nan         |\n| squareroot_firstorder_InterquartileRange                      | 0.94865       | squareroot_firstorder_MeanAbsoluteDeviation                   | nan          | nan         |\n| squareroot_firstorder_90Percentile                            | 0.914742      | squareroot_firstorder_Mean                                    | nan          | nan         |\n| squareroot_firstorder_Mean                                    | 0.980426      | squareroot_firstorder_Median                                  | nan          | nan         |\n| squareroot_firstorder_MeanAbsoluteDeviation                   | 0.978448      | squareroot_firstorder_RobustMeanAbsoluteDeviation             | nan          | nan         |\n| squareroot_firstorder_Median                                  | 0.973541      | squareroot_firstorder_RootMeanSquared                         | nan          | nan         |\n| squareroot_firstorder_RootMeanSquared                         | 0.928279      | squareroot_firstorder_Skewness                                | nan          | nan         |\n| squareroot_firstorder_Energy                                  | 0.986684      | squareroot_firstorder_TotalEnergy                             | nan          | nan         |\n| squareroot_firstorder_RobustMeanAbsoluteDeviation             | 0.933193      | squareroot_firstorder_Variance                                | nan          | nan         |\n| squareroot_firstorder_Variance                                | 0.989342      | squareroot_glcm_ClusterTendency                               | nan          | nan         |\n| squareroot_glcm_ClusterProminence                             | 0.91712       | squareroot_glcm_ClusterTendency                               | nan          | nan         |\n| original_glcm_Correlation                                     | 0.972023      | squareroot_glcm_Correlation                                   | nan          | nan         |\n| squareroot_glcm_Contrast                                      | 0.980724      | squareroot_glcm_DifferenceAverage                             | nan          | nan         |\n| squareroot_firstorder_Entropy                                 | 0.958259      | squareroot_glcm_DifferenceEntropy                             | nan          | nan         |\n| squareroot_glcm_DifferenceAverage                             | 0.946353      | squareroot_glcm_DifferenceVariance                            | nan          | nan         |\n| squareroot_glcm_Autocorrelation                               | 0.975948      | squareroot_glcm_JointAverage                                  | nan          | nan         |\n| squareroot_glcm_JointAverage                                  | 1             | squareroot_glcm_SumAverage                                    | nan          | nan         |\n| squareroot_glcm_DifferenceEntropy                             | 0.961973      | squareroot_glcm_SumEntropy                                    | nan          | nan         |\n| squareroot_glcm_JointEntropy                                  | 0.9641        | squareroot_glcm_SumEntropy                                    | nan          | nan         |\n| squareroot_glcm_ClusterTendency                               | 0.991481      | squareroot_glcm_SumSquares                                    | nan          | nan         |\n| squareroot_glcm_Imc1                                          | 0.934681      | squareroot_glrlm_GrayLevelNonUniformity                       | nan          | nan         |\n| squareroot_glcm_SumSquares                                    | 0.996999      | squareroot_glrlm_GrayLevelVariance                            | nan          | nan         |\n| squareroot_glcm_SumAverage                                    | 0.977029      | squareroot_glrlm_HighGrayLevelRunEmphasis                     | nan          | nan         |\n| squareroot_glrlm_HighGrayLevelRunEmphasis                     | 0.999962      | squareroot_glrlm_LongRunHighGrayLevelEmphasis                 | nan          | nan         |\n| squareroot_glcm_SumEntropy                                    | 0.953292      | squareroot_glrlm_RunEntropy                                   | nan          | nan         |\n| square_gldm_DependenceNonUniformity                           | 0.937147      | squareroot_glrlm_RunLengthNonUniformity                       | nan          | nan         |\n| squareroot_glrlm_LongRunHighGrayLevelEmphasis                 | 0.999941      | squareroot_glrlm_ShortRunHighGrayLevelEmphasis                | nan          | nan         |\n| squareroot_glrlm_GrayLevelNonUniformity                       | 0.999709      | squareroot_glszm_GrayLevelNonUniformity                       | nan          | nan         |\n| squareroot_glrlm_GrayLevelVariance                            | 0.999985      | squareroot_glszm_GrayLevelVariance                            | nan          | nan         |\n| squareroot_glrlm_ShortRunHighGrayLevelEmphasis                | 0.999996      | squareroot_glszm_HighGrayLevelZoneEmphasis                    | nan          | nan         |\n| squareroot_glszm_HighGrayLevelZoneEmphasis                    | 0.99936       | squareroot_glszm_LargeAreaHighGrayLevelEmphasis               | nan          | nan         |\n| squareroot_glrlm_RunLengthNonUniformity                       | 0.999685      | squareroot_glszm_SizeZoneNonUniformity                        | nan          | nan         |\n| squareroot_glszm_LargeAreaHighGrayLevelEmphasis               | 0.999022      | squareroot_glszm_SmallAreaHighGrayLevelEmphasis               | nan          | nan         |\n| squareroot_glrlm_RunEntropy                                   | 0.999626      | squareroot_glszm_ZoneEntropy                                  | nan          | nan         |\n| square_ngtdm_Contrast                                         | 0.950572      | squareroot_ngtdm_Contrast                                     | nan          | nan         |\n| squareroot_glszm_ZoneEntropy                                  | 0.999818      | squareroot_gldm_DependenceEntropy                             | nan          | nan         |\n| squareroot_glszm_SizeZoneNonUniformity                        | 0.999958      | squareroot_gldm_DependenceNonUniformity                       | nan          | nan         |\n| squareroot_glszm_GrayLevelNonUniformity                       | 0.999482      | squareroot_gldm_GrayLevelNonUniformity                        | nan          | nan         |\n| squareroot_glszm_GrayLevelVariance                            | 0.999974      | squareroot_gldm_GrayLevelVariance                             | nan          | nan         |\n| squareroot_glszm_SmallAreaHighGrayLevelEmphasis               | 0.999951      | squareroot_gldm_HighGrayLevelEmphasis                         | nan          | nan         |\n| squareroot_gldm_HighGrayLevelEmphasis                         | 0.997676      | squareroot_gldm_LargeDependenceHighGrayLevelEmphasis          | nan          | nan         |\n| squareroot_gldm_LargeDependenceHighGrayLevelEmphasis          | 0.996407      | squareroot_gldm_SmallDependenceHighGrayLevelEmphasis          | nan          | nan         |\n| squareroot_firstorder_10Percentile                            | 0.952028      | logarithm_firstorder_10Percentile                             | nan          | nan         |\n| squareroot_firstorder_TotalEnergy                             | 0.932639      | logarithm_firstorder_Energy                                   | nan          | nan         |\n| squareroot_gldm_DependenceEntropy                             | 0.986621      | logarithm_firstorder_Entropy                                  | nan          | nan         |\n| squareroot_firstorder_Maximum                                 | 0.910511      | logarithm_firstorder_Maximum                                  | nan          | nan         |\n| squareroot_firstorder_Range                                   | 0.904696      | logarithm_firstorder_Maximum                                  | nan          | nan         |\n| squareroot_gldm_GrayLevelVariance                             | 0.930483      | logarithm_firstorder_MeanAbsoluteDeviation                    | nan          | nan         |\n| logarithm_firstorder_InterquartileRange                       | 0.924825      | logarithm_firstorder_MeanAbsoluteDeviation                    | nan          | nan         |\n| squareroot_firstorder_Skewness                                | 0.927599      | logarithm_firstorder_Mean                                     | nan          | nan         |\n| logarithm_firstorder_Mean                                     | 0.960462      | logarithm_firstorder_Median                                   | nan          | nan         |\n| diagnostics_Image-interpolated_Maximum                        | 0.926888      | logarithm_firstorder_Minimum                                  | nan          | nan         |\n| squareroot_firstorder_Minimum                                 | 0.900712      | logarithm_firstorder_Minimum                                  | nan          | nan         |\n| logarithm_firstorder_Maximum                                  | 0.931042      | logarithm_firstorder_Range                                    | nan          | nan         |\n| logarithm_firstorder_MeanAbsoluteDeviation                    | 0.970882      | logarithm_firstorder_RobustMeanAbsoluteDeviation              | nan          | nan         |\n| logarithm_firstorder_Median                                   | 0.931882      | logarithm_firstorder_RootMeanSquared                          | nan          | nan         |\n| logarithm_firstorder_Energy                                   | 0.986008      | logarithm_firstorder_TotalEnergy                              | nan          | nan         |\n| logarithm_firstorder_RobustMeanAbsoluteDeviation              | 0.927337      | logarithm_firstorder_Variance                                 | nan          | nan         |\n| squareroot_gldm_SmallDependenceHighGrayLevelEmphasis          | 0.970426      | logarithm_glcm_Autocorrelation                                | nan          | nan         |\n| squareroot_glcm_ClusterShade                                  | 0.941417      | logarithm_glcm_ClusterShade                                   | nan          | nan         |\n| logarithm_firstorder_Variance                                 | 0.989298      | logarithm_glcm_ClusterTendency                                | nan          | nan         |\n| squareroot_glcm_DifferenceVariance                            | 0.933376      | logarithm_glcm_Contrast                                       | nan          | nan         |\n| squareroot_glcm_Correlation                                   | 0.97952       | logarithm_glcm_Correlation                                    | nan          | nan         |\n| logarithm_glcm_Contrast                                       | 0.972445      | logarithm_glcm_DifferenceAverage                              | nan          | nan         |\n| logarithm_firstorder_Entropy                                  | 0.963132      | logarithm_glcm_DifferenceEntropy                              | nan          | nan         |\n| logarithm_glcm_DifferenceAverage                              | 0.932278      | logarithm_glcm_DifferenceVariance                             | nan          | nan         |\n| logarithm_firstorder_RootMeanSquared                          | 0.937448      | logarithm_glcm_JointAverage                                   | nan          | nan         |\n| logarithm_glcm_Autocorrelation                                | 0.972417      | logarithm_glcm_JointAverage                                   | nan          | nan         |\n| logarithm_glcm_JointAverage                                   | 1             | logarithm_glcm_SumAverage                                     | nan          | nan         |\n| logarithm_glcm_DifferenceEntropy                              | 0.958663      | logarithm_glcm_SumEntropy                                     | nan          | nan         |\n| logarithm_glcm_JointEntropy                                   | 0.949348      | logarithm_glcm_SumEntropy                                     | nan          | nan         |\n| logarithm_glcm_ClusterTendency                                | 0.991607      | logarithm_glcm_SumSquares                                     | nan          | nan         |\n| logarithm_glcm_DifferenceVariance                             | 0.928961      | logarithm_glcm_SumSquares                                     | nan          | nan         |\n| squareroot_gldm_GrayLevelNonUniformity                        | 0.984399      | logarithm_glrlm_GrayLevelNonUniformity                        | nan          | nan         |\n| logarithm_glcm_Imc1                                           | 0.930764      | logarithm_glrlm_GrayLevelNonUniformity                        | nan          | nan         |\n| logarithm_glcm_SumSquares                                     | 0.997186      | logarithm_glrlm_GrayLevelVariance                             | nan          | nan         |\n| logarithm_glcm_SumAverage                                     | 0.977996      | logarithm_glrlm_HighGrayLevelRunEmphasis                      | nan          | nan         |\n| logarithm_glrlm_HighGrayLevelRunEmphasis                      | 0.99997       | logarithm_glrlm_LongRunHighGrayLevelEmphasis                  | nan          | nan         |\n| logarithm_glcm_SumEntropy                                     | 0.952729      | logarithm_glrlm_RunEntropy                                    | nan          | nan         |\n| squareroot_gldm_DependenceNonUniformity                       | 0.99942       | logarithm_glrlm_RunLengthNonUniformity                        | nan          | nan         |\n| logarithm_glrlm_LongRunHighGrayLevelEmphasis                  | 0.999954      | logarithm_glrlm_ShortRunHighGrayLevelEmphasis                 | nan          | nan         |\n| logarithm_glrlm_GrayLevelNonUniformity                        | 0.99976       | logarithm_glszm_GrayLevelNonUniformity                        | nan          | nan         |\n| logarithm_glrlm_GrayLevelVariance                             | 0.999992      | logarithm_glszm_GrayLevelVariance                             | nan          | nan         |\n| logarithm_glrlm_ShortRunHighGrayLevelEmphasis                 | 0.999996      | logarithm_glszm_HighGrayLevelZoneEmphasis                     | nan          | nan         |\n| logarithm_glszm_HighGrayLevelZoneEmphasis                     | 0.999507      | logarithm_glszm_LargeAreaHighGrayLevelEmphasis                | nan          | nan         |\n| logarithm_glrlm_RunLengthNonUniformity                        | 0.999631      | logarithm_glszm_SizeZoneNonUniformity                         | nan          | nan         |\n| logarithm_glszm_LargeAreaHighGrayLevelEmphasis                | 0.999243      | logarithm_glszm_SmallAreaHighGrayLevelEmphasis                | nan          | nan         |\n| logarithm_glrlm_RunEntropy                                    | 0.9996        | logarithm_glszm_ZoneEntropy                                   | nan          | nan         |\n| squareroot_ngtdm_Complexity                                   | 0.968414      | logarithm_ngtdm_Complexity                                    | nan          | nan         |\n| squareroot_ngtdm_Contrast                                     | 0.979159      | logarithm_ngtdm_Contrast                                      | nan          | nan         |\n| squareroot_ngtdm_Strength                                     | 0.968489      | logarithm_ngtdm_Strength                                      | nan          | nan         |\n| logarithm_glszm_ZoneEntropy                                   | 0.999812      | logarithm_gldm_DependenceEntropy                              | nan          | nan         |\n| logarithm_glszm_SizeZoneNonUniformity                         | 0.999948      | logarithm_gldm_DependenceNonUniformity                        | nan          | nan         |\n| logarithm_glszm_GrayLevelNonUniformity                        | 0.999573      | logarithm_gldm_GrayLevelNonUniformity                         | nan          | nan         |\n| logarithm_glszm_GrayLevelVariance                             | 0.999985      | logarithm_gldm_GrayLevelVariance                              | nan          | nan         |\n| logarithm_glszm_SmallAreaHighGrayLevelEmphasis                | 0.999955      | logarithm_gldm_HighGrayLevelEmphasis                          | nan          | nan         |\n| logarithm_gldm_HighGrayLevelEmphasis                          | 0.998194      | logarithm_gldm_LargeDependenceHighGrayLevelEmphasis           | nan          | nan         |\n| logarithm_gldm_LargeDependenceHighGrayLevelEmphasis           | 0.997196      | logarithm_gldm_SmallDependenceHighGrayLevelEmphasis           | nan          | nan         |\n| logarithm_gldm_DependenceNonUniformity                        | 0.998899      | exponential_firstorder_Energy                                 | nan          | nan         |\n| exponential_firstorder_90Percentile                           | 0.98514       | exponential_firstorder_InterquartileRange                     | nan          | nan         |\n| exponential_firstorder_InterquartileRange                     | 0.961294      | exponential_firstorder_Mean                                   | nan          | nan         |\n| exponential_firstorder_MeanAbsoluteDeviation                  | 0.926155      | exponential_firstorder_Mean                                   | nan          | nan         |\n| exponential_firstorder_10Percentile                           | 0.912164      | exponential_firstorder_Median                                 | nan          | nan         |\n| exponential_firstorder_Mean                                   | 0.91123       | exponential_firstorder_Median                                 | nan          | nan         |\n| exponential_firstorder_Maximum                                | 0.999989      | exponential_firstorder_Range                                  | nan          | nan         |\n| exponential_firstorder_RobustMeanAbsoluteDeviation            | 0.957105      | exponential_firstorder_RootMeanSquared                        | nan          | nan         |\n| exponential_firstorder_Kurtosis                               | 0.943508      | exponential_firstorder_Skewness                               | nan          | nan         |\n| exponential_firstorder_Energy                                 | 0.991866      | exponential_firstorder_TotalEnergy                            | nan          | nan         |\n| exponential_firstorder_Entropy                                | 0.936959      | exponential_firstorder_Uniformity                             | nan          | nan         |\n| square_glcm_ClusterShade                                      | 0.96877       | exponential_glcm_Autocorrelation                              | nan          | nan         |\n| exponential_firstorder_Variance                               | 0.929397      | exponential_glcm_ClusterProminence                            | nan          | nan         |\n| exponential_glcm_ClusterProminence                            | 0.99839       | exponential_glcm_ClusterShade                                 | nan          | nan         |\n| exponential_glcm_Autocorrelation                              | 0.936423      | exponential_glcm_ClusterTendency                              | nan          | nan         |\n| exponential_glcm_ClusterShade                                 | 0.968976      | exponential_glcm_ClusterTendency                              | nan          | nan         |\n| exponential_glcm_ClusterTendency                              | 0.958533      | exponential_glcm_Contrast                                     | nan          | nan         |\n| exponential_firstorder_RootMeanSquared                        | 0.946884      | exponential_glcm_DifferenceAverage                            | nan          | nan         |\n| exponential_firstorder_Uniformity                             | 0.908323      | exponential_glcm_DifferenceEntropy                            | nan          | nan         |\n| exponential_glcm_Contrast                                     | 0.997736      | exponential_glcm_DifferenceVariance                           | nan          | nan         |\n| exponential_glcm_DifferenceEntropy                            | 0.970256      | exponential_glcm_Id                                           | nan          | nan         |\n| exponential_glcm_Id                                           | 0.999235      | exponential_glcm_Idm                                          | nan          | nan         |\n| exponential_firstorder_Median                                 | 0.903659      | exponential_glcm_JointAverage                                 | nan          | nan         |\n| exponential_glcm_Idm                                          | 0.938013      | exponential_glcm_JointEntropy                                 | nan          | nan         |\n| exponential_glcm_JointAverage                                 | 1             | exponential_glcm_SumAverage                                   | nan          | nan         |\n| exponential_glcm_JointEntropy                                 | 0.987759      | exponential_glcm_SumEntropy                                   | nan          | nan         |\n| exponential_glcm_DifferenceVariance                           | 0.978537      | exponential_glcm_SumSquares                                   | nan          | nan         |\n| exponential_firstorder_TotalEnergy                            | 0.930052      | exponential_glrlm_GrayLevelNonUniformity                      | nan          | nan         |\n| exponential_glcm_SumSquares                                   | 0.978596      | exponential_glrlm_GrayLevelVariance                           | nan          | nan         |\n| exponential_glrlm_GrayLevelVariance                           | 0.995134      | exponential_glrlm_HighGrayLevelRunEmphasis                    | nan          | nan         |\n| exponential_glcm_MaximumProbability                           | 0.926455      | exponential_glrlm_LongRunEmphasis                             | nan          | nan         |\n| exponential_glrlm_HighGrayLevelRunEmphasis                    | 0.999949      | exponential_glrlm_LongRunHighGrayLevelEmphasis                | nan          | nan         |\n| exponential_glcm_SumEntropy                                   | 0.911181      | exponential_glrlm_RunEntropy                                  | nan          | nan         |\n| exponential_glrlm_RunLengthNonUniformityNormalized            | 0.987817      | exponential_glrlm_RunPercentage                               | nan          | nan         |\n| exponential_glrlm_LongRunEmphasis                             | 0.99434       | exponential_glrlm_RunVariance                                 | nan          | nan         |\n| exponential_glrlm_LongRunHighGrayLevelEmphasis                | 0.999936      | exponential_glrlm_ShortRunHighGrayLevelEmphasis               | nan          | nan         |\n| exponential_glrlm_GrayLevelNonUniformity                      | 0.902093      | exponential_glszm_GrayLevelNonUniformity                      | nan          | nan         |\n| exponential_glrlm_ShortRunHighGrayLevelEmphasis               | 0.989744      | exponential_glszm_GrayLevelVariance                           | nan          | nan         |\n| exponential_glszm_GrayLevelVariance                           | 0.995554      | exponential_glszm_HighGrayLevelZoneEmphasis                   | nan          | nan         |\n| exponential_glrlm_RunLengthNonUniformity                      | 0.910935      | exponential_glszm_SizeZoneNonUniformity                       | nan          | nan         |\n| exponential_glszm_HighGrayLevelZoneEmphasis                   | 0.999979      | exponential_glszm_SmallAreaHighGrayLevelEmphasis              | nan          | nan         |\n| exponential_glrlm_RunPercentage                               | 0.946603      | exponential_glszm_ZonePercentage                              | nan          | nan         |\n| exponential_glszm_LargeAreaEmphasis                           | 0.999133      | exponential_glszm_ZoneVariance                                | nan          | nan         |\n| exponential_glszm_SmallAreaHighGrayLevelEmphasis              | 0.977379      | exponential_ngtdm_Complexity                                  | nan          | nan         |\n| exponential_firstorder_Range                                  | 0.934449      | exponential_ngtdm_Strength                                    | nan          | nan         |\n| exponential_ngtdm_Complexity                                  | 0.931437      | exponential_ngtdm_Strength                                    | nan          | nan         |\n| exponential_glrlm_RunEntropy                                  | 0.91459       | exponential_gldm_DependenceEntropy                            | nan          | nan         |\n| exponential_glszm_ZoneEntropy                                 | 0.922259      | exponential_gldm_DependenceEntropy                            | nan          | nan         |\n| exponential_glszm_ZonePercentage                              | 0.934586      | exponential_gldm_DependenceNonUniformityNormalized            | nan          | nan         |\n| exponential_ngtdm_Strength                                    | 0.954934      | exponential_gldm_GrayLevelVariance                            | nan          | nan         |\n| exponential_gldm_GrayLevelVariance                            | 0.994819      | exponential_gldm_HighGrayLevelEmphasis                        | nan          | nan         |\n| exponential_glrlm_RunVariance                                 | 0.931705      | exponential_gldm_LargeDependenceEmphasis                      | nan          | nan         |\n| exponential_gldm_DependenceVariance                           | 0.933224      | exponential_gldm_LargeDependenceEmphasis                      | nan          | nan         |\n| exponential_gldm_HighGrayLevelEmphasis                        | 0.997644      | exponential_gldm_LargeDependenceHighGrayLevelEmphasis         | nan          | nan         |\n| exponential_glrlm_LongRunLowGrayLevelEmphasis                 | 0.981298      | exponential_gldm_LargeDependenceLowGrayLevelEmphasis          | nan          | nan         |\n| exponential_glrlm_LowGrayLevelRunEmphasis                     | 0.90872       | exponential_gldm_LargeDependenceLowGrayLevelEmphasis          | nan          | nan         |\n| exponential_gldm_LargeDependenceLowGrayLevelEmphasis          | 0.931509      | exponential_gldm_LowGrayLevelEmphasis                         | nan          | nan         |\n| exponential_glszm_SizeZoneNonUniformityNormalized             | 0.900866      | exponential_gldm_SmallDependenceEmphasis                      | nan          | nan         |\n| exponential_gldm_DependenceNonUniformityNormalized            | 0.953005      | exponential_gldm_SmallDependenceEmphasis                      | nan          | nan         |\n| exponential_gldm_LargeDependenceHighGrayLevelEmphasis         | 0.996887      | exponential_gldm_SmallDependenceHighGrayLevelEmphasis         | nan          | nan         |\n| diagnostics_Image-original_Minimum                            | nan           | nan                                                           | low_variance | 0           |\n| original_shape_Sphericity                                     | nan           | nan                                                           | low_variance | 0.0063278   |\n| original_firstorder_Uniformity                                | nan           | nan                                                           | low_variance | 2.61751e-05 |\n| original_glcm_Id                                              | nan           | nan                                                           | low_variance | 4.80509e-05 |\n| original_glcm_Idm                                             | nan           | nan                                                           | low_variance | 1.30322e-05 |\n| original_glcm_Idmn                                            | nan           | nan                                                           | low_variance | 0.000123531 |\n| original_glcm_Idn                                             | nan           | nan                                                           | low_variance | 0.000517724 |\n| original_glcm_Imc2                                            | nan           | nan                                                           | low_variance | 5.12614e-07 |\n| original_glcm_InverseVariance                                 | nan           | nan                                                           | low_variance | 1.26335e-05 |\n| original_glcm_JointEnergy                                     | nan           | nan                                                           | low_variance | 1.42755e-05 |\n| original_glcm_MCC                                             | nan           | nan                                                           | low_variance | 0.00264757  |\n| original_glcm_MaximumProbability                              | nan           | nan                                                           | low_variance | 1.57571e-05 |\n| original_glrlm_GrayLevelNonUniformityNormalized               | nan           | nan                                                           | low_variance | 2.61219e-05 |\n| original_glrlm_LongRunEmphasis                                | nan           | nan                                                           | low_variance | 4.9629e-05  |\n| original_glrlm_LongRunLowGrayLevelEmphasis                    | nan           | nan                                                           | low_variance | 2.87107e-05 |\n| original_glrlm_LowGrayLevelRunEmphasis                        | nan           | nan                                                           | low_variance | 2.86966e-05 |\n| original_glrlm_RunLengthNonUniformityNormalized               | nan           | nan                                                           | low_variance | 2.08126e-05 |\n| original_glrlm_RunPercentage                                  | nan           | nan                                                           | low_variance | 5.29171e-06 |\n| original_glrlm_RunVariance                                    | nan           | nan                                                           | low_variance | 5.49625e-06 |\n| original_glrlm_ShortRunEmphasis                               | nan           | nan                                                           | low_variance | 3.0222e-06  |\n| original_glrlm_ShortRunLowGrayLevelEmphasis                   | nan           | nan                                                           | low_variance | 2.86958e-05 |\n| original_glszm_GrayLevelNonUniformityNormalized               | nan           | nan                                                           | low_variance | 2.59566e-05 |\n| original_glszm_LargeAreaEmphasis                              | nan           | nan                                                           | low_variance | 0.000896548 |\n| original_glszm_LargeAreaLowGrayLevelEmphasis                  | nan           | nan                                                           | low_variance | 2.89462e-05 |\n| original_glszm_LowGrayLevelZoneEmphasis                       | nan           | nan                                                           | low_variance | 2.88537e-05 |\n| original_glszm_SizeZoneNonUniformityNormalized                | nan           | nan                                                           | low_variance | 0.000292146 |\n| original_glszm_SmallAreaEmphasis                              | nan           | nan                                                           | low_variance | 4.60375e-05 |\n| original_glszm_SmallAreaLowGrayLevelEmphasis                  | nan           | nan                                                           | low_variance | 2.88738e-05 |\n| original_glszm_ZonePercentage                                 | nan           | nan                                                           | low_variance | 8.18205e-05 |\n| original_glszm_ZoneVariance                                   | nan           | nan                                                           | low_variance | 0.000109161 |\n| original_ngtdm_Busyness                                       | nan           | nan                                                           | low_variance | 1.17422e-05 |\n| original_ngtdm_Coarseness                                     | nan           | nan                                                           | low_variance | 1.09245e-05 |\n| original_gldm_DependenceNonUniformityNormalized               | nan           | nan                                                           | low_variance | 0.00102138  |\n| original_gldm_DependenceVariance                              | nan           | nan                                                           | low_variance | 0.000342775 |\n| original_gldm_LargeDependenceEmphasis                         | nan           | nan                                                           | low_variance | 0.00326142  |\n| original_gldm_LargeDependenceLowGrayLevelEmphasis             | nan           | nan                                                           | low_variance | 2.9279e-05  |\n| original_gldm_LowGrayLevelEmphasis                            | nan           | nan                                                           | low_variance | 2.86501e-05 |\n| original_gldm_SmallDependenceEmphasis                         | nan           | nan                                                           | low_variance | 0.000176164 |\n| original_gldm_SmallDependenceLowGrayLevelEmphasis             | nan           | nan                                                           | low_variance | 2.86618e-05 |\n| log-sigma-0-6-mm-3D_firstorder_Uniformity                     | nan           | nan                                                           | low_variance | 2.80343e-05 |\n| log-sigma-0-6-mm-3D_glcm_Correlation                          | nan           | nan                                                           | low_variance | 0.00527264  |\n| log-sigma-0-6-mm-3D_glcm_Id                                   | nan           | nan                                                           | low_variance | 0.00016465  |\n| log-sigma-0-6-mm-3D_glcm_Idm                                  | nan           | nan                                                           | low_variance | 4.6369e-05  |\n| log-sigma-0-6-mm-3D_glcm_Idmn                                 | nan           | nan                                                           | low_variance | 0.000128874 |\n| log-sigma-0-6-mm-3D_glcm_Idn                                  | nan           | nan                                                           | low_variance | 0.000556219 |\n| log-sigma-0-6-mm-3D_glcm_Imc2                                 | nan           | nan                                                           | low_variance | 3.97268e-06 |\n| log-sigma-0-6-mm-3D_glcm_InverseVariance                      | nan           | nan                                                           | low_variance | 5.25568e-05 |\n| log-sigma-0-6-mm-3D_glcm_JointEnergy                          | nan           | nan                                                           | low_variance | 1.44725e-05 |\n| log-sigma-0-6-mm-3D_glcm_MCC                                  | nan           | nan                                                           | low_variance | 0.0063019   |\n| log-sigma-0-6-mm-3D_glcm_MaximumProbability                   | nan           | nan                                                           | low_variance | 2.17172e-05 |\n| log-sigma-0-6-mm-3D_glrlm_GrayLevelNonUniformityNormalized    | nan           | nan                                                           | low_variance | 2.77776e-05 |\n| log-sigma-0-6-mm-3D_glrlm_LongRunEmphasis                     | nan           | nan                                                           | low_variance | 7.88677e-05 |\n| log-sigma-0-6-mm-3D_glrlm_LongRunLowGrayLevelEmphasis         | nan           | nan                                                           | low_variance | 2.97212e-05 |\n| log-sigma-0-6-mm-3D_glrlm_LowGrayLevelRunEmphasis             | nan           | nan                                                           | low_variance | 2.97258e-05 |\n| log-sigma-0-6-mm-3D_glrlm_RunLengthNonUniformityNormalized    | nan           | nan                                                           | low_variance | 3.1418e-05  |\n| log-sigma-0-6-mm-3D_glrlm_RunPercentage                       | nan           | nan                                                           | low_variance | 8.17832e-06 |\n| log-sigma-0-6-mm-3D_glrlm_RunVariance                         | nan           | nan                                                           | low_variance | 9.20639e-06 |\n| log-sigma-0-6-mm-3D_glrlm_ShortRunEmphasis                    | nan           | nan                                                           | low_variance | 4.60514e-06 |\n| log-sigma-0-6-mm-3D_glrlm_ShortRunLowGrayLevelEmphasis        | nan           | nan                                                           | low_variance | 2.97275e-05 |\n| log-sigma-0-6-mm-3D_glszm_GrayLevelNonUniformityNormalized    | nan           | nan                                                           | low_variance | 2.71619e-05 |\n| log-sigma-0-6-mm-3D_glszm_LargeAreaEmphasis                   | nan           | nan                                                           | low_variance | 0.0014479   |\n| log-sigma-0-6-mm-3D_glszm_LargeAreaLowGrayLevelEmphasis       | nan           | nan                                                           | low_variance | 3.06058e-05 |\n| log-sigma-0-6-mm-3D_glszm_LowGrayLevelZoneEmphasis            | nan           | nan                                                           | low_variance | 3.06183e-05 |\n| log-sigma-0-6-mm-3D_glszm_SizeZoneNonUniformityNormalized     | nan           | nan                                                           | low_variance | 0.000445337 |\n| log-sigma-0-6-mm-3D_glszm_SmallAreaEmphasis                   | nan           | nan                                                           | low_variance | 7.15797e-05 |\n| log-sigma-0-6-mm-3D_glszm_SmallAreaLowGrayLevelEmphasis       | nan           | nan                                                           | low_variance | 3.06307e-05 |\n| log-sigma-0-6-mm-3D_glszm_ZonePercentage                      | nan           | nan                                                           | low_variance | 0.000126885 |\n| log-sigma-0-6-mm-3D_glszm_ZoneVariance                        | nan           | nan                                                           | low_variance | 0.000191855 |\n| log-sigma-0-6-mm-3D_ngtdm_Busyness                            | nan           | nan                                                           | low_variance | 1.07206e-05 |\n| log-sigma-0-6-mm-3D_ngtdm_Coarseness                          | nan           | nan                                                           | low_variance | 4.45256e-05 |\n| log-sigma-0-6-mm-3D_gldm_DependenceNonUniformityNormalized    | nan           | nan                                                           | low_variance | 0.00150163  |\n| log-sigma-0-6-mm-3D_gldm_DependenceVariance                   | nan           | nan                                                           | low_variance | 0.000548208 |\n| log-sigma-0-6-mm-3D_gldm_LargeDependenceEmphasis              | nan           | nan                                                           | low_variance | 0.00510533  |\n| log-sigma-0-6-mm-3D_gldm_LargeDependenceLowGrayLevelEmphasis  | nan           | nan                                                           | low_variance | 2.95054e-05 |\n| log-sigma-0-6-mm-3D_gldm_LowGrayLevelEmphasis                 | nan           | nan                                                           | low_variance | 2.94397e-05 |\n| log-sigma-0-6-mm-3D_gldm_SmallDependenceEmphasis              | nan           | nan                                                           | low_variance | 0.00026978  |\n| log-sigma-0-6-mm-3D_gldm_SmallDependenceLowGrayLevelEmphasis  | nan           | nan                                                           | low_variance | 2.9457e-05  |\n| log-sigma-2-mm-3D_firstorder_Uniformity                       | nan           | nan                                                           | low_variance | 2.76545e-05 |\n| log-sigma-2-mm-3D_glcm_Correlation                            | nan           | nan                                                           | low_variance | 0.00242713  |\n| log-sigma-2-mm-3D_glcm_Id                                     | nan           | nan                                                           | low_variance | 0.00135262  |\n| log-sigma-2-mm-3D_glcm_Idm                                    | nan           | nan                                                           | low_variance | 0.000608072 |\n| log-sigma-2-mm-3D_glcm_Idmn                                   | nan           | nan                                                           | low_variance | 2.97318e-05 |\n| log-sigma-2-mm-3D_glcm_Idn                                    | nan           | nan                                                           | low_variance | 0.000415181 |\n| log-sigma-2-mm-3D_glcm_Imc2                                   | nan           | nan                                                           | low_variance | 1.02719e-07 |\n| log-sigma-2-mm-3D_glcm_InverseVariance                        | nan           | nan                                                           | low_variance | 0.000670529 |\n| log-sigma-2-mm-3D_glcm_JointEnergy                            | nan           | nan                                                           | low_variance | 1.44537e-05 |\n| log-sigma-2-mm-3D_glcm_MCC                                    | nan           | nan                                                           | low_variance | 0.000415032 |\n| log-sigma-2-mm-3D_glcm_MaximumProbability                     | nan           | nan                                                           | low_variance | 2.19221e-05 |\n| log-sigma-2-mm-3D_glrlm_GrayLevelNonUniformityNormalized      | nan           | nan                                                           | low_variance | 2.73903e-05 |\n| log-sigma-2-mm-3D_glrlm_LongRunEmphasis                       | nan           | nan                                                           | low_variance | 0.000818012 |\n| log-sigma-2-mm-3D_glrlm_LongRunLowGrayLevelEmphasis           | nan           | nan                                                           | low_variance | 2.97485e-05 |\n| log-sigma-2-mm-3D_glrlm_LowGrayLevelRunEmphasis               | nan           | nan                                                           | low_variance | 2.97133e-05 |\n| log-sigma-2-mm-3D_glrlm_RunLengthNonUniformityNormalized      | nan           | nan                                                           | low_variance | 0.000232058 |\n| log-sigma-2-mm-3D_glrlm_RunPercentage                         | nan           | nan                                                           | low_variance | 7.10669e-05 |\n| log-sigma-2-mm-3D_glrlm_RunVariance                           | nan           | nan                                                           | low_variance | 0.000115715 |\n| log-sigma-2-mm-3D_glrlm_ShortRunEmphasis                      | nan           | nan                                                           | low_variance | 3.62439e-05 |\n| log-sigma-2-mm-3D_glrlm_ShortRunLowGrayLevelEmphasis          | nan           | nan                                                           | low_variance | 2.97115e-05 |\n| log-sigma-2-mm-3D_glszm_GrayLevelNonUniformityNormalized      | nan           | nan                                                           | low_variance | 2.6678e-05  |\n| log-sigma-2-mm-3D_glszm_LargeAreaLowGrayLevelEmphasis         | nan           | nan                                                           | low_variance | 3.06904e-05 |\n| log-sigma-2-mm-3D_glszm_LowGrayLevelZoneEmphasis              | nan           | nan                                                           | low_variance | 3.04301e-05 |\n| log-sigma-2-mm-3D_glszm_SizeZoneNonUniformityNormalized       | nan           | nan                                                           | low_variance | 0.00241571  |\n| log-sigma-2-mm-3D_glszm_SmallAreaEmphasis                     | nan           | nan                                                           | low_variance | 0.000466738 |\n| log-sigma-2-mm-3D_glszm_SmallAreaLowGrayLevelEmphasis         | nan           | nan                                                           | low_variance | 3.04894e-05 |\n| log-sigma-2-mm-3D_glszm_ZonePercentage                        | nan           | nan                                                           | low_variance | 0.000986372 |\n| log-sigma-2-mm-3D_glszm_ZoneVariance                          | nan           | nan                                                           | low_variance | 0.00360284  |\n| log-sigma-2-mm-3D_ngtdm_Busyness                              | nan           | nan                                                           | low_variance | 1.71407e-06 |\n| log-sigma-2-mm-3D_ngtdm_Coarseness                            | nan           | nan                                                           | low_variance | 0.000357776 |\n| log-sigma-2-mm-3D_gldm_DependenceNonUniformityNormalized      | nan           | nan                                                           | low_variance | 0.00733429  |\n| log-sigma-2-mm-3D_gldm_DependenceVariance                     | nan           | nan                                                           | low_variance | 0.00625363  |\n| log-sigma-2-mm-3D_gldm_LargeDependenceLowGrayLevelEmphasis    | nan           | nan                                                           | low_variance | 3.10932e-05 |\n| log-sigma-2-mm-3D_gldm_LowGrayLevelEmphasis                   | nan           | nan                                                           | low_variance | 2.94912e-05 |\n| log-sigma-2-mm-3D_gldm_SmallDependenceEmphasis                | nan           | nan                                                           | low_variance | 0.00182018  |\n| log-sigma-2-mm-3D_gldm_SmallDependenceLowGrayLevelEmphasis    | nan           | nan                                                           | low_variance | 2.95252e-05 |\n| log-sigma-3-mm-3D_firstorder_Uniformity                       | nan           | nan                                                           | low_variance | 3.31044e-05 |\n| log-sigma-3-mm-3D_glcm_Correlation                            | nan           | nan                                                           | low_variance | 0.0020682   |\n| log-sigma-3-mm-3D_glcm_Id                                     | nan           | nan                                                           | low_variance | 0.00181066  |\n| log-sigma-3-mm-3D_glcm_Idm                                    | nan           | nan                                                           | low_variance | 0.000956564 |\n| log-sigma-3-mm-3D_glcm_Idmn                                   | nan           | nan                                                           | low_variance | 2.80525e-05 |\n| log-sigma-3-mm-3D_glcm_Idn                                    | nan           | nan                                                           | low_variance | 0.000383895 |\n| log-sigma-3-mm-3D_glcm_Imc2                                   | nan           | nan                                                           | low_variance | 5.39194e-08 |\n| log-sigma-3-mm-3D_glcm_InverseVariance                        | nan           | nan                                                           | low_variance | 0.00100653  |\n| log-sigma-3-mm-3D_glcm_JointEnergy                            | nan           | nan                                                           | low_variance | 1.45532e-05 |\n| log-sigma-3-mm-3D_glcm_MCC                                    | nan           | nan                                                           | low_variance | 0.000154558 |\n| log-sigma-3-mm-3D_glcm_MaximumProbability                     | nan           | nan                                                           | low_variance | 2.35127e-05 |\n| log-sigma-3-mm-3D_glrlm_GrayLevelNonUniformityNormalized      | nan           | nan                                                           | low_variance | 3.25954e-05 |\n| log-sigma-3-mm-3D_glrlm_LongRunEmphasis                       | nan           | nan                                                           | low_variance | 0.00165107  |\n| log-sigma-3-mm-3D_glrlm_LongRunLowGrayLevelEmphasis           | nan           | nan                                                           | low_variance | 3.07357e-05 |\n| log-sigma-3-mm-3D_glrlm_LowGrayLevelRunEmphasis               | nan           | nan                                                           | low_variance | 3.03571e-05 |\n| log-sigma-3-mm-3D_glrlm_RunLengthNonUniformityNormalized      | nan           | nan                                                           | low_variance | 0.000332294 |\n| log-sigma-3-mm-3D_glrlm_RunPercentage                         | nan           | nan                                                           | low_variance | 0.000118625 |\n| log-sigma-3-mm-3D_glrlm_RunVariance                           | nan           | nan                                                           | low_variance | 0.000292741 |\n| log-sigma-3-mm-3D_glrlm_ShortRunEmphasis                      | nan           | nan                                                           | low_variance | 5.41795e-05 |\n| log-sigma-3-mm-3D_glrlm_ShortRunLowGrayLevelEmphasis          | nan           | nan                                                           | low_variance | 3.03161e-05 |\n| log-sigma-3-mm-3D_glszm_GrayLevelNonUniformityNormalized      | nan           | nan                                                           | low_variance | 3.12171e-05 |\n| log-sigma-3-mm-3D_glszm_LargeAreaLowGrayLevelEmphasis         | nan           | nan                                                           | low_variance | 3.53235e-05 |\n| log-sigma-3-mm-3D_glszm_LowGrayLevelZoneEmphasis              | nan           | nan                                                           | low_variance | 3.13406e-05 |\n| log-sigma-3-mm-3D_glszm_SizeZoneNonUniformityNormalized       | nan           | nan                                                           | low_variance | 0.00321037  |\n| log-sigma-3-mm-3D_glszm_SmallAreaEmphasis                     | nan           | nan                                                           | low_variance | 0.000667295 |\n| log-sigma-3-mm-3D_glszm_SmallAreaLowGrayLevelEmphasis         | nan           | nan                                                           | low_variance | 3.1455e-05  |\n| log-sigma-3-mm-3D_glszm_ZonePercentage                        | nan           | nan                                                           | low_variance | 0.00159567  |\n| log-sigma-3-mm-3D_ngtdm_Busyness                              | nan           | nan                                                           | low_variance | 1.2633e-06  |\n| log-sigma-3-mm-3D_ngtdm_Coarseness                            | nan           | nan                                                           | low_variance | 0.00103614  |\n| log-sigma-3-mm-3D_gldm_DependenceNonUniformityNormalized      | nan           | nan                                                           | low_variance | 0.00938007  |\n| log-sigma-3-mm-3D_gldm_LargeDependenceLowGrayLevelEmphasis    | nan           | nan                                                           | low_variance | 4.04724e-05 |\n| log-sigma-3-mm-3D_gldm_LowGrayLevelEmphasis                   | nan           | nan                                                           | low_variance | 3.00892e-05 |\n| log-sigma-3-mm-3D_gldm_SmallDependenceEmphasis                | nan           | nan                                                           | low_variance | 0.00268848  |\n| log-sigma-3-mm-3D_gldm_SmallDependenceLowGrayLevelEmphasis    | nan           | nan                                                           | low_variance | 2.99896e-05 |\n| wavelet-LH_firstorder_Uniformity                              | nan           | nan                                                           | low_variance | 2.56502e-05 |\n| wavelet-LH_glcm_Correlation                                   | nan           | nan                                                           | low_variance | 0.00910022  |\n| wavelet-LH_glcm_Id                                            | nan           | nan                                                           | low_variance | 4.98769e-05 |\n| wavelet-LH_glcm_Idm                                           | nan           | nan                                                           | low_variance | 1.22033e-05 |\n| wavelet-LH_glcm_Idmn                                          | nan           | nan                                                           | low_variance | 0.000215484 |\n| wavelet-LH_glcm_Idn                                           | nan           | nan                                                           | low_variance | 0.00059418  |\n| wavelet-LH_glcm_Imc2                                          | nan           | nan                                                           | low_variance | 6.82321e-08 |\n| wavelet-LH_glcm_InverseVariance                               | nan           | nan                                                           | low_variance | 1.45237e-05 |\n| wavelet-LH_glcm_JointEnergy                                   | nan           | nan                                                           | low_variance | 1.42988e-05 |\n| wavelet-LH_glcm_MCC                                           | nan           | nan                                                           | low_variance | 0.00814855  |\n| wavelet-LH_glcm_MaximumProbability                            | nan           | nan                                                           | low_variance | 1.65314e-05 |\n| wavelet-LH_glrlm_GrayLevelNonUniformityNormalized             | nan           | nan                                                           | low_variance | 2.56167e-05 |\n| wavelet-LH_glrlm_LongRunEmphasis                              | nan           | nan                                                           | low_variance | 2.56302e-05 |\n| wavelet-LH_glrlm_LongRunLowGrayLevelEmphasis                  | nan           | nan                                                           | low_variance | 4.14429e-05 |\n| wavelet-LH_glrlm_LowGrayLevelRunEmphasis                      | nan           | nan                                                           | low_variance | 4.14493e-05 |\n| wavelet-LH_glrlm_RunLengthNonUniformityNormalized             | nan           | nan                                                           | low_variance | 1.10301e-05 |\n| wavelet-LH_glrlm_RunPercentage                                | nan           | nan                                                           | low_variance | 2.77484e-06 |\n| wavelet-LH_glrlm_RunVariance                                  | nan           | nan                                                           | low_variance | 2.82627e-06 |\n| wavelet-LH_glrlm_ShortRunEmphasis                             | nan           | nan                                                           | low_variance | 1.58563e-06 |\n| wavelet-LH_glrlm_ShortRunLowGrayLevelEmphasis                 | nan           | nan                                                           | low_variance | 4.14517e-05 |\n| wavelet-LH_glszm_GrayLevelNonUniformityNormalized             | nan           | nan                                                           | low_variance | 2.55072e-05 |\n| wavelet-LH_glszm_LargeAreaEmphasis                            | nan           | nan                                                           | low_variance | 0.000436533 |\n| wavelet-LH_glszm_LargeAreaLowGrayLevelEmphasis                | nan           | nan                                                           | low_variance | 4.16565e-05 |\n| wavelet-LH_glszm_LowGrayLevelZoneEmphasis                     | nan           | nan                                                           | low_variance | 4.16719e-05 |\n| wavelet-LH_glszm_SizeZoneNonUniformityNormalized              | nan           | nan                                                           | low_variance | 0.000168799 |\n| wavelet-LH_glszm_SmallAreaEmphasis                            | nan           | nan                                                           | low_variance | 2.53846e-05 |\n| wavelet-LH_glszm_SmallAreaLowGrayLevelEmphasis                | nan           | nan                                                           | low_variance | 4.16885e-05 |\n| wavelet-LH_glszm_ZonePercentage                               | nan           | nan                                                           | low_variance | 4.40947e-05 |\n| wavelet-LH_glszm_ZoneVariance                                 | nan           | nan                                                           | low_variance | 4.97119e-05 |\n| wavelet-LH_ngtdm_Busyness                                     | nan           | nan                                                           | low_variance | 1.3472e-05  |\n| wavelet-LH_ngtdm_Coarseness                                   | nan           | nan                                                           | low_variance | 1.11878e-05 |\n| wavelet-LH_gldm_DependenceNonUniformityNormalized             | nan           | nan                                                           | low_variance | 0.000612588 |\n| wavelet-LH_gldm_DependenceVariance                            | nan           | nan                                                           | low_variance | 0.00017145  |\n| wavelet-LH_gldm_LargeDependenceEmphasis                       | nan           | nan                                                           | low_variance | 0.0016371   |\n| wavelet-LH_gldm_LargeDependenceLowGrayLevelEmphasis           | nan           | nan                                                           | low_variance | 4.14786e-05 |\n| wavelet-LH_gldm_LowGrayLevelEmphasis                          | nan           | nan                                                           | low_variance | 4.13791e-05 |\n| wavelet-LH_gldm_SmallDependenceEmphasis                       | nan           | nan                                                           | low_variance | 9.74203e-05 |\n| wavelet-LH_gldm_SmallDependenceLowGrayLevelEmphasis           | nan           | nan                                                           | low_variance | 4.14034e-05 |\n| wavelet-HL_firstorder_Uniformity                              | nan           | nan                                                           | low_variance | 2.71733e-05 |\n| wavelet-HL_glcm_Correlation                                   | nan           | nan                                                           | low_variance | 0.00719563  |\n| wavelet-HL_glcm_Id                                            | nan           | nan                                                           | low_variance | 4.45406e-05 |\n| wavelet-HL_glcm_Idm                                           | nan           | nan                                                           | low_variance | 1.05801e-05 |\n| wavelet-HL_glcm_Idmn                                          | nan           | nan                                                           | low_variance | 0.000253239 |\n| wavelet-HL_glcm_Idn                                           | nan           | nan                                                           | low_variance | 0.000696643 |\n| wavelet-HL_glcm_Imc2                                          | nan           | nan                                                           | low_variance | 3.2957e-08  |\n| wavelet-HL_glcm_InverseVariance                               | nan           | nan                                                           | low_variance | 1.32836e-05 |\n| wavelet-HL_glcm_JointEnergy                                   | nan           | nan                                                           | low_variance | 1.43713e-05 |\n| wavelet-HL_glcm_MCC                                           | nan           | nan                                                           | low_variance | 0.00648006  |\n| wavelet-HL_glcm_MaximumProbability                            | nan           | nan                                                           | low_variance | 1.79504e-05 |\n| wavelet-HL_glrlm_GrayLevelNonUniformityNormalized             | nan           | nan                                                           | low_variance | 2.70331e-05 |\n| wavelet-HL_glrlm_LongRunEmphasis                              | nan           | nan                                                           | low_variance | 2.03674e-05 |\n| wavelet-HL_glrlm_LongRunLowGrayLevelEmphasis                  | nan           | nan                                                           | low_variance | 3.13937e-05 |\n| wavelet-HL_glrlm_LowGrayLevelRunEmphasis                      | nan           | nan                                                           | low_variance | 3.13932e-05 |\n| wavelet-HL_glrlm_RunLengthNonUniformityNormalized             | nan           | nan                                                           | low_variance | 8.62028e-06 |\n| wavelet-HL_glrlm_RunPercentage                                | nan           | nan                                                           | low_variance | 2.18208e-06 |\n| wavelet-HL_glrlm_RunVariance                                  | nan           | nan                                                           | low_variance | 2.30669e-06 |\n| wavelet-HL_glrlm_ShortRunEmphasis                             | nan           | nan                                                           | low_variance | 1.24197e-06 |\n| wavelet-HL_glrlm_ShortRunLowGrayLevelEmphasis                 | nan           | nan                                                           | low_variance | 3.13931e-05 |\n| wavelet-HL_glszm_GrayLevelNonUniformityNormalized             | nan           | nan                                                           | low_variance | 2.66189e-05 |\n| wavelet-HL_glszm_LargeAreaEmphasis                            | nan           | nan                                                           | low_variance | 0.000346378 |\n| wavelet-HL_glszm_LargeAreaLowGrayLevelEmphasis                | nan           | nan                                                           | low_variance | 3.19114e-05 |\n| wavelet-HL_glszm_LowGrayLevelZoneEmphasis                     | nan           | nan                                                           | low_variance | 3.19097e-05 |\n| wavelet-HL_glszm_SizeZoneNonUniformityNormalized              | nan           | nan                                                           | low_variance | 0.000130859 |\n| wavelet-HL_glszm_SmallAreaEmphasis                            | nan           | nan                                                           | low_variance | 1.95718e-05 |\n| wavelet-HL_glszm_SmallAreaLowGrayLevelEmphasis                | nan           | nan                                                           | low_variance | 3.19092e-05 |\n| wavelet-HL_glszm_ZonePercentage                               | nan           | nan                                                           | low_variance | 3.45931e-05 |\n| wavelet-HL_glszm_ZoneVariance                                 | nan           | nan                                                           | low_variance | 4.17176e-05 |\n| wavelet-HL_ngtdm_Busyness                                     | nan           | nan                                                           | low_variance | 1.2203e-05  |\n| wavelet-HL_ngtdm_Coarseness                                   | nan           | nan                                                           | low_variance | 9.15905e-06 |\n| wavelet-HL_gldm_DependenceNonUniformityNormalized             | nan           | nan                                                           | low_variance | 0.000482061 |\n| wavelet-HL_gldm_DependenceVariance                            | nan           | nan                                                           | low_variance | 0.0001408   |\n| wavelet-HL_gldm_LargeDependenceEmphasis                       | nan           | nan                                                           | low_variance | 0.00129939  |\n| wavelet-HL_gldm_LargeDependenceLowGrayLevelEmphasis           | nan           | nan                                                           | low_variance | 3.12268e-05 |\n| wavelet-HL_gldm_LowGrayLevelEmphasis                          | nan           | nan                                                           | low_variance | 3.12234e-05 |\n| wavelet-HL_gldm_SmallDependenceEmphasis                       | nan           | nan                                                           | low_variance | 7.59249e-05 |\n| wavelet-HL_gldm_SmallDependenceLowGrayLevelEmphasis           | nan           | nan                                                           | low_variance | 3.12226e-05 |\n| wavelet-HH_firstorder_Uniformity                              | nan           | nan                                                           | low_variance | 2.69264e-05 |\n| wavelet-HH_glcm_Correlation                                   | nan           | nan                                                           | low_variance | 0.000365748 |\n| wavelet-HH_glcm_Id                                            | nan           | nan                                                           | low_variance | 6.53506e-05 |\n| wavelet-HH_glcm_Idm                                           | nan           | nan                                                           | low_variance | 1.59685e-05 |\n| wavelet-HH_glcm_Idmn                                          | nan           | nan                                                           | low_variance | 0.000242874 |\n| wavelet-HH_glcm_Idn                                           | nan           | nan                                                           | low_variance | 0.000479042 |\n| wavelet-HH_glcm_Imc2                                          | nan           | nan                                                           | low_variance | 1.37818e-06 |\n| wavelet-HH_glcm_InverseVariance                               | nan           | nan                                                           | low_variance | 2.14244e-05 |\n| wavelet-HH_glcm_JointEnergy                                   | nan           | nan                                                           | low_variance | 1.43297e-05 |\n| wavelet-HH_glcm_MaximumProbability                            | nan           | nan                                                           | low_variance | 1.74457e-05 |\n| wavelet-HH_glrlm_GrayLevelNonUniformityNormalized             | nan           | nan                                                           | low_variance | 2.68602e-05 |\n| wavelet-HH_glrlm_LongRunEmphasis                              | nan           | nan                                                           | low_variance | 2.41388e-05 |\n| wavelet-HH_glrlm_LongRunLowGrayLevelEmphasis                  | nan           | nan                                                           | low_variance | 3.17219e-05 |\n| wavelet-HH_glrlm_LowGrayLevelRunEmphasis                      | nan           | nan                                                           | low_variance | 3.17219e-05 |\n| wavelet-HH_glrlm_RunLengthNonUniformityNormalized             | nan           | nan                                                           | low_variance | 1.02972e-05 |\n| wavelet-HH_glrlm_RunPercentage                                | nan           | nan                                                           | low_variance | 2.60773e-06 |\n| wavelet-HH_glrlm_RunVariance                                  | nan           | nan                                                           | low_variance | 2.69608e-06 |\n| wavelet-HH_glrlm_ShortRunEmphasis                             | nan           | nan                                                           | low_variance | 1.478e-06   |\n| wavelet-HH_glrlm_ShortRunLowGrayLevelEmphasis                 | nan           | nan                                                           | low_variance | 3.1722e-05  |\n| wavelet-HH_glszm_GrayLevelNonUniformityNormalized             | nan           | nan                                                           | low_variance | 2.66648e-05 |\n| wavelet-HH_glszm_LargeAreaEmphasis                            | nan           | nan                                                           | low_variance | 0.000425153 |\n| wavelet-HH_glszm_LargeAreaLowGrayLevelEmphasis                | nan           | nan                                                           | low_variance | 3.19381e-05 |\n| wavelet-HH_glszm_LowGrayLevelZoneEmphasis                     | nan           | nan                                                           | low_variance | 3.19384e-05 |\n| wavelet-HH_glszm_SizeZoneNonUniformityNormalized              | nan           | nan                                                           | low_variance | 0.000152234 |\n| wavelet-HH_glszm_SmallAreaEmphasis                            | nan           | nan                                                           | low_variance | 2.30072e-05 |\n| wavelet-HH_glszm_SmallAreaLowGrayLevelEmphasis                | nan           | nan                                                           | low_variance | 3.19385e-05 |\n| wavelet-HH_glszm_ZonePercentage                               | nan           | nan                                                           | low_variance | 4.11976e-05 |\n| wavelet-HH_glszm_ZoneVariance                                 | nan           | nan                                                           | low_variance | 5.34559e-05 |\n| wavelet-HH_ngtdm_Busyness                                     | nan           | nan                                                           | low_variance | 2.93521e-05 |\n| wavelet-HH_ngtdm_Coarseness                                   | nan           | nan                                                           | low_variance | 1.56764e-05 |\n| wavelet-HH_gldm_DependenceNonUniformityNormalized             | nan           | nan                                                           | low_variance | 0.000559587 |\n| wavelet-HH_gldm_DependenceVariance                            | nan           | nan                                                           | low_variance | 0.000170925 |\n| wavelet-HH_gldm_LargeDependenceEmphasis                       | nan           | nan                                                           | low_variance | 0.00156905  |\n| wavelet-HH_gldm_LargeDependenceLowGrayLevelEmphasis           | nan           | nan                                                           | low_variance | 3.16501e-05 |\n| wavelet-HH_gldm_LowGrayLevelEmphasis                          | nan           | nan                                                           | low_variance | 3.16509e-05 |\n| wavelet-HH_gldm_SmallDependenceEmphasis                       | nan           | nan                                                           | low_variance | 8.95841e-05 |\n| wavelet-HH_gldm_SmallDependenceLowGrayLevelEmphasis           | nan           | nan                                                           | low_variance | 3.16511e-05 |\n| wavelet2-LH_firstorder_Uniformity                             | nan           | nan                                                           | low_variance | 2.55767e-05 |\n| wavelet2-LH_glcm_Correlation                                  | nan           | nan                                                           | low_variance | 0.0053787   |\n| wavelet2-LH_glcm_Id                                           | nan           | nan                                                           | low_variance | 7.79228e-05 |\n| wavelet2-LH_glcm_Idm                                          | nan           | nan                                                           | low_variance | 1.7667e-05  |\n| wavelet2-LH_glcm_Idmn                                         | nan           | nan                                                           | low_variance | 0.000120089 |\n| wavelet2-LH_glcm_Idn                                          | nan           | nan                                                           | low_variance | 0.000537699 |\n| wavelet2-LH_glcm_Imc2                                         | nan           | nan                                                           | low_variance | 7.86953e-08 |\n| wavelet2-LH_glcm_InverseVariance                              | nan           | nan                                                           | low_variance | 2.49003e-05 |\n| wavelet2-LH_glcm_JointEnergy                                  | nan           | nan                                                           | low_variance | 1.43815e-05 |\n| wavelet2-LH_glcm_MCC                                          | nan           | nan                                                           | low_variance | 0.00424353  |\n| wavelet2-LH_glcm_MaximumProbability                           | nan           | nan                                                           | low_variance | 1.88732e-05 |\n| wavelet2-LH_glrlm_GrayLevelNonUniformityNormalized            | nan           | nan                                                           | low_variance | 2.54768e-05 |\n| wavelet2-LH_glrlm_LongRunEmphasis                             | nan           | nan                                                           | low_variance | 3.16026e-05 |\n| wavelet2-LH_glrlm_LongRunLowGrayLevelEmphasis                 | nan           | nan                                                           | low_variance | 3.02557e-05 |\n| wavelet2-LH_glrlm_LowGrayLevelRunEmphasis                     | nan           | nan                                                           | low_variance | 3.02595e-05 |\n| wavelet2-LH_glrlm_RunLengthNonUniformityNormalized            | nan           | nan                                                           | low_variance | 1.33372e-05 |\n| wavelet2-LH_glrlm_RunPercentage                               | nan           | nan                                                           | low_variance | 3.38628e-06 |\n| wavelet2-LH_glrlm_RunVariance                                 | nan           | nan                                                           | low_variance | 3.55819e-06 |\n| wavelet2-LH_glrlm_ShortRunEmphasis                            | nan           | nan                                                           | low_variance | 1.92227e-06 |\n| wavelet2-LH_glrlm_ShortRunLowGrayLevelEmphasis                | nan           | nan                                                           | low_variance | 3.02613e-05 |\n| wavelet2-LH_glszm_GrayLevelNonUniformityNormalized            | nan           | nan                                                           | low_variance | 2.51765e-05 |\n| wavelet2-LH_glszm_LargeAreaEmphasis                           | nan           | nan                                                           | low_variance | 0.000543846 |\n| wavelet2-LH_glszm_LargeAreaLowGrayLevelEmphasis               | nan           | nan                                                           | low_variance | 3.07006e-05 |\n| wavelet2-LH_glszm_LowGrayLevelZoneEmphasis                    | nan           | nan                                                           | low_variance | 3.07043e-05 |\n| wavelet2-LH_glszm_SizeZoneNonUniformityNormalized             | nan           | nan                                                           | low_variance | 0.000196868 |\n| wavelet2-LH_glszm_SmallAreaEmphasis                           | nan           | nan                                                           | low_variance | 2.97531e-05 |\n| wavelet2-LH_glszm_SmallAreaLowGrayLevelEmphasis               | nan           | nan                                                           | low_variance | 3.07193e-05 |\n| wavelet2-LH_glszm_ZonePercentage                              | nan           | nan                                                           | low_variance | 5.32012e-05 |\n| wavelet2-LH_glszm_ZoneVariance                                | nan           | nan                                                           | low_variance | 6.63252e-05 |\n| wavelet2-LH_ngtdm_Busyness                                    | nan           | nan                                                           | low_variance | 7.49579e-06 |\n| wavelet2-LH_ngtdm_Coarseness                                  | nan           | nan                                                           | low_variance | 1.58637e-05 |\n| wavelet2-LH_gldm_DependenceNonUniformityNormalized            | nan           | nan                                                           | low_variance | 0.000725016 |\n| wavelet2-LH_gldm_DependenceVariance                           | nan           | nan                                                           | low_variance | 0.000222092 |\n| wavelet2-LH_gldm_LargeDependenceEmphasis                      | nan           | nan                                                           | low_variance | 0.00203857  |\n| wavelet2-LH_gldm_LargeDependenceLowGrayLevelEmphasis          | nan           | nan                                                           | low_variance | 3.02517e-05 |\n| wavelet2-LH_gldm_LowGrayLevelEmphasis                         | nan           | nan                                                           | low_variance | 3.01156e-05 |\n| wavelet2-LH_gldm_SmallDependenceEmphasis                      | nan           | nan                                                           | low_variance | 0.000116013 |\n| wavelet2-LH_gldm_SmallDependenceLowGrayLevelEmphasis          | nan           | nan                                                           | low_variance | 3.01358e-05 |\n| wavelet2-HL_firstorder_Uniformity                             | nan           | nan                                                           | low_variance | 2.59872e-05 |\n| wavelet2-HL_glcm_Correlation                                  | nan           | nan                                                           | low_variance | 0.00446822  |\n| wavelet2-HL_glcm_Id                                           | nan           | nan                                                           | low_variance | 8.08491e-05 |\n| wavelet2-HL_glcm_Idm                                          | nan           | nan                                                           | low_variance | 2.16586e-05 |\n| wavelet2-HL_glcm_Idmn                                         | nan           | nan                                                           | low_variance | 0.000131901 |\n| wavelet2-HL_glcm_Idn                                          | nan           | nan                                                           | low_variance | 0.000586409 |\n| wavelet2-HL_glcm_Imc2                                         | nan           | nan                                                           | low_variance | 5.42211e-08 |\n| wavelet2-HL_glcm_InverseVariance                              | nan           | nan                                                           | low_variance | 1.97388e-05 |\n| wavelet2-HL_glcm_JointEnergy                                  | nan           | nan                                                           | low_variance | 1.44326e-05 |\n| wavelet2-HL_glcm_MCC                                          | nan           | nan                                                           | low_variance | 0.0036526   |\n| wavelet2-HL_glcm_MaximumProbability                           | nan           | nan                                                           | low_variance | 1.97012e-05 |\n| wavelet2-HL_glrlm_GrayLevelNonUniformityNormalized            | nan           | nan                                                           | low_variance | 2.58159e-05 |\n| wavelet2-HL_glrlm_LongRunEmphasis                             | nan           | nan                                                           | low_variance | 4.59903e-05 |\n| wavelet2-HL_glrlm_LongRunLowGrayLevelEmphasis                 | nan           | nan                                                           | low_variance | 3.13897e-05 |\n| wavelet2-HL_glrlm_LowGrayLevelRunEmphasis                     | nan           | nan                                                           | low_variance | 3.13907e-05 |\n| wavelet2-HL_glrlm_RunLengthNonUniformityNormalized            | nan           | nan                                                           | low_variance | 1.90113e-05 |\n| wavelet2-HL_glrlm_RunPercentage                               | nan           | nan                                                           | low_variance | 4.79273e-06 |\n| wavelet2-HL_glrlm_RunVariance                                 | nan           | nan                                                           | low_variance | 4.93709e-06 |\n| wavelet2-HL_glrlm_ShortRunEmphasis                            | nan           | nan                                                           | low_variance | 2.83875e-06 |\n| wavelet2-HL_glrlm_ShortRunLowGrayLevelEmphasis                | nan           | nan                                                           | low_variance | 3.13915e-05 |\n| wavelet2-HL_glszm_GrayLevelNonUniformityNormalized            | nan           | nan                                                           | low_variance | 2.52768e-05 |\n| wavelet2-HL_glszm_LargeAreaEmphasis                           | nan           | nan                                                           | low_variance | 0.000791751 |\n| wavelet2-HL_glszm_LargeAreaLowGrayLevelEmphasis               | nan           | nan                                                           | low_variance | 3.22396e-05 |\n| wavelet2-HL_glszm_LowGrayLevelZoneEmphasis                    | nan           | nan                                                           | low_variance | 3.22333e-05 |\n| wavelet2-HL_glszm_SizeZoneNonUniformityNormalized             | nan           | nan                                                           | low_variance | 0.00029254  |\n| wavelet2-HL_glszm_SmallAreaEmphasis                           | nan           | nan                                                           | low_variance | 4.66672e-05 |\n| wavelet2-HL_glszm_SmallAreaLowGrayLevelEmphasis               | nan           | nan                                                           | low_variance | 3.22415e-05 |\n| wavelet2-HL_glszm_ZonePercentage                              | nan           | nan                                                           | low_variance | 7.61094e-05 |\n| wavelet2-HL_glszm_ZoneVariance                                | nan           | nan                                                           | low_variance | 8.57192e-05 |\n| wavelet2-HL_ngtdm_Busyness                                    | nan           | nan                                                           | low_variance | 5.22195e-06 |\n| wavelet2-HL_ngtdm_Coarseness                                  | nan           | nan                                                           | low_variance | 1.65933e-05 |\n| wavelet2-HL_gldm_DependenceNonUniformityNormalized            | nan           | nan                                                           | low_variance | 0.00095685  |\n| wavelet2-HL_gldm_DependenceVariance                           | nan           | nan                                                           | low_variance | 0.000271595 |\n| wavelet2-HL_gldm_LargeDependenceEmphasis                      | nan           | nan                                                           | low_variance | 0.00282739  |\n| wavelet2-HL_gldm_LargeDependenceLowGrayLevelEmphasis          | nan           | nan                                                           | low_variance | 3.12295e-05 |\n| wavelet2-HL_gldm_LowGrayLevelEmphasis                         | nan           | nan                                                           | low_variance | 3.11234e-05 |\n| wavelet2-HL_gldm_SmallDependenceEmphasis                      | nan           | nan                                                           | low_variance | 0.000168679 |\n| wavelet2-HL_gldm_SmallDependenceLowGrayLevelEmphasis          | nan           | nan                                                           | low_variance | 3.11347e-05 |\n| wavelet2-HH_firstorder_Uniformity                             | nan           | nan                                                           | low_variance | 2.74704e-05 |\n| wavelet2-HH_glcm_Correlation                                  | nan           | nan                                                           | low_variance | 0.00151591  |\n| wavelet2-HH_glcm_Id                                           | nan           | nan                                                           | low_variance | 0.000170165 |\n| wavelet2-HH_glcm_Idm                                          | nan           | nan                                                           | low_variance | 5.55612e-05 |\n| wavelet2-HH_glcm_Idmn                                         | nan           | nan                                                           | low_variance | 0.000185899 |\n| wavelet2-HH_glcm_Idn                                          | nan           | nan                                                           | low_variance | 0.000520348 |\n| wavelet2-HH_glcm_Imc2                                         | nan           | nan                                                           | low_variance | 0.000137537 |\n| wavelet2-HH_glcm_InverseVariance                              | nan           | nan                                                           | low_variance | 6.15363e-05 |\n| wavelet2-HH_glcm_JointEnergy                                  | nan           | nan                                                           | low_variance | 1.46428e-05 |\n| wavelet2-HH_glcm_MaximumProbability                           | nan           | nan                                                           | low_variance | 2.48364e-05 |\n| wavelet2-HH_glrlm_GrayLevelNonUniformityNormalized            | nan           | nan                                                           | low_variance | 2.71137e-05 |\n| wavelet2-HH_glrlm_LongRunEmphasis                             | nan           | nan                                                           | low_variance | 8.59113e-05 |\n| wavelet2-HH_glrlm_LongRunLowGrayLevelEmphasis                 | nan           | nan                                                           | low_variance | 3.06975e-05 |\n| wavelet2-HH_glrlm_LowGrayLevelRunEmphasis                     | nan           | nan                                                           | low_variance | 3.06879e-05 |\n| wavelet2-HH_glrlm_RunLengthNonUniformityNormalized            | nan           | nan                                                           | low_variance | 3.49906e-05 |\n| wavelet2-HH_glrlm_RunPercentage                               | nan           | nan                                                           | low_variance | 8.99172e-06 |\n| wavelet2-HH_glrlm_RunVariance                                 | nan           | nan                                                           | low_variance | 9.75541e-06 |\n| wavelet2-HH_glrlm_ShortRunEmphasis                            | nan           | nan                                                           | low_variance | 5.13395e-06 |\n| wavelet2-HH_glrlm_ShortRunLowGrayLevelEmphasis                | nan           | nan                                                           | low_variance | 3.06855e-05 |\n| wavelet2-HH_glszm_GrayLevelNonUniformityNormalized            | nan           | nan                                                           | low_variance | 2.60465e-05 |\n| wavelet2-HH_glszm_LargeAreaEmphasis                           | nan           | nan                                                           | low_variance | 0.00156738  |\n| wavelet2-HH_glszm_LargeAreaLowGrayLevelEmphasis               | nan           | nan                                                           | low_variance | 3.16061e-05 |\n| wavelet2-HH_glszm_LowGrayLevelZoneEmphasis                    | nan           | nan                                                           | low_variance | 3.15653e-05 |\n| wavelet2-HH_glszm_SizeZoneNonUniformityNormalized             | nan           | nan                                                           | low_variance | 0.000480327 |\n| wavelet2-HH_glszm_SmallAreaEmphasis                           | nan           | nan                                                           | low_variance | 7.85727e-05 |\n| wavelet2-HH_glszm_SmallAreaLowGrayLevelEmphasis               | nan           | nan                                                           | low_variance | 3.15546e-05 |\n| wavelet2-HH_glszm_ZonePercentage                              | nan           | nan                                                           | low_variance | 0.000136481 |\n| wavelet2-HH_glszm_ZoneVariance                                | nan           | nan                                                           | low_variance | 0.000196885 |\n| wavelet2-HH_ngtdm_Busyness                                    | nan           | nan                                                           | low_variance | 6.24778e-05 |\n| wavelet2-HH_ngtdm_Coarseness                                  | nan           | nan                                                           | low_variance | 6.28459e-05 |\n| wavelet2-HH_gldm_DependenceNonUniformityNormalized            | nan           | nan                                                           | low_variance | 0.00157112  |\n| wavelet2-HH_gldm_DependenceVariance                           | nan           | nan                                                           | low_variance | 0.000599095 |\n| wavelet2-HH_gldm_LargeDependenceEmphasis                      | nan           | nan                                                           | low_variance | 0.00567588  |\n| wavelet2-HH_gldm_LargeDependenceLowGrayLevelEmphasis          | nan           | nan                                                           | low_variance | 3.0481e-05  |\n| wavelet2-HH_gldm_LowGrayLevelEmphasis                         | nan           | nan                                                           | low_variance | 3.04096e-05 |\n| wavelet2-HH_gldm_SmallDependenceEmphasis                      | nan           | nan                                                           | low_variance | 0.000292346 |\n| wavelet2-HH_gldm_SmallDependenceLowGrayLevelEmphasis          | nan           | nan                                                           | low_variance | 3.03911e-05 |\n| wavelet2-LL_firstorder_Uniformity                             | nan           | nan                                                           | low_variance | 2.77593e-05 |\n| wavelet2-LL_glcm_Correlation                                  | nan           | nan                                                           | low_variance | 0.00850074  |\n| wavelet2-LL_glcm_Id                                           | nan           | nan                                                           | low_variance | 3.32777e-05 |\n| wavelet2-LL_glcm_Idm                                          | nan           | nan                                                           | low_variance | 6.13567e-06 |\n| wavelet2-LL_glcm_Idmn                                         | nan           | nan                                                           | low_variance | 6.87283e-05 |\n| wavelet2-LL_glcm_Idn                                          | nan           | nan                                                           | low_variance | 0.000485492 |\n| wavelet2-LL_glcm_Imc1                                         | nan           | nan                                                           | low_variance | 0.00275889  |\n| wavelet2-LL_glcm_Imc2                                         | nan           | nan                                                           | low_variance | 7.72115e-10 |\n| wavelet2-LL_glcm_InverseVariance                              | nan           | nan                                                           | low_variance | 6.78288e-06 |\n| wavelet2-LL_glcm_JointEnergy                                  | nan           | nan                                                           | low_variance | 1.43097e-05 |\n| wavelet2-LL_glcm_MCC                                          | nan           | nan                                                           | low_variance | 4.51093e-05 |\n| wavelet2-LL_glcm_MaximumProbability                           | nan           | nan                                                           | low_variance | 1.60648e-05 |\n| wavelet2-LL_glrlm_GrayLevelNonUniformityNormalized            | nan           | nan                                                           | low_variance | 2.77296e-05 |\n| wavelet2-LL_glrlm_LongRunEmphasis                             | nan           | nan                                                           | low_variance | 1.45199e-05 |\n| wavelet2-LL_glrlm_LongRunLowGrayLevelEmphasis                 | nan           | nan                                                           | low_variance | 2.84937e-05 |\n| wavelet2-LL_glrlm_LowGrayLevelRunEmphasis                     | nan           | nan                                                           | low_variance | 2.84983e-05 |\n| wavelet2-LL_glrlm_RunLengthNonUniformityNormalized            | nan           | nan                                                           | low_variance | 6.30372e-06 |\n| wavelet2-LL_glrlm_RunPercentage                               | nan           | nan                                                           | low_variance | 1.5827e-06  |\n| wavelet2-LL_glrlm_RunVariance                                 | nan           | nan                                                           | low_variance | 1.60012e-06 |\n| wavelet2-LL_glrlm_ShortRunEmphasis                            | nan           | nan                                                           | low_variance | 9.01459e-07 |\n| wavelet2-LL_glrlm_ShortRunLowGrayLevelEmphasis                | nan           | nan                                                           | low_variance | 2.85003e-05 |\n| wavelet2-LL_glszm_GrayLevelNonUniformityNormalized            | nan           | nan                                                           | low_variance | 2.7637e-05  |\n| wavelet2-LL_glszm_LargeAreaEmphasis                           | nan           | nan                                                           | low_variance | 0.000239922 |\n| wavelet2-LL_glszm_LargeAreaLowGrayLevelEmphasis               | nan           | nan                                                           | low_variance | 2.86114e-05 |\n| wavelet2-LL_glszm_LowGrayLevelZoneEmphasis                    | nan           | nan                                                           | low_variance | 2.86183e-05 |\n| wavelet2-LL_glszm_SizeZoneNonUniformityNormalized             | nan           | nan                                                           | low_variance | 9.40342e-05 |\n| wavelet2-LL_glszm_SmallAreaEmphasis                           | nan           | nan                                                           | low_variance | 1.39152e-05 |\n| wavelet2-LL_glszm_SmallAreaLowGrayLevelEmphasis               | nan           | nan                                                           | low_variance | 2.86344e-05 |\n| wavelet2-LL_glszm_ZonePercentage                              | nan           | nan                                                           | low_variance | 2.46375e-05 |\n| wavelet2-LL_glszm_ZoneVariance                                | nan           | nan                                                           | low_variance | 2.77749e-05 |\n| wavelet2-LL_ngtdm_Busyness                                    | nan           | nan                                                           | low_variance | 6.23928e-06 |\n| wavelet2-LL_ngtdm_Coarseness                                  | nan           | nan                                                           | low_variance | 3.99314e-06 |\n| wavelet2-LL_gldm_DependenceNonUniformityNormalized            | nan           | nan                                                           | low_variance | 0.000353954 |\n| wavelet2-LL_gldm_DependenceVariance                           | nan           | nan                                                           | low_variance | 0.000106382 |\n| wavelet2-LL_gldm_LargeDependenceEmphasis                      | nan           | nan                                                           | low_variance | 0.000948388 |\n| wavelet2-LL_gldm_LargeDependenceLowGrayLevelEmphasis          | nan           | nan                                                           | low_variance | 2.8597e-05  |\n| wavelet2-LL_gldm_LowGrayLevelEmphasis                         | nan           | nan                                                           | low_variance | 2.84606e-05 |\n| wavelet2-LL_gldm_SmallDependenceEmphasis                      | nan           | nan                                                           | low_variance | 5.45743e-05 |\n| wavelet2-LL_gldm_SmallDependenceLowGrayLevelEmphasis          | nan           | nan                                                           | low_variance | 2.8483e-05  |\n| square_firstorder_Minimum                                     | nan           | nan                                                           | low_variance | 0.00640253  |\n| square_firstorder_Uniformity                                  | nan           | nan                                                           | low_variance | 0.00101729  |\n| square_glcm_Id                                                | nan           | nan                                                           | low_variance | 0.00492405  |\n| square_glcm_Idm                                               | nan           | nan                                                           | low_variance | 0.00385963  |\n| square_glcm_Idmn                                              | nan           | nan                                                           | low_variance | 0.000204907 |\n| square_glcm_Idn                                               | nan           | nan                                                           | low_variance | 0.00102175  |\n| square_glcm_Imc2                                              | nan           | nan                                                           | low_variance | 0.00317267  |\n| square_glcm_InverseVariance                                   | nan           | nan                                                           | low_variance | 0.00181433  |\n| square_glcm_JointEnergy                                       | nan           | nan                                                           | low_variance | 4.71556e-05 |\n| square_glcm_MaximumProbability                                | nan           | nan                                                           | low_variance | 0.00095081  |\n| square_glrlm_GrayLevelNonUniformityNormalized                 | nan           | nan                                                           | low_variance | 0.000573014 |\n| square_glrlm_LowGrayLevelRunEmphasis                          | nan           | nan                                                           | low_variance | 0.00604114  |\n| square_glrlm_RunLengthNonUniformityNormalized                 | nan           | nan                                                           | low_variance | 0.0025287   |\n| square_glrlm_RunPercentage                                    | nan           | nan                                                           | low_variance | 0.00113573  |\n| square_glrlm_RunVariance                                      | nan           | nan                                                           | low_variance | 0.00494049  |\n| square_glrlm_ShortRunEmphasis                                 | nan           | nan                                                           | low_variance | 0.00047803  |\n| square_glrlm_ShortRunLowGrayLevelEmphasis                     | nan           | nan                                                           | low_variance | 0.00384835  |\n| square_glszm_GrayLevelNonUniformityNormalized                 | nan           | nan                                                           | low_variance | 0.000200955 |\n| square_glszm_LowGrayLevelZoneEmphasis                         | nan           | nan                                                           | low_variance | 0.00168728  |\n| square_glszm_SizeZoneNonUniformityNormalized                  | nan           | nan                                                           | low_variance | 0.00576058  |\n| square_glszm_SmallAreaEmphasis                                | nan           | nan                                                           | low_variance | 0.00137803  |\n| square_glszm_SmallAreaLowGrayLevelEmphasis                    | nan           | nan                                                           | low_variance | 0.000668683 |\n| square_glszm_ZonePercentage                                   | nan           | nan                                                           | low_variance | 0.00848363  |\n| square_ngtdm_Coarseness                                       | nan           | nan                                                           | low_variance | 4.31062e-05 |\n| square_gldm_LowGrayLevelEmphasis                              | nan           | nan                                                           | low_variance | 0.00905747  |\n| square_gldm_SmallDependenceLowGrayLevelEmphasis               | nan           | nan                                                           | low_variance | 0.000660762 |\n| squareroot_firstorder_Uniformity                              | nan           | nan                                                           | low_variance | 2.75644e-05 |\n| squareroot_glcm_Id                                            | nan           | nan                                                           | low_variance | 6.61077e-05 |\n| squareroot_glcm_Idm                                           | nan           | nan                                                           | low_variance | 1.31655e-05 |\n| squareroot_glcm_Idmn                                          | nan           | nan                                                           | low_variance | 0.000332867 |\n| squareroot_glcm_Idn                                           | nan           | nan                                                           | low_variance | 0.000965092 |\n| squareroot_glcm_Imc2                                          | nan           | nan                                                           | low_variance | 5.13125e-07 |\n| squareroot_glcm_InverseVariance                               | nan           | nan                                                           | low_variance | 1.34168e-05 |\n| squareroot_glcm_JointEnergy                                   | nan           | nan                                                           | low_variance | 1.42682e-05 |\n| squareroot_glcm_MCC                                           | nan           | nan                                                           | low_variance | 0.00104379  |\n| squareroot_glcm_MaximumProbability                            | nan           | nan                                                           | low_variance | 1.53886e-05 |\n| squareroot_glrlm_GrayLevelNonUniformityNormalized             | nan           | nan                                                           | low_variance | 2.75326e-05 |\n| squareroot_glrlm_LongRunEmphasis                              | nan           | nan                                                           | low_variance | 4.52724e-05 |\n| squareroot_glrlm_LongRunLowGrayLevelEmphasis                  | nan           | nan                                                           | low_variance | 2.86016e-05 |\n| squareroot_glrlm_LowGrayLevelRunEmphasis                      | nan           | nan                                                           | low_variance | 2.85942e-05 |\n| squareroot_glrlm_RunLengthNonUniformityNormalized             | nan           | nan                                                           | low_variance | 1.89187e-05 |\n| squareroot_glrlm_RunPercentage                                | nan           | nan                                                           | low_variance | 4.82524e-06 |\n| squareroot_glrlm_RunVariance                                  | nan           | nan                                                           | low_variance | 5.03599e-06 |\n| squareroot_glrlm_ShortRunEmphasis                             | nan           | nan                                                           | low_variance | 2.74455e-06 |\n| squareroot_glrlm_ShortRunLowGrayLevelEmphasis                 | nan           | nan                                                           | low_variance | 2.85951e-05 |\n| squareroot_glszm_GrayLevelNonUniformityNormalized             | nan           | nan                                                           | low_variance | 2.74376e-05 |\n| squareroot_glszm_LargeAreaEmphasis                            | nan           | nan                                                           | low_variance | 0.000822342 |\n| squareroot_glszm_LargeAreaLowGrayLevelEmphasis                | nan           | nan                                                           | low_variance | 2.87742e-05 |\n| squareroot_glszm_LowGrayLevelZoneEmphasis                     | nan           | nan                                                           | low_variance | 2.87085e-05 |\n| squareroot_glszm_SizeZoneNonUniformityNormalized              | nan           | nan                                                           | low_variance | 0.000259106 |\n| squareroot_glszm_SmallAreaEmphasis                            | nan           | nan                                                           | low_variance | 4.07771e-05 |\n| squareroot_glszm_SmallAreaLowGrayLevelEmphasis                | nan           | nan                                                           | low_variance | 2.87362e-05 |\n| squareroot_glszm_ZonePercentage                               | nan           | nan                                                           | low_variance | 7.41449e-05 |\n| squareroot_glszm_ZoneVariance                                 | nan           | nan                                                           | low_variance | 0.000102505 |\n| squareroot_ngtdm_Busyness                                     | nan           | nan                                                           | low_variance | 1.89031e-05 |\n| squareroot_ngtdm_Coarseness                                   | nan           | nan                                                           | low_variance | 4.11648e-06 |\n| squareroot_gldm_DependenceNonUniformityNormalized             | nan           | nan                                                           | low_variance | 0.000924848 |\n| squareroot_gldm_DependenceVariance                            | nan           | nan                                                           | low_variance | 0.000321976 |\n| squareroot_gldm_LargeDependenceEmphasis                       | nan           | nan                                                           | low_variance | 0.00300267  |\n| squareroot_gldm_LargeDependenceLowGrayLevelEmphasis           | nan           | nan                                                           | low_variance | 2.91479e-05 |\n| squareroot_gldm_LowGrayLevelEmphasis                          | nan           | nan                                                           | low_variance | 2.85605e-05 |\n| squareroot_gldm_SmallDependenceEmphasis                       | nan           | nan                                                           | low_variance | 0.00015841  |\n| squareroot_gldm_SmallDependenceLowGrayLevelEmphasis           | nan           | nan                                                           | low_variance | 2.85864e-05 |\n| logarithm_firstorder_Uniformity                               | nan           | nan                                                           | low_variance | 2.7942e-05  |\n| logarithm_glcm_Id                                             | nan           | nan                                                           | low_variance | 0.000146151 |\n| logarithm_glcm_Idm                                            | nan           | nan                                                           | low_variance | 2.7233e-05  |\n| logarithm_glcm_Idmn                                           | nan           | nan                                                           | low_variance | 0.000825582 |\n| logarithm_glcm_Idn                                            | nan           | nan                                                           | low_variance | 0.00191619  |\n| logarithm_glcm_Imc2                                           | nan           | nan                                                           | low_variance | 5.13187e-07 |\n| logarithm_glcm_InverseVariance                                | nan           | nan                                                           | low_variance | 2.65322e-05 |\n| logarithm_glcm_JointEnergy                                    | nan           | nan                                                           | low_variance | 1.42752e-05 |\n| logarithm_glcm_MCC                                            | nan           | nan                                                           | low_variance | 0.0010128   |\n| logarithm_glcm_MaximumProbability                             | nan           | nan                                                           | low_variance | 1.54519e-05 |\n| logarithm_glrlm_GrayLevelNonUniformityNormalized              | nan           | nan                                                           | low_variance | 2.78796e-05 |\n| logarithm_glrlm_LongRunEmphasis                               | nan           | nan                                                           | low_variance | 5.59122e-05 |\n| logarithm_glrlm_LongRunLowGrayLevelEmphasis                   | nan           | nan                                                           | low_variance | 2.87537e-05 |\n| logarithm_glrlm_LowGrayLevelRunEmphasis                       | nan           | nan                                                           | low_variance | 2.87415e-05 |\n| logarithm_glrlm_RunLengthNonUniformityNormalized              | nan           | nan                                                           | low_variance | 2.33968e-05 |\n| logarithm_glrlm_RunPercentage                                 | nan           | nan                                                           | low_variance | 5.961e-06   |\n| logarithm_glrlm_RunVariance                                   | nan           | nan                                                           | low_variance | 6.20372e-06 |\n| logarithm_glrlm_ShortRunEmphasis                              | nan           | nan                                                           | low_variance | 3.39628e-06 |\n| logarithm_glrlm_ShortRunLowGrayLevelEmphasis                  | nan           | nan                                                           | low_variance | 2.87412e-05 |\n| logarithm_glszm_GrayLevelNonUniformityNormalized              | nan           | nan                                                           | low_variance | 2.76937e-05 |\n| logarithm_glszm_LargeAreaEmphasis                             | nan           | nan                                                           | low_variance | 0.00100381  |\n| logarithm_glszm_LargeAreaLowGrayLevelEmphasis                 | nan           | nan                                                           | low_variance | 2.89825e-05 |\n| logarithm_glszm_LowGrayLevelZoneEmphasis                      | nan           | nan                                                           | low_variance | 2.88984e-05 |\n| logarithm_glszm_SizeZoneNonUniformityNormalized               | nan           | nan                                                           | low_variance | 0.000328414 |\n| logarithm_glszm_SmallAreaEmphasis                             | nan           | nan                                                           | low_variance | 5.13513e-05 |\n| logarithm_glszm_SmallAreaLowGrayLevelEmphasis                 | nan           | nan                                                           | low_variance | 2.89209e-05 |\n| logarithm_glszm_ZonePercentage                                | nan           | nan                                                           | low_variance | 9.23697e-05 |\n| logarithm_glszm_ZoneVariance                                  | nan           | nan                                                           | low_variance | 0.000122472 |\n| logarithm_ngtdm_Busyness                                      | nan           | nan                                                           | low_variance | 3.0338e-05  |\n| logarithm_ngtdm_Coarseness                                    | nan           | nan                                                           | low_variance | 3.84547e-06 |\n| logarithm_gldm_DependenceNonUniformityNormalized              | nan           | nan                                                           | low_variance | 0.00117142  |\n| logarithm_gldm_DependenceVariance                             | nan           | nan                                                           | low_variance | 0.00038524  |\n| logarithm_gldm_LargeDependenceEmphasis                        | nan           | nan                                                           | low_variance | 0.00366181  |\n| logarithm_gldm_LargeDependenceLowGrayLevelEmphasis            | nan           | nan                                                           | low_variance | 2.93199e-05 |\n| logarithm_gldm_LowGrayLevelEmphasis                           | nan           | nan                                                           | low_variance | 2.86944e-05 |\n| logarithm_gldm_SmallDependenceEmphasis                        | nan           | nan                                                           | low_variance | 0.000198615 |\n| logarithm_gldm_SmallDependenceLowGrayLevelEmphasis            | nan           | nan                                                           | low_variance | 2.87109e-05 |\n| exponential_glcm_Idmn                                         | nan           | nan                                                           | low_variance | 8.79305e-05 |\n| exponential_glcm_Idn                                          | nan           | nan                                                           | low_variance | 0.000701832 |\n| exponential_glcm_InverseVariance                              | nan           | nan                                                           | low_variance | 0.0070379   |\n| exponential_glcm_JointEnergy                                  | nan           | nan                                                           | low_variance | 0.00546576  |\n| exponential_glrlm_GrayLevelNonUniformityNormalized            | nan           | nan                                                           | low_variance | 0.00847903  |\n| exponential_glrlm_ShortRunEmphasis                            | nan           | nan                                                           | low_variance | 0.00936734  |\n| exponential_glrlm_ShortRunLowGrayLevelEmphasis                | nan           | nan                                                           | low_variance | 0.00424834  |\n| exponential_glszm_GrayLevelNonUniformityNormalized            | nan           | nan                                                           | low_variance | 0.00459847  |\n| exponential_glszm_LowGrayLevelZoneEmphasis                    | nan           | nan                                                           | low_variance | 0.00902248  |\n| exponential_glszm_SmallAreaEmphasis                           | nan           | nan                                                           | low_variance | 0.00727115  |\n| exponential_glszm_SmallAreaLowGrayLevelEmphasis               | nan           | nan                                                           | low_variance | 0.00282952  |\n| exponential_ngtdm_Coarseness                                  | nan           | nan                                                           | low_variance | 0.000446468 |\n| exponential_gldm_SmallDependenceLowGrayLevelEmphasis          | nan           | nan                                                           | low_variance | 0.000231508 |\n\nFinal Features: ['diagnostics_Image-interpolated_Mean', 'original_shape_Maximum3DDiameter', 'log-sigma-2-mm-3D_firstorder_Range', 'log-sigma-2-mm-3D_ngtdm_Strength', 'log-sigma-3-mm-3D_firstorder_Maximum', 'log-sigma-3-mm-3D_glcm_DifferenceVariance', 'wavelet-HL_gldm_SmallDependenceHighGrayLevelEmphasis', 'wavelet-HH_firstorder_RootMeanSquared', 'wavelet2-HL_gldm_SmallDependenceHighGrayLevelEmphasis', 'wavelet2-LL_gldm_GrayLevelVariance']\n\n\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n\n\n\n#@title Funciones de Evaluación y Entrenamiento\n\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\n\ndef train_and_evaluate(model, X_train, y_train, X_test, y_test, model_name=\"Model\"):\n    \"\"\"Trains and evaluates a model, printing and returning evaluation metrics.\"\"\"\n\n    # --- Cross-validation (on training data) ---\n    if y_train.nunique() &gt; 2:  # Multiclass\n        cv_scoring = 'accuracy'  # Or other appropriate multiclass metric\n    else: # Binary\n        cv_scoring = 'roc_auc'  # Use AUC for binary classification\n\n    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring=cv_scoring)\n    print(f\"\\nCross-validation scores ({cv_scoring}): {cv_scores}\")\n    print(f\"Mean cross-validation score ({cv_scoring}): {cv_scores.mean()}\")\n\n    # --- Train on full training set ---\n    model.fit(X_train, y_train)\n\n    # --- Evaluate on test set (if available) ---\n    if X_test is not None and y_test is not None:\n        y_pred = model.predict(X_test)\n\n        if hasattr(model, \"predict_proba\"):\n            y_prob = model.predict_proba(X_test)\n        else:\n            y_prob = None\n\n        print(f\"\\n--- {model_name} Test Set Evaluation ---\")\n\n        if y_test.nunique() &gt; 2:  # Multiclass classification\n            # Ensure y_test and y_pred are integer type\n            y_test_int = y_test.astype(int)\n            y_pred_int = y_pred.astype(int)\n            print(\"Classification Report:\\n\", classification_report(y_test_int, y_pred_int))\n            print(\"Confusion Matrix:\\n\", confusion_matrix(y_test_int, y_pred_int))\n\n        else:  # Binary classification\n            print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n            print(\"Precision:\", precision_score(y_test, y_pred))\n            print(\"Recall:\", recall_score(y_test, y_pred))\n            print(\"F1-score:\", f1_score(y_test, y_pred))\n            if y_prob is not None:\n                try:\n                    print(\"AUC-ROC:\", roc_auc_score(y_test, y_prob[:, 1]))\n                except ValueError as e:\n                    print(f\"AUC-ROC calculation failed: {e}\")\n\n            print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n\n        return model, cv_scores.mean()\n    else:\n        return model, cv_scores.mean()\n\n\ndef train_evaluate_and_plot_importance(model, X_train, y_train, X_test, y_test, model_name=\"Model\"):\n    \"\"\"Trains, evaluates, and plots feature importances (if available).\"\"\"\n\n    trained_model, cv_score = train_and_evaluate(model, X_train, y_train, X_test, y_test, model_name)\n\n    # --- Feature Importance ---\n    if hasattr(trained_model, 'coef_'):  # Linear models\n        try:\n            feature_importances = pd.DataFrame({'feature': X_train.columns, 'importance': np.abs(trained_model.coef_[0])})\n            feature_importances = feature_importances.sort_values('importance', ascending=False)\n            print(\"\\nFeature Importances:\")\n            print(feature_importances.to_markdown(index=False))\n\n            plt.figure(figsize=(10, 6))\n            sns.barplot(x='importance', y='feature', data=feature_importances)\n            plt.title(f'{model_name} Feature Importance')\n            plt.show()\n        except Exception as e:\n            print(f\"Error getting feature importances: {e}\")\n\n    elif hasattr(trained_model, 'feature_importances_'):  # Tree-based models\n        try:\n            feature_importances = pd.DataFrame({'feature': X_train.columns, 'importance': trained_model.feature_importances_})\n            feature_importances = feature_importances.sort_values('importance', ascending=False)\n            print(\"\\nFeature Importances:\")\n            print(feature_importances.to_markdown(index=False))\n\n            plt.figure(figsize=(10, 6))\n            sns.barplot(x='importance', y='feature', data=feature_importances)\n            plt.title(f'{model_name} Feature Importance')\n            plt.show()\n        except Exception as e:\n            print(f\"Error getting feature importances: {e}\")\n    else:\n        print(f\"Feature importances not available for {model_name}.\")\n\n    return trained_model, cv_score\n\n\ndef train_evaluate_multiple_models(X_train, y_train, X_test, y_test, models):\n    \"\"\"\n    Trains and evaluates multiple models.\n\n    Args:\n        X_train: Training features.\n        y_train: Training target.\n        X_test: Test features.\n        y_test: Test target.\n        models: A dictionary of models, where keys are model names and values are model instances.\n\n    Returns:\n        A dictionary of trained models and their cross-validation scores.\n    \"\"\"\n\n    results = {}\n    for model_name, model in models.items():\n        print(f\"\\n===== Training and Evaluating: {model_name} =====\")\n        trained_model, cv_score = train_evaluate_and_plot_importance(\n            model, X_train, y_train, X_test, y_test, model_name=model_name\n        )\n        results[model_name] = {'model': trained_model, 'cv_score': cv_score}\n    return results\n\n\n# @title Entrenamiento y Evaluación de Múltiples Modelos (Clinically Significant)\n\n# --- Define the models ---\nmodels_cs = {\n    'Logistic Regression': LogisticRegression(solver='liblinear', random_state=42),\n    'Random Forest': RandomForestClassifier(random_state=42),\n    'SVM': SVC(probability=True, random_state=42),  # Add probability=True for ROC AUC\n    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42), #Added\n    'LightGBM': LGBMClassifier(random_state=42), #Added\n    'CatBoost': CatBoostClassifier(random_state=42, verbose=False) #Added\n\n}\n\n# --- Train and evaluate the models ---\nresults_cs = train_evaluate_multiple_models(\n    X_train_processed_cs, y_train_cs, X_test_processed_cs, y_test_cs, models_cs\n)\n\n\n===== Training and Evaluating: Logistic Regression =====\n\nCross-validation scores (roc_auc): [0.78009259 0.77777778 0.75462963 0.69010989 0.8       ]\nMean cross-validation score (roc_auc): 0.7605219780219781\n\n--- Logistic Regression Test Set Evaluation ---\nAccuracy: 0.65\nPrecision: 0.2\nRecall: 0.13333333333333333\nF1-score: 0.16\nAUC-ROC: 0.49333333333333323\nConfusion Matrix:\n [[37  8]\n [13  2]]\n\nFeature Importances:\n| feature                                               |   importance |\n|:------------------------------------------------------|-------------:|\n| log-sigma-2-mm-3D_firstorder_Range                    |     1.2787   |\n| wavelet-HL_gldm_SmallDependenceHighGrayLevelEmphasis  |     1.17398  |\n| wavelet2-HL_gldm_SmallDependenceHighGrayLevelEmphasis |     1.09658  |\n| log-sigma-2-mm-3D_ngtdm_Strength                      |     0.988786 |\n| log-sigma-3-mm-3D_firstorder_Maximum                  |     0.762111 |\n| log-sigma-3-mm-3D_glcm_DifferenceVariance             |     0.730583 |\n| wavelet2-LL_gldm_GrayLevelVariance                    |     0.586351 |\n| diagnostics_Image-interpolated_Mean                   |     0.583298 |\n| original_shape_Maximum3DDiameter                      |     0.576019 |\n| wavelet-HH_firstorder_RootMeanSquared                 |     0.556672 |\n\n\n\n\n\n\n\n\n\n\n===== Training and Evaluating: Random Forest =====\n\nCross-validation scores (roc_auc): [0.63078704 0.61226852 0.63425926 0.5021978  0.58928571]\nMean cross-validation score (roc_auc): 0.5937596662596663\n\n--- Random Forest Test Set Evaluation ---\nAccuracy: 0.7666666666666667\nPrecision: 0.6\nRecall: 0.2\nF1-score: 0.3\nAUC-ROC: 0.6207407407407407\nConfusion Matrix:\n [[43  2]\n [12  3]]\n\nFeature Importances:\n| feature                                               |   importance |\n|:------------------------------------------------------|-------------:|\n| log-sigma-3-mm-3D_firstorder_Maximum                  |    0.115924  |\n| diagnostics_Image-interpolated_Mean                   |    0.115871  |\n| original_shape_Maximum3DDiameter                      |    0.11087   |\n| wavelet-HL_gldm_SmallDependenceHighGrayLevelEmphasis  |    0.105252  |\n| log-sigma-2-mm-3D_firstorder_Range                    |    0.100544  |\n| log-sigma-3-mm-3D_glcm_DifferenceVariance             |    0.099874  |\n| log-sigma-2-mm-3D_ngtdm_Strength                      |    0.0936723 |\n| wavelet-HH_firstorder_RootMeanSquared                 |    0.0917353 |\n| wavelet2-HL_gldm_SmallDependenceHighGrayLevelEmphasis |    0.0838022 |\n| wavelet2-LL_gldm_GrayLevelVariance                    |    0.0824548 |\n\n\n\n\n\n\n\n\n\n\n===== Training and Evaluating: SVM =====\n\nCross-validation scores (roc_auc): [0.68287037 0.69675926 0.7662037  0.52747253 0.62857143]\nMean cross-validation score (roc_auc): 0.6603754578754579\n\n--- SVM Test Set Evaluation ---\nAccuracy: 0.75\nPrecision: 0.0\nRecall: 0.0\nF1-score: 0.0\nAUC-ROC: 0.5792592592592594\nConfusion Matrix:\n [[45  0]\n [15  0]]\nFeature importances not available for SVM.\n\n===== Training and Evaluating: XGBoost =====\n\n\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [08:16:52] WARNING: /workspace/src/learner.cc:740: \nParameters: { \"use_label_encoder\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [08:16:52] WARNING: /workspace/src/learner.cc:740: \nParameters: { \"use_label_encoder\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [08:16:52] WARNING: /workspace/src/learner.cc:740: \nParameters: { \"use_label_encoder\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [08:16:52] WARNING: /workspace/src/learner.cc:740: \nParameters: { \"use_label_encoder\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [08:16:52] WARNING: /workspace/src/learner.cc:740: \nParameters: { \"use_label_encoder\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [08:16:52] WARNING: /workspace/src/learner.cc:740: \nParameters: { \"use_label_encoder\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n\n\n\nCross-validation scores (roc_auc): [0.67361111 0.62731481 0.54398148 0.56263736 0.62619048]\nMean cross-validation score (roc_auc): 0.6067470492470493\n\n--- XGBoost Test Set Evaluation ---\nAccuracy: 0.7666666666666667\nPrecision: 0.5555555555555556\nRecall: 0.3333333333333333\nF1-score: 0.4166666666666667\nAUC-ROC: 0.5985185185185184\nConfusion Matrix:\n [[41  4]\n [10  5]]\n\nFeature Importances:\n| feature                                               |   importance |\n|:------------------------------------------------------|-------------:|\n| diagnostics_Image-interpolated_Mean                   |    0.153196  |\n| log-sigma-3-mm-3D_firstorder_Maximum                  |    0.138982  |\n| original_shape_Maximum3DDiameter                      |    0.124222  |\n| wavelet-HL_gldm_SmallDependenceHighGrayLevelEmphasis  |    0.11138   |\n| log-sigma-2-mm-3D_firstorder_Range                    |    0.104056  |\n| log-sigma-3-mm-3D_glcm_DifferenceVariance             |    0.0875508 |\n| wavelet-HH_firstorder_RootMeanSquared                 |    0.0819854 |\n| wavelet2-HL_gldm_SmallDependenceHighGrayLevelEmphasis |    0.0818431 |\n| log-sigma-2-mm-3D_ngtdm_Strength                      |    0.0633371 |\n| wavelet2-LL_gldm_GrayLevelVariance                    |    0.0534489 |\n\n\n\n\n\n\n\n\n\n\n===== Training and Evaluating: LightGBM =====\n[LightGBM] [Info] Number of positive: 49, number of negative: 142\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000118 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 653\n[LightGBM] [Info] Number of data points in the train set: 191, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.256545 -&gt; initscore=-1.064007\n[LightGBM] [Info] Start training from score -1.064007\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 49, number of negative: 142\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000038 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 651\n[LightGBM] [Info] Number of data points in the train set: 191, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.256545 -&gt; initscore=-1.064007\n[LightGBM] [Info] Start training from score -1.064007\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 49, number of negative: 142\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000042 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 653\n[LightGBM] [Info] Number of data points in the train set: 191, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.256545 -&gt; initscore=-1.064007\n[LightGBM] [Info] Start training from score -1.064007\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 48, number of negative: 143\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000037 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 653\n[LightGBM] [Info] Number of data points in the train set: 191, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.251309 -&gt; initscore=-1.091644\n[LightGBM] [Info] Start training from score -1.091644\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 49, number of negative: 143\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000040 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 651\n[LightGBM] [Info] Number of data points in the train set: 192, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.255208 -&gt; initscore=-1.071024\n[LightGBM] [Info] Start training from score -1.071024\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\nCross-validation scores (roc_auc): [0.68287037 0.59490741 0.56481481 0.56043956 0.55238095]\nMean cross-validation score (roc_auc): 0.591082621082621\n[LightGBM] [Info] Number of positive: 61, number of negative: 178\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000044 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 811\n[LightGBM] [Info] Number of data points in the train set: 239, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.255230 -&gt; initscore=-1.070910\n[LightGBM] [Info] Start training from score -1.070910\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n--- LightGBM Test Set Evaluation ---\nAccuracy: 0.6666666666666666\nPrecision: 0.3076923076923077\nRecall: 0.26666666666666666\nF1-score: 0.2857142857142857\nAUC-ROC: 0.5999999999999999\nConfusion Matrix:\n [[36  9]\n [11  4]]\n\nFeature Importances:\n| feature                                               |   importance |\n|:------------------------------------------------------|-------------:|\n| diagnostics_Image-interpolated_Mean                   |           99 |\n| log-sigma-3-mm-3D_glcm_DifferenceVariance             |           98 |\n| log-sigma-3-mm-3D_firstorder_Maximum                  |           97 |\n| log-sigma-2-mm-3D_firstorder_Range                    |           94 |\n| wavelet-HL_gldm_SmallDependenceHighGrayLevelEmphasis  |           86 |\n| original_shape_Maximum3DDiameter                      |           85 |\n| log-sigma-2-mm-3D_ngtdm_Strength                      |           83 |\n| wavelet2-HL_gldm_SmallDependenceHighGrayLevelEmphasis |           83 |\n| wavelet-HH_firstorder_RootMeanSquared                 |           68 |\n| wavelet2-LL_gldm_GrayLevelVariance                    |           56 |\n\n\n\n\n\n\n\n\n\n\n===== Training and Evaluating: CatBoost =====\n\nCross-validation scores (roc_auc): [0.62731481 0.63657407 0.68287037 0.54285714 0.65      ]\nMean cross-validation score (roc_auc): 0.6279232804232804\n\n--- CatBoost Test Set Evaluation ---\nAccuracy: 0.75\nPrecision: 0.5\nRecall: 0.13333333333333333\nF1-score: 0.21052631578947367\nAUC-ROC: 0.5896296296296296\nConfusion Matrix:\n [[43  2]\n [13  2]]\n\nFeature Importances:\n| feature                                               |   importance |\n|:------------------------------------------------------|-------------:|\n| log-sigma-3-mm-3D_firstorder_Maximum                  |     12.7031  |\n| log-sigma-3-mm-3D_glcm_DifferenceVariance             |     12.6283  |\n| diagnostics_Image-interpolated_Mean                   |     11.8555  |\n| original_shape_Maximum3DDiameter                      |     10.9736  |\n| log-sigma-2-mm-3D_firstorder_Range                    |      9.42587 |\n| wavelet-HH_firstorder_RootMeanSquared                 |      9.35764 |\n| wavelet-HL_gldm_SmallDependenceHighGrayLevelEmphasis  |      9.30629 |\n| log-sigma-2-mm-3D_ngtdm_Strength                      |      8.16406 |\n| wavelet2-HL_gldm_SmallDependenceHighGrayLevelEmphasis |      7.87704 |\n| wavelet2-LL_gldm_GrayLevelVariance                    |      7.70862 |\n\n\n\n\n\n\n\n\n\n\n# @title Entrenamiento y Evaluación de Modelos\n\n# --- Modelo para Clinically Significant ---\nprint(\"\\n===== Clinically Significant Model =====\")\nmodel_cs = LogisticRegression(solver='liblinear', random_state=42)  # Or any other suitable binary classifier\ntrained_model_cs, cv_score_cs = train_evaluate_and_plot_importance(\n    model_cs, X_train_processed_cs, y_train_cs, X_test_processed_cs, y_test_cs, model_name=\"Logistic Regression (CS)\"\n)\n\n\n===== Clinically Significant Model =====\n\nCross-validation scores (roc_auc): [0.78009259 0.77777778 0.75462963 0.69010989 0.8       ]\nMean cross-validation score (roc_auc): 0.7605219780219781\n\n--- Logistic Regression (CS) Test Set Evaluation ---\nAccuracy: 0.65\nPrecision: 0.2\nRecall: 0.13333333333333333\nF1-score: 0.16\nAUC-ROC: 0.49333333333333323\nConfusion Matrix:\n [[37  8]\n [13  2]]\n\nFeature Importances:\n| feature                                               |   importance |\n|:------------------------------------------------------|-------------:|\n| log-sigma-2-mm-3D_firstorder_Range                    |     1.2787   |\n| wavelet-HL_gldm_SmallDependenceHighGrayLevelEmphasis  |     1.17398  |\n| wavelet2-HL_gldm_SmallDependenceHighGrayLevelEmphasis |     1.09658  |\n| log-sigma-2-mm-3D_ngtdm_Strength                      |     0.988786 |\n| log-sigma-3-mm-3D_firstorder_Maximum                  |     0.762111 |\n| log-sigma-3-mm-3D_glcm_DifferenceVariance             |     0.730583 |\n| wavelet2-LL_gldm_GrayLevelVariance                    |     0.586351 |\n| diagnostics_Image-interpolated_Mean                   |     0.583298 |\n| original_shape_Maximum3DDiameter                      |     0.576019 |\n| wavelet-HH_firstorder_RootMeanSquared                 |     0.556672 |\n\n\n\n\n\n\n\n\n\n\n# @title Guardar Resultados (Opcional)\n\n# Create a results directory if it doesn't exist.\nimport os\nif not os.path.exists(\"results\"):\n    os.makedirs(\"results\")\n\n# --- Save Processed Data ---\nX_train_processed_cs.to_csv('results/X_train_processed_cs.csv', index=False)\nif X_test_processed_cs is not None:\n    X_test_processed_cs.to_csv('results/X_test_processed_cs.csv', index=False)\n\nX_train_processed_gleason.to_csv('results/X_train_processed_gleason.csv', index=False)\nif X_test_processed_gleason is not None:\n    X_test_processed_gleason.to_csv('results/X_test_processed_gleason.csv', index=False)\n\ndropped_df.to_csv('results/dropped_columns.csv', index=False)\n\n# --- Save Models (using pickle) ---\nimport pickle\n\n# Save models for Clinically Significant\nfor model_name, model_data in results_cs.items():\n    model = model_data['model']\n    filename = f'results/model_cs_{model_name.replace(\" \", \"_\").lower()}.pkl'  # Create unique filenames\n    pickle.dump(model, open(filename, 'wb'))\n\n#You can save the Gleason models in a similar way if you train them\n\n# --- Save PCA results (if applicable)---\nif results['pca'] is not None:\n    pca_results = results['pca']\n    pca_df = pd.DataFrame(pca_results['components'], columns=X_train_scaled.columns)\n    pca_df.to_csv('results/pca_loadings.csv')\n    #Guardar la varianza explicada\n    explained_variance_df = pd.DataFrame({\n        'Principal Component': range(1, len(pca_results['explained_variance_ratio']) + 1),\n        'Explained Variance Ratio': pca_results['explained_variance_ratio'],\n        'Cumulative Explained Variance': pca_results['cumulative_explained_variance']\n    })\n    explained_variance_df.to_csv('results/pca_explained_variance.csv', index=False)\n\n    # Gráfico de Varianza Explicada\n    plt.figure(figsize=(10, 6))\n    plt.bar(range(1, len(pca_results['explained_variance_ratio']) + 1), pca_results['explained_variance_ratio'], alpha=0.5, align='center',\n            label='Individual explained variance', color = 'blue')\n    plt.step(range(1, len(pca_results['cumulative_explained_variance']) + 1), pca_results['cumulative_explained_variance'], where='mid',\n             label='Cumulative explained variance', color = 'red')\n    plt.ylabel('Explained Variance Ratio')\n    plt.xlabel('Principal Component')\n    plt.legend(loc='best')\n    plt.title('Explained Variance by Principal Components')\n    plt.tight_layout()\n    plt.savefig('results/pca_explained_variance_plot.png') #Save the plot\n    plt.show()\n\n\n# --- Save ICA results (if applicable) ---\nif results['ica'] is not None:\n    ica_results = results['ica']\n    ica_df = pd.DataFrame(ica_results['mixing_matrix'], index=X_train_scaled.columns)  # Use components as index\n    ica_df.to_csv('results/ica_mixing_matrix.csv')  #Save the mixing matrix\n\nprint(\"Results saved to the 'results' directory.\")\n\n\n\n\n\n\n\n\nResults saved to the 'results' directory."
  },
  {
    "objectID": "posts/Presentaciones/Procesamiento_datos.html#creación-del-la-db-de-grafos",
    "href": "posts/Presentaciones/Procesamiento_datos.html#creación-del-la-db-de-grafos",
    "title": "Procesamiento de prospectos",
    "section": "Creación del la DB de grafos",
    "text": "Creación del la DB de grafos\n\n\nNeo4j\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/Procesamiento_datos.html#preguntas",
    "href": "posts/Presentaciones/Procesamiento_datos.html#preguntas",
    "title": "Procesamiento de prospectos",
    "section": "Preguntas",
    "text": "Preguntas\n\nMe duele mucho la cabeza y tengo el estómago delicado, ¿hay algún analgésico que no me lo irrite?\nTengo dolor de cabeza y estoy tomando anticoagulantes, ¿hay alguna interacción que deba tener en cuenta?\nSoy mayor y tengo dolor de articulaciones, ¿qué medicamento es seguro para mí?\nTengo dolor menstrual muy fuerte, ¿qué medicamento me puede ayudar?\nMe duele mucho la cabeza y tengo el estómago delicado, ¿hay algún analgésico que no me lo irrite?\n\n\nNotes"
  }
]