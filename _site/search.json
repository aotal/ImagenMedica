[
  {
    "objectID": "posts/Presentaciones/IntroIM.html#índice",
    "href": "posts/Presentaciones/IntroIM.html#índice",
    "title": "Introducción a la imagen médica",
    "section": "Índice",
    "text": "Índice\n\nIntroducción a la imagen digital\nConceptos básicos de imagen\nAdquisición de imágenes médicas\nProcesamiento de imágenes\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/IntroIM.html#definición-de-imagen-digital",
    "href": "posts/Presentaciones/IntroIM.html#definición-de-imagen-digital",
    "title": "Introducción a la imagen médica",
    "section": "Definición de imagen digital",
    "text": "Definición de imagen digital\nUna imagen digital o gráfico digital es una representación bidimensional de una imagen a partir de una matriz numérica, frecuentemente en binario (unos y ceros). Dependiendo de si la resolución de la imagen es estática o dinámica, puede tratarse de una imagen matricial (o mapa de bits) o de un gráfico vectorial. El mapa de bits es el formato más utilizado en informática.\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/IntroIM.html#definición-de-radiología-digital",
    "href": "posts/Presentaciones/IntroIM.html#definición-de-radiología-digital",
    "title": "Introducción a la imagen médica",
    "section": "Definición de radiología digital",
    "text": "Definición de radiología digital\nLa radiología digital es el conjunto de técnicas para obtener imágenes radiológicas escaneadas en formato digital\n\nA partir del escaneo de una la película tradicional (analógica) una vez revelada.\nPor escaneo de una Placa fotoestimulable de fósforo reutilizable que se graba con la imagen de la radiografía (CR)\nUtilizando detectores sensibles expuestos directa o indirectamente a los detectores de rayos X.\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/IntroIM.html#breve-historia-y-evolución-de-la-imagen-digital",
    "href": "posts/Presentaciones/IntroIM.html#breve-historia-y-evolución-de-la-imagen-digital",
    "title": "Introducción a la imagen médica",
    "section": "Breve historia y evolución de la imagen digital",
    "text": "Breve historia y evolución de la imagen digital\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/IntroIM.html#importancia-de-la-imagen-digital-en-la-medicina",
    "href": "posts/Presentaciones/IntroIM.html#importancia-de-la-imagen-digital-en-la-medicina",
    "title": "Introducción a la imagen médica",
    "section": "Importancia de la imagen digital en la medicina",
    "text": "Importancia de la imagen digital en la medicina\n\nDiagnóstico preciso.\nProcedimiento no invasivo\nInvestigación y desarrollo\nReducción de la dosis al paciente\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/IntroIM.html#qué-es-un-pixel",
    "href": "posts/Presentaciones/IntroIM.html#qué-es-un-pixel",
    "title": "Introducción a la imagen médica",
    "section": "¿Qué es un pixel?",
    "text": "¿Qué es un pixel?\n\n\n\n\n\n\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/IntroIM.html#resolución",
    "href": "posts/Presentaciones/IntroIM.html#resolución",
    "title": "Introducción a la imagen médica",
    "section": "Resolución",
    "text": "Resolución\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/IntroIM.html#profundidad-de-bit",
    "href": "posts/Presentaciones/IntroIM.html#profundidad-de-bit",
    "title": "Introducción a la imagen médica",
    "section": "Profundidad de bit",
    "text": "Profundidad de bit"
  },
  {
    "objectID": "posts/Presentaciones/IntroIM.html#otras-características",
    "href": "posts/Presentaciones/IntroIM.html#otras-características",
    "title": "Introducción a la imagen médica",
    "section": "Otras características",
    "text": "Otras características\n\nEspacio de color (RGB,CMYK)\nRelación de aspecto\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/IntroIM.html#modalidades-de-imagen-médica-radiografías",
    "href": "posts/Presentaciones/IntroIM.html#modalidades-de-imagen-médica-radiografías",
    "title": "Introducción a la imagen médica",
    "section": "Modalidades de imagen médica: Radiografías",
    "text": "Modalidades de imagen médica: Radiografías\n\n\n\nTécnica 2D\nRayos X\nExploración anatómica\nGrafía\n\n\n\n\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/IntroIM.html#modalidades-de-imagen-médica-arcos-de-quirófano",
    "href": "posts/Presentaciones/IntroIM.html#modalidades-de-imagen-médica-arcos-de-quirófano",
    "title": "Introducción a la imagen médica",
    "section": "Modalidades de imagen médica: Arcos de quirófano",
    "text": "Modalidades de imagen médica: Arcos de quirófano\n\n\n\nTécnica 2D\nRayos X\nExploración anatómica\nEscopia\n\n\n\n\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/IntroIM.html#modalidades-de-imagen-médica-tomografía-computerizada",
    "href": "posts/Presentaciones/IntroIM.html#modalidades-de-imagen-médica-tomografía-computerizada",
    "title": "Introducción a la imagen médica",
    "section": "Modalidades de imagen médica: Tomografía computerizada",
    "text": "Modalidades de imagen médica: Tomografía computerizada\n\n\n\nTécnica 3D\nRayos X\nExploración anatómica\nImagen tomográfica\n\n\n\n\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/IntroIM.html#modalidades-de-imagen-médica-imágenes-por-resonancia-magnética-nuclear",
    "href": "posts/Presentaciones/IntroIM.html#modalidades-de-imagen-médica-imágenes-por-resonancia-magnética-nuclear",
    "title": "Introducción a la imagen médica",
    "section": "Modalidades de imagen médica: Imágenes por resonancia magnética nuclear",
    "text": "Modalidades de imagen médica: Imágenes por resonancia magnética nuclear\n\n\n\nTécnica 3D\nResonancia magnética nuclear\nExploración anatómica\nImagen tomográfica\n\n\n\n\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/IntroIM.html#modalidades-de-imagen-médica-equipos-de-ultrasonidos",
    "href": "posts/Presentaciones/IntroIM.html#modalidades-de-imagen-médica-equipos-de-ultrasonidos",
    "title": "Introducción a la imagen médica",
    "section": "Modalidades de imagen médica: Equipos de ultrasonidos",
    "text": "Modalidades de imagen médica: Equipos de ultrasonidos\n\n\n\nTécnica 2D\nOndas sonoras de alta frecuencia\nExploración anatómica\n\n\n\n\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/IntroIM.html#modalidades-de-imagen-médica-spect",
    "href": "posts/Presentaciones/IntroIM.html#modalidades-de-imagen-médica-spect",
    "title": "Introducción a la imagen médica",
    "section": "Modalidades de imagen médica: SPECT",
    "text": "Modalidades de imagen médica: SPECT\n\n\n\nTécnica 3D\nRadiofármacos que emiten radiación gamma (\\(^{99}Tc^{m}\\))\nExploración funcional\n\n\n\n\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/IntroIM.html#modalidades-de-imagen-médica-pet",
    "href": "posts/Presentaciones/IntroIM.html#modalidades-de-imagen-médica-pet",
    "title": "Introducción a la imagen médica",
    "section": "Modalidades de imagen médica: PET",
    "text": "Modalidades de imagen médica: PET\n\n\n\nTécnica 3D\nRadiofármacos que emiten positrones (\\(^{18}F\\))\nExploración funcional\n\n\n\n\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/IntroIM.html#segmentación-de-órganos",
    "href": "posts/Presentaciones/IntroIM.html#segmentación-de-órganos",
    "title": "Introducción a la imagen médica",
    "section": "Segmentación de órganos",
    "text": "Segmentación de órganos\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/IntroIM.html#segmentación-de-células",
    "href": "posts/Presentaciones/IntroIM.html#segmentación-de-células",
    "title": "Introducción a la imagen médica",
    "section": "Segmentación de células",
    "text": "Segmentación de células\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/IntroIM.html#segmentación-de-tumores",
    "href": "posts/Presentaciones/IntroIM.html#segmentación-de-tumores",
    "title": "Introducción a la imagen médica",
    "section": "Segmentación de tumores",
    "text": "Segmentación de tumores\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/IntroIM.html#instalar-3dslicer-y-darse-de-alta-en-huggingface",
    "href": "posts/Presentaciones/IntroIM.html#instalar-3dslicer-y-darse-de-alta-en-huggingface",
    "title": "Introducción a la imagen médica",
    "section": "Instalar 3DSlicer y darse de alta en Huggingface",
    "text": "Instalar 3DSlicer y darse de alta en Huggingface\n\nDescargar el programa\nSeguir las intrucciones del ejecutable\nIr a la página de Huggingface\nDarse de alta\n\n\n\n\nIAMED"
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#índice",
    "href": "posts/Presentaciones/Datos_1.html#índice",
    "title": "Datos en el ámbito de la salud",
    "section": "Índice",
    "text": "Índice\n\nIntroducción\nClasificación de los datos\nAnotación de datos\nPrivacidad y confidencialidad\nRegistros electrónicos de salud\nPreprocesado de datos médicos\nVectorizaciones (Embeddings)\nRadiómica (Radiomics)\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#disponibilidad-masiva-de-datos",
    "href": "posts/Presentaciones/Datos_1.html#disponibilidad-masiva-de-datos",
    "title": "Datos en el ámbito de la salud",
    "section": "Disponibilidad masiva de datos",
    "text": "Disponibilidad masiva de datos\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#machine-learning",
    "href": "posts/Presentaciones/Datos_1.html#machine-learning",
    "title": "Datos en el ámbito de la salud",
    "section": "Machine learning",
    "text": "Machine learning\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#problema-con-los-datos",
    "href": "posts/Presentaciones/Datos_1.html#problema-con-los-datos",
    "title": "Datos en el ámbito de la salud",
    "section": "Problema con los datos",
    "text": "Problema con los datos\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#por-tipos-de-datos",
    "href": "posts/Presentaciones/Datos_1.html#por-tipos-de-datos",
    "title": "Datos en el ámbito de la salud",
    "section": "Por tipos de datos",
    "text": "Por tipos de datos\n\n\n\nDatos estructurados\nDatos no estructurados\nDatos semiestructurados\n\n\n\n\n\n\n\n\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#datos-estructurados",
    "href": "posts/Presentaciones/Datos_1.html#datos-estructurados",
    "title": "Datos en el ámbito de la salud",
    "section": "Datos estructurados",
    "text": "Datos estructurados\n\n\n\n\n\n\n\n\n\n\n\nA_id\nSize\nWeight\nSweetness\nCrunchiness\nJuiciness\nRipeness\nAcidity\nQuality\n\n\n\n\n0\n0.0\n-3.970049\n-2.512336\n5.346330\n-1.012009\n1.844900\n0.329840\n-0.491590483\ngood\n\n\n1\n1.0\n-1.195217\n-2.839257\n3.664059\n1.588232\n0.853286\n0.867530\n-0.722809367\ngood\n\n\n2\n2.0\n-0.292024\n-1.351282\n-1.738429\n-0.342616\n2.838636\n-0.038033\n2.621636473\nbad\n\n\n3\n3.0\n-0.657196\n-2.271627\n1.324874\n-0.097875\n3.637970\n-3.413761\n0.790723217\ngood\n\n\n4\n4.0\n1.364217\n-1.296612\n-0.384658\n-0.553006\n3.030874\n-1.303849\n0.501984036\ngood\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3996\n3996.0\n-0.293118\n1.949253\n-0.204020\n-0.640196\n0.024523\n-1.087900\n1.854235285\ngood\n\n\n3997\n3997.0\n-2.634515\n-2.138247\n-2.440461\n0.657223\n2.199709\n4.763859\n-1.334611391\nbad\n\n\n3998\n3998.0\n-4.008004\n-1.779337\n2.366397\n-0.200329\n2.161435\n0.214488\n-2.229719806\ngood\n\n\n3999\n3999.0\n0.278540\n-1.715505\n0.121217\n-1.154075\n1.266677\n-0.776571\n1.599796456\ngood\n\n\n4000\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nCreated_by_Nidula_Elgiriyewithana\nNaN\n\n\n\n\n4001 rows × 9 columns\n\n\n\n\n\n\n\n\n\n\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#ejemplos-de-datos-estructurados",
    "href": "posts/Presentaciones/Datos_1.html#ejemplos-de-datos-estructurados",
    "title": "Datos en el ámbito de la salud",
    "section": "Ejemplos de datos estructurados",
    "text": "Ejemplos de datos estructurados\n\nRegistros Electrónicos de Salud\nDatos de Laboratorio Clínico"
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#datos-no-estructurados",
    "href": "posts/Presentaciones/Datos_1.html#datos-no-estructurados",
    "title": "Datos en el ámbito de la salud",
    "section": "Datos no estructurados",
    "text": "Datos no estructurados\n\n\n\nTexto\nImágenes\nAudio\nVideo\n…\n\n\n\n\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#datos-semiestructurados",
    "href": "posts/Presentaciones/Datos_1.html#datos-semiestructurados",
    "title": "Datos en el ámbito de la salud",
    "section": "Datos semiestructurados",
    "text": "Datos semiestructurados\n\n\n\n\n\n\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#por-naturaleza-de-los-datos",
    "href": "posts/Presentaciones/Datos_1.html#por-naturaleza-de-los-datos",
    "title": "Datos en el ámbito de la salud",
    "section": "Por naturaleza de los datos",
    "text": "Por naturaleza de los datos\n\nCategóricos\nNuméricos\nTemporales\nEspaciales\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#por-granularidad-de-los-datos",
    "href": "posts/Presentaciones/Datos_1.html#por-granularidad-de-los-datos",
    "title": "Datos en el ámbito de la salud",
    "section": "Por granularidad de los datos",
    "text": "Por granularidad de los datos\n\nDatos a nivel de instancia: Datos individuales que representan observaciones o eventos específicos, como registros de pacientes o imágenes médicas.\nDatos a nivel de atributo: Datos que representan características o atributos de las instancias, como la edad, el peso o los síntomas clínicos.\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#fuente-de-los-datos",
    "href": "posts/Presentaciones/Datos_1.html#fuente-de-los-datos",
    "title": "Datos en el ámbito de la salud",
    "section": "Fuente de los datos",
    "text": "Fuente de los datos\n\nDatos primarios: Datos recopilados directamente de la fuente original, como registros médicos electrónicos (EHR) o resultados de pruebas de laboratorio.\nDatos secundarios: Datos que han sido recopilados o procesados por terceros, como conjuntos de datos disponibles en repositorios públicos o bases de datos comerciales.\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#datos-anotados-vs-no-anotados",
    "href": "posts/Presentaciones/Datos_1.html#datos-anotados-vs-no-anotados",
    "title": "Datos en el ámbito de la salud",
    "section": "Datos anotados vs no anotados",
    "text": "Datos anotados vs no anotados\n\nEstructura semántica: Los datos anotados tienen metadatos adicionales que proporcionan contexto y significado, como las etiquetas que marcan entidades nombradas.\nInteroperabilidad y procesabilidad: Los datos anotados son más interoperables y procesables, lo que facilita su integración con otros sistemas y su procesamiento automatizado.\nFacilidad de búsqueda y recuperación: Los datos anotados son más fáciles de buscar y recuperar, ya que las anotaciones permiten identificar elementos específicos.\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#qué-es-la-anotación-de-datos",
    "href": "posts/Presentaciones/Datos_1.html#qué-es-la-anotación-de-datos",
    "title": "Datos en el ámbito de la salud",
    "section": "¿Qué es la anotación de datos?",
    "text": "¿Qué es la anotación de datos?\nLa anotación de datos es el proceso de atribución o etiquetado de datos para ayudar a los algoritmos de aprendizaje automático a comprender y clasificar la información que procesan.\n\nTexto\nImágenes\nAudio\nVideo"
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#anotación-de-imágenes",
    "href": "posts/Presentaciones/Datos_1.html#anotación-de-imágenes",
    "title": "Datos en el ámbito de la salud",
    "section": "Anotación de imágenes",
    "text": "Anotación de imágenes\n\n\n\nClasificación\nReconocimiento/Detección\nSegmentación\n\n\n\nVGG"
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#anotación-de-audio",
    "href": "posts/Presentaciones/Datos_1.html#anotación-de-audio",
    "title": "Datos en el ámbito de la salud",
    "section": "Anotación de audio",
    "text": "Anotación de audio"
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#anotación-de-video",
    "href": "posts/Presentaciones/Datos_1.html#anotación-de-video",
    "title": "Datos en el ámbito de la salud",
    "section": "Anotación de video",
    "text": "Anotación de video"
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#anotación-de-texto",
    "href": "posts/Presentaciones/Datos_1.html#anotación-de-texto",
    "title": "Datos en el ámbito de la salud",
    "section": "Anotación de texto",
    "text": "Anotación de texto\n\n\n\nSemántica\nIntención\nOpinión\n\ntabula"
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#antecedentes",
    "href": "posts/Presentaciones/Datos_1.html#antecedentes",
    "title": "Datos en el ámbito de la salud",
    "section": "Antecedentes",
    "text": "Antecedentes\n\n\n\n\n\n\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#problemas-de-la-historia-clínica-en-papel",
    "href": "posts/Presentaciones/Datos_1.html#problemas-de-la-historia-clínica-en-papel",
    "title": "Datos en el ámbito de la salud",
    "section": "Problemas de la historia clínica en papel",
    "text": "Problemas de la historia clínica en papel\n\nAccesibilidad limitada\nRiesgo de pérdida o daño\nDificultad para mantener actualizada la información\nEspacio de almacenamiento y organización\nLimitaciones en la compartición y colaboración\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#componentes-de-los-ehr",
    "href": "posts/Presentaciones/Datos_1.html#componentes-de-los-ehr",
    "title": "Datos en el ámbito de la salud",
    "section": "Componentes de los EHR",
    "text": "Componentes de los EHR\n\nMódulos de Registro y Documentación\nPedidos y Resultados de laboratorio\nGestión de Medicamentos\nIntegración de Imágenes Médicas (PACS)\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#sap-1",
    "href": "posts/Presentaciones/Datos_1.html#sap-1",
    "title": "Datos en el ámbito de la salud",
    "section": "SAP 1",
    "text": "SAP 1\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#sap-2",
    "href": "posts/Presentaciones/Datos_1.html#sap-2",
    "title": "Datos en el ámbito de la salud",
    "section": "SAP 2",
    "text": "SAP 2\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#sap-3",
    "href": "posts/Presentaciones/Datos_1.html#sap-3",
    "title": "Datos en el ámbito de la salud",
    "section": "SAP 3",
    "text": "SAP 3\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#aria",
    "href": "posts/Presentaciones/Datos_1.html#aria",
    "title": "Datos en el ámbito de la salud",
    "section": "ARIA",
    "text": "ARIA\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#limpieza-de-datos",
    "href": "posts/Presentaciones/Datos_1.html#limpieza-de-datos",
    "title": "Datos en el ámbito de la salud",
    "section": "Limpieza de datos",
    "text": "Limpieza de datos\n\nEliminación de observaciones incompletas\nImputación de valores\nAnálisis de patrones de datos faltantes\nUso de modelos predictivos\nTécnicas de imputación múltiple\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#normalización-de-variables-continuas",
    "href": "posts/Presentaciones/Datos_1.html#normalización-de-variables-continuas",
    "title": "Datos en el ámbito de la salud",
    "section": "Normalización de variables continuas",
    "text": "Normalización de variables continuas\n\nLas variables continuas pueden tener diferentes escalas y rangos de valores\nLa normalización ayuda a mitigar este problema al transformar las variables para que estén en una escala similar\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#técnicas-de-normalización",
    "href": "posts/Presentaciones/Datos_1.html#técnicas-de-normalización",
    "title": "Datos en el ámbito de la salud",
    "section": "Técnicas de normalización",
    "text": "Técnicas de normalización\n\nDatos con una media de 0 y desviación estándar 1\nNormalización min-max (0-1)\nPuede mejorar la convergencia\n\n\n\n\n\n\n\nAdvertencia\n\n\nEs importante aplicar la misma transformación de normalización tanto al conjunto de entrenamiento como al conjunto de prueba para evitar sesgos en los datos.\n\n\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#integración-de-datos",
    "href": "posts/Presentaciones/Datos_1.html#integración-de-datos",
    "title": "Datos en el ámbito de la salud",
    "section": "Integración de datos",
    "text": "Integración de datos\n\nFuentes de Datos Diversas\nResolución de inconsistencias\nLimpieza de Datos\nGestión de Datos Temporales\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#la-ingeniería-de-características",
    "href": "posts/Presentaciones/Datos_1.html#la-ingeniería-de-características",
    "title": "Datos en el ámbito de la salud",
    "section": "La ingeniería de características",
    "text": "La ingeniería de características\n\nCreación de Nuevas características\nSelección de Características Relevantes\n\n\nPor ejemplo el índice de masa corporal"
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#anonimización-de-los-datos",
    "href": "posts/Presentaciones/Datos_1.html#anonimización-de-los-datos",
    "title": "Datos en el ámbito de la salud",
    "section": "Anonimización de los datos",
    "text": "Anonimización de los datos\n\nLa anonimización es el proceso de eliminar o modificar datos personales de tal manera que ya no sea posible identificar a un individuo a partir de los datos.\nLa anonimización está vinculada a lo que dicen las leyes de protección de datos en la UE (RGPD)\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#técnicas-de-anonimización",
    "href": "posts/Presentaciones/Datos_1.html#técnicas-de-anonimización",
    "title": "Datos en el ámbito de la salud",
    "section": "Técnicas de anonimización",
    "text": "Técnicas de anonimización\n\nEliminación de identificadores directos\nPseudonimización\nEnmascaramiento\nAgregación\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#consideraciones-al-elegir-una-técnica-de-anonimización",
    "href": "posts/Presentaciones/Datos_1.html#consideraciones-al-elegir-una-técnica-de-anonimización",
    "title": "Datos en el ámbito de la salud",
    "section": "Consideraciones al elegir una técnica de anonimización",
    "text": "Consideraciones al elegir una técnica de anonimización\n\nEl tipo de datos que se procesan\nEl nivel de riesgo de reidentificación\nLos fines para los que se utilizarán los datos anonimizados\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#data-augmentation",
    "href": "posts/Presentaciones/Datos_1.html#data-augmentation",
    "title": "Datos en el ámbito de la salud",
    "section": "Data Augmentation",
    "text": "Data Augmentation\nEs una técnica que consiste en crear nuevas muestras de datos a partir de datos existentes. Estas nuevas muestras se generan mediante la aplicación de transformaciones predefinidas a los datos originales\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#ventajas-de-data-augmentation",
    "href": "posts/Presentaciones/Datos_1.html#ventajas-de-data-augmentation",
    "title": "Datos en el ámbito de la salud",
    "section": "Ventajas de Data Augmentation",
    "text": "Ventajas de Data Augmentation\n\nAumenta el tamaño del conjunto de datos\nMejora la generalización del modelo\nReduce el sesgo del modelo\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#data-augmentation-en-imágenes-médicas",
    "href": "posts/Presentaciones/Datos_1.html#data-augmentation-en-imágenes-médicas",
    "title": "Datos en el ámbito de la salud",
    "section": "Data Augmentation en imágenes médicas",
    "text": "Data Augmentation en imágenes médicas\n\nRotaciones\nZoom\nCambio de contraste\nAdición de ruido\nSimulación de artefactos\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#data-augmentation-en-señales-biomédicas",
    "href": "posts/Presentaciones/Datos_1.html#data-augmentation-en-señales-biomédicas",
    "title": "Datos en el ámbito de la salud",
    "section": "Data Augmentation en señales biomédicas",
    "text": "Data Augmentation en señales biomédicas\nComo el Electrocardiograma o el Electroencefalograma\n\nAdición de ruido\nCambio de escala\nSegmentación de señales\nSimulación de artefactos"
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#ángulo-entre-dos-vectores",
    "href": "posts/Presentaciones/Datos_1.html#ángulo-entre-dos-vectores",
    "title": "Datos en el ámbito de la salud",
    "section": "Ángulo entre dos vectores",
    "text": "Ángulo entre dos vectores"
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#qué-son-las-vectorizaciones",
    "href": "posts/Presentaciones/Datos_1.html#qué-son-las-vectorizaciones",
    "title": "Datos en el ámbito de la salud",
    "section": "¿Qué son las vectorizaciones?",
    "text": "¿Qué son las vectorizaciones?"
  },
  {
    "objectID": "posts/Presentaciones/Datos_1.html#práctica",
    "href": "posts/Presentaciones/Datos_1.html#práctica",
    "title": "Datos en el ámbito de la salud",
    "section": "Práctica",
    "text": "Práctica\nNotebook\njuego\n\n\n\nIAMED"
  },
  {
    "objectID": "posts/code/Embedding.html",
    "href": "posts/code/Embedding.html",
    "title": "Embeddings",
    "section": "",
    "text": "Colab\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom transformers import BertTokenizer, BertModel\nimport torch\nfrom scipy.spatial.distance import cosine\n\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.nn.functional as F\nimport random\nimport torchvision.models as models\n\nDefinimos vectores\n\nvector_1 = np.array([1, 2, 3])\nvector_2 = np.array([4, 5, 6])\nvector_3 = np.array([2, -1, 0])\nvector_4 = np.array([2, 4, 6])\n\nCalculamos la similitud del coseno de dos vectores\n\n\n\nSimilitud de dos vectores\n\n\n\ndef cosine_similarity(v1, v2):\n    dot_product = np.dot(v1, v2)\n    norm_v1 = np.linalg.norm(v1)\n    norm_v2 = np.linalg.norm(v2)\n    return dot_product / (norm_v1 * norm_v2)\n\n\nsimilarity_1_2 = cosine_similarity(vector_1, vector_2)\nsimilarity_1_3 = cosine_similarity(vector_1, vector_3)\nsimilarity_2_3 = cosine_similarity(vector_2, vector_3)\nsimilarity_1_4 = cosine_similarity(vector_1, vector_4)\n\nMostramos los resultados\n\nprint(\"Similitud de coseno entre vector_1 y vector_2:\", similarity_1_2)\nprint(\"Similitud de coseno entre vector_1 y vector_3:\", similarity_1_3)\nprint(\"Similitud de coseno entre vector_2 y vector_3:\", similarity_2_3)\nprint(\"Similitud de coseno entre vector_1 y vector_4:\", similarity_1_4)\n\nSimilitud de coseno entre vector_1 y vector_2: 0.9746318461970762\nSimilitud de coseno entre vector_1 y vector_3: 0.0\nSimilitud de coseno entre vector_2 y vector_3: 0.15289415743128765\nSimilitud de coseno entre vector_1 y vector_4: 1.0\n\n\nVamos ahora a por un modelo de texto.Cargamos el modelo\n\nmodel_name_bert = 'bert-base-uncased'\ntokenizer = BertTokenizer.from_pretrained(model_name_bert)\nmodel = BertModel.from_pretrained(model_name_bert)\n\n/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \nThe secret `HF_TOKEN` does not exist in your Colab secrets.\nTo authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\nYou will be able to reuse this secret in all of your notebooks.\nPlease note that authentication is recommended but still optional to access public models or datasets.\n  warnings.warn(\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndef cosine_similarity(v1, v2):\n    return 1 - cosine(v1, v2)\n\n\ndef get_bert_embeddings(text):\n    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n    with torch.no_grad():\n        outputs = model(**inputs)\n    embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n    return embeddings\n\n\ndef calculate_text_similarity(text1, text2):\n    \"\"\"\n    Función para calcular la similitud de coseno entre dos textos basada en sus representaciones vectoriales BERT.\n\n    Args:\n        text1 (str): Primer texto.\n        text2 (str): Segundo texto.\n\n    Returns:\n        float: Similitud de coseno entre las representaciones vectoriales de los dos textos.\n    \"\"\"\n    # Obtener las representaciones vectoriales BERT para ambos textos\n    embeddings_text1 = get_bert_embeddings(text1)\n    embeddings_text2 = get_bert_embeddings(text2)\n\n    # Calcular la similitud de coseno entre las representaciones vectoriales\n    similarity = cosine_similarity(embeddings_text1, embeddings_text2)\n    return similarity\n\n\n# Ejemplo de uso:\ntext1 = \"orange is a color\"\ntext2 = \"orange is a fruit\"\nsimilarity = calculate_text_similarity(text1, text2)\nprint(\"Similarity:\", similarity)\n\nSimilarity: 0.8144723176956177\n\n\nProbemos ahora con un modelo de imágenes\nPrimero bajamos las imágenes de nuestra querida base de datos MNIST\n\n# Transformación para las imágenes\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n])\n\n# Descargar la base de datos MNIST y aplicar transformaciones\ntrain_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\ntest_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n\n# Crear cargadores de datos para acceder a los datos por lotes\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=1, shuffle=True)\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=1, shuffle=True)\n\n\n\nDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\nExtracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\nExtracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\nExtracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\nExtracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n\n\n\n100%|██████████| 9912422/9912422 [00:00&lt;00:00, 175017306.71it/s]\n100%|██████████| 28881/28881 [00:00&lt;00:00, 26847449.87it/s]\n100%|██████████| 1648877/1648877 [00:00&lt;00:00, 43325866.23it/s]\n100%|██████████| 4542/4542 [00:00&lt;00:00, 4608255.63it/s]\n\n\n\ndef get_random_images(number1, number2, dataset=test_dataset, seed=42):\n    \"\"\"\n    Función para obtener dos imágenes al azar de un conjunto de datos para dos números específicos.\n\n    Args:\n        dataset: Conjunto de datos del que se seleccionarán las imágenes.\n        number1: Primer número para seleccionar imágenes.\n        number2: Segundo número para seleccionar imágenes.\n        seed: Semilla para reproducibilidad. Por defecto, 42.\n\n    Returns:\n        tuple: Tupla de dos imágenes y etiquetas seleccionadas al azar.\n    \"\"\"\n    # Obtener los índices de las imágenes para cada número\n    indices1 = [i for i, (image, label) in enumerate(dataset) if label == number1]\n    indices2 = [i for i, (image, label) in enumerate(dataset) if label == number2]\n\n    # Seleccionar dos índices al azar para cada número\n    random.seed(seed)\n    rand_idx1 = random.randint(0, len(indices1) - 1)\n    rand_idx2 = random.randint(0, len(indices2) - 1)\n\n    # Obtener los datos de las imágenes seleccionadas\n    image1, label1 = dataset[indices1[rand_idx1]]\n    image2, label2 = dataset[indices2[rand_idx2]]\n\n    return (image1, label1), (image2, label2)\n\ndef show_images(image_label1, image_label2):\n    \"\"\"\n    Función para mostrar dos imágenes junto con sus etiquetas en la misma ventana.\n\n    Args:\n        image_label1: Tupla que contiene la primera imagen y su etiqueta.\n        image_label2: Tupla que contiene la segunda imagen y su etiqueta.\n    \"\"\"\n    plt.figure(figsize=(8, 4))\n\n    plt.subplot(1, 2, 1)\n    plt.imshow(image_label1[0].squeeze(), cmap='gray')\n    plt.title('Label: {}'.format(image_label1[1]))\n    plt.axis('off')\n\n    plt.subplot(1, 2, 2)\n    plt.imshow(image_label2[0].squeeze(), cmap='gray')\n    plt.title('Label: {}'.format(image_label2[1]))\n    plt.axis('off')\n\n    plt.show()\n\n\ntupla1,tupla2=get_random_images(1,5)\n\n\nshow_images(tupla1,tupla2)\n\n\n\n\nCargamos un modelo genérico, en nuestro caso ResNet\n\n# Cargar el modelo pre-entrenado\nmodel_resnet = models.resnet18(pretrained=True)\n# Eliminar la capa de clasificación\nmodel_resnet = torch.nn.Sequential(*(list(model_resnet.children())[:-1]))\nmodel_resnet.eval()\n\nSequential(\n  (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (2): ReLU(inplace=True)\n  (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (5): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (6): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (7): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (8): AdaptiveAvgPool2d(output_size=(1, 1))\n)\n\n\n\ndef calculate_image_similarity(image_label1, image_label2, model=model_resnet):\n    \"\"\"\n    Función para calcular la similitud de coseno entre dos imágenes basadas en sus representaciones vectoriales.\n\n    Args:\n        image_label1: Tupla que contiene la primera imagen y su etiqueta.\n        image_label2: Tupla que contiene la segunda imagen y su etiqueta.\n        model: Modelo utilizado para obtener las representaciones vectoriales de las imágenes.\n\n    Returns:\n        float: Similitud de coseno entre las representaciones vectoriales de las dos imágenes.\n    \"\"\"\n    def get_image_embeddings(image_tensor):\n        with torch.no_grad():\n            # Expande la dimensión de los canales para que coincida con lo que el modelo espera (de 1 a 3 canales)\n            image_tensor = image_tensor.repeat(1, 3, 1, 1)\n            # Pasar la imagen a través del modelo y obtener la representación vectorial\n            output = model(image_tensor)\n            # Aplanar la salida\n            output = output.view(output.size(0), -1)\n            # Normalizar la salida\n            output = F.normalize(output, p=2, dim=1)\n        return output\n\n    # Obtener los tensores de imagen de las tuplas\n    image_tensor1, _ = image_label1\n    image_tensor2, _ = image_label2\n\n    # Obtener las representaciones vectoriales de las imágenes\n    image1_embedding = get_image_embeddings(image_tensor1.unsqueeze(0))\n    image2_embedding = get_image_embeddings(image_tensor2.unsqueeze(0))\n\n    # Calcular la similitud de coseno entre las representaciones vectoriales\n    similarity = F.cosine_similarity(image1_embedding, image2_embedding)\n    return similarity.item()\n\n\ncalculate_image_similarity(tupla1,tupla2)\n\n0.6484431028366089\n\n\nCargamos ahora un modelo finetuneado para el conjunto MNIST\n\ndef preprocess_input(input_tensor, target_size=(224, 224)):\n    \"\"\"\n    Función para preprocesar un tensor de entrada para que coincida con el tamaño y formato esperado por el modelo.\n\n    Args:\n        input_tensor (torch.Tensor): Tensor de entrada con un solo canal.\n        target_size (tuple): Tamaño al que se debe redimensionar la imagen. Por defecto, (224, 224).\n\n    Returns:\n        torch.Tensor: Tensor de entrada preprocesado con el tamaño y formato correctos.\n    \"\"\"\n    # Definir una transformación para redimensionar la imagen\n    resize_transform = transforms.Compose([\n        transforms.Resize(target_size),  # Redimensionar la imagen al tamaño objetivo\n    ])\n\n    # Redimensionar el tensor de entrada\n    input_tensor_resized = resize_transform(input_tensor)\n\n    # Replicar el canal único en tres canales\n    input_tensor_rgb = input_tensor_resized.expand(1, 3, *target_size)  # Expandir el tensor a tres canales\n\n    return input_tensor_rgb\n\n\ndef preprocess_and_infer(tupla1, tupla2, model=model_mnist2):\n    \"\"\"\n    Función para preprocesar dos imágenes de entrada y realizar inferencia con un modelo dado.\n\n    Args:\n        tupla1: Tupla que contiene la primera imagen y su etiqueta.\n        tupla2: Tupla que contiene la segunda imagen y su etiqueta.\n        model: Modelo utilizado para la inferencia.\n\n    Returns:\n        torch.Tensor: Salida del modelo para la primera imagen.\n        torch.Tensor: Salida del modelo para la segunda imagen.\n    \"\"\"\n    # Añadir un nuevo eje de canal a las imágenes y realizar la inferencia con el modelo\n    with torch.no_grad():\n        image1_tensor = preprocess_input(tupla1[0])\n        image2_tensor = preprocess_input(tupla2[0])\n        outputs1 = model(image1_tensor)\n        outputs2 = model(image2_tensor)\n\n    return F.cosine_similarity(outputs1.to_tuple()[0], outputs2.to_tuple()[0], dim=1)\n\n# Ejemplo de uso:\n# outputs1, outputs2 = preprocess_and_infer(tupla1, tupla2, model_mnist2)\n\n\npreprocess_and_infer(tupla1, tupla2)\n\n/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n  warnings.warn(\n\n\ntensor([-0.1767])"
  },
  {
    "objectID": "posts/code/PruebaMonai.html",
    "href": "posts/code/PruebaMonai.html",
    "title": "Servidor Monai",
    "section": "",
    "text": "Colab\n\n\nfrom google.colab import userdata\n\n\n!pip install pyngrok\n\nCollecting pyngrok\n  Downloading pyngrok-7.1.2-py3-none-any.whl (22 kB)\nRequirement already satisfied: PyYAML&gt;=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.1)\nInstalling collected packages: pyngrok\nSuccessfully installed pyngrok-7.1.2\n\n\n\nfrom pyngrok import ngrok\nngrok.set_auth_token(userdata.get('ngrok'))\nport = \"8000\"\n# Open a ngrok tunnel to the HTTP server\npublic_url = ngrok.connect(port).public_url\npublic_url\n\n\n\n\n'https://50d7-34-126-84-49.ngrok-free.app'\n\n\n\n# install MONAI Label\n!pip install monailabel\n\nCollecting monailabel\n  Downloading monailabel-0.8.1-202311030103-py3-none-any.whl (11.9 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.9/11.9 MB 8.6 MB/s eta 0:00:00\nCollecting bcrypt==4.0.1 (from monailabel)\n  Downloading bcrypt-4.0.1-cp36-abi3-manylinux_2_28_x86_64.whl (593 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 593.7/593.7 kB 46.7 MB/s eta 0:00:00\nCollecting cachetools==5.3.0 (from monailabel)\n  Downloading cachetools-5.3.0-py3-none-any.whl (9.3 kB)\nCollecting dicomweb-client==0.59.1 (from monailabel)\n  Downloading dicomweb_client-0.59.1-py3-none-any.whl (60 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 60.9/60.9 kB 8.7 MB/s eta 0:00:00\nCollecting einops&gt;=0.6.0 (from monailabel)\n  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 44.6/44.6 kB 5.8 MB/s eta 0:00:00\nCollecting expiring-dict==1.1.0 (from monailabel)\n  Downloading expiring_dict-1.1.0-py3-none-any.whl (3.6 kB)\nCollecting expiringdict==1.2.2 (from monailabel)\n  Downloading expiringdict-1.2.2-py3-none-any.whl (8.5 kB)\nCollecting fastapi==0.95.0 (from monailabel)\n  Downloading fastapi-0.95.0-py3-none-any.whl (57 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.1/57.1 kB 6.6 MB/s eta 0:00:00\nCollecting filelock==3.11.0 (from monailabel)\n  Downloading filelock-3.11.0-py3-none-any.whl (10.0 kB)\nCollecting girder-client==3.1.17 (from monailabel)\n  Downloading girder-client-3.1.17.tar.gz (20 kB)\n  Preparing metadata (setup.py) ... done\nCollecting httpx==0.23.3 (from monailabel)\n  Downloading httpx-0.23.3-py3-none-any.whl (71 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 71.5/71.5 kB 10.2 MB/s eta 0:00:00\nCollecting monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0 (from monailabel)\n  Downloading monai-1.3.0-202310121228-py3-none-any.whl (1.3 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 76.8 MB/s eta 0:00:00\nCollecting ninja==1.11.1 (from monailabel)\n  Downloading ninja-1.11.1-py2.py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (145 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 146.0/146.0 kB 20.0 MB/s eta 0:00:00\nCollecting numpymaxflow==0.0.6 (from monailabel)\n  Downloading numpymaxflow-0.0.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (51 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 51.6/51.6 kB 7.1 MB/s eta 0:00:00\nCollecting opencv-python-headless==4.7.0.72 (from monailabel)\n  Downloading opencv_python_headless-4.7.0.72-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.2 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 49.2/49.2 MB 19.1 MB/s eta 0:00:00\nCollecting passlib==1.7.4 (from monailabel)\n  Downloading passlib-1.7.4-py2.py3-none-any.whl (525 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 525.6/525.6 kB 57.5 MB/s eta 0:00:00\nRequirement already satisfied: pydantic&gt;=1.10.7 in /usr/local/lib/python3.10/dist-packages (from monailabel) (2.6.1)\nCollecting pydicom-seg==0.4.1 (from monailabel)\n  Downloading pydicom_seg-0.4.1-py3-none-any.whl (27 kB)\nCollecting pydicom==2.3.1 (from monailabel)\n  Downloading pydicom-2.3.1-py3-none-any.whl (2.0 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.0/2.0 MB 89.8 MB/s eta 0:00:00\nCollecting pynetdicom==2.0.2 (from monailabel)\n  Downloading pynetdicom-2.0.2-py3-none-any.whl (1.6 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.6/1.6 MB 86.1 MB/s eta 0:00:00\nCollecting pynrrd==1.0.0 (from monailabel)\n  Downloading pynrrd-1.0.0-py2.py3-none-any.whl (19 kB)\nCollecting python-dotenv==1.0.0 (from monailabel)\n  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\nCollecting python-jose[cryptography]==3.3.0 (from monailabel)\n  Downloading python_jose-3.3.0-py2.py3-none-any.whl (33 kB)\nCollecting python-multipart==0.0.6 (from monailabel)\n  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 45.7/45.7 kB 7.5 MB/s eta 0:00:00\nCollecting pyyaml==6.0 (from monailabel)\n  Downloading PyYAML-6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (682 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 682.2/682.2 kB 57.0 MB/s eta 0:00:00\nCollecting requests-toolbelt==0.10.1 (from monailabel)\n  Downloading requests_toolbelt-0.10.1-py2.py3-none-any.whl (54 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 54.5/54.5 kB 8.4 MB/s eta 0:00:00\nCollecting requests==2.28.2 (from monailabel)\n  Downloading requests-2.28.2-py3-none-any.whl (62 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.8/62.8 kB 8.3 MB/s eta 0:00:00\nCollecting schedule==1.1.0 (from monailabel)\n  Downloading schedule-1.1.0-py2.py3-none-any.whl (10 kB)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from monailabel) (1.2.2)\nRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from monailabel) (1.11.4)\nCollecting shapely==2.0.1 (from monailabel)\n  Downloading shapely-2.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.3/2.3 MB 96.7 MB/s eta 0:00:00\nCollecting timeloop==1.0.2 (from monailabel)\n  Downloading timeloop-1.0.2.tar.gz (2.9 kB)\n  Preparing metadata (setup.py) ... done\nCollecting uvicorn==0.21.1 (from monailabel)\n  Downloading uvicorn-0.21.1-py3-none-any.whl (57 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.8/57.8 kB 8.3 MB/s eta 0:00:00\nCollecting watchdog==3.0.0 (from monailabel)\n  Downloading watchdog-3.0.0-py3-none-manylinux2014_x86_64.whl (82 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 82.1/82.1 kB 11.5 MB/s eta 0:00:00\nRequirement already satisfied: numpy&gt;=1.19 in /usr/local/lib/python3.10/dist-packages (from dicomweb-client==0.59.1-&gt;monailabel) (1.25.2)\nCollecting retrying&gt;=1.3.3 (from dicomweb-client==0.59.1-&gt;monailabel)\n  Downloading retrying-1.3.4-py3-none-any.whl (11 kB)\nRequirement already satisfied: Pillow&gt;=8.3 in /usr/local/lib/python3.10/dist-packages (from dicomweb-client==0.59.1-&gt;monailabel) (9.4.0)\nRequirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from expiring-dict==1.1.0-&gt;monailabel) (2.4.0)\nCollecting pydantic&gt;=1.10.7 (from monailabel)\n  Downloading pydantic-1.10.14-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.1/3.1 MB 97.8 MB/s eta 0:00:00\nCollecting starlette&lt;0.27.0,&gt;=0.26.1 (from fastapi==0.95.0-&gt;monailabel)\n  Downloading starlette-0.26.1-py3-none-any.whl (66 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 66.9/66.9 kB 9.5 MB/s eta 0:00:00\nRequirement already satisfied: click&gt;=6.7 in /usr/local/lib/python3.10/dist-packages (from girder-client==3.1.17-&gt;monailabel) (8.1.7)\nCollecting diskcache (from girder-client==3.1.17-&gt;monailabel)\n  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 45.5/45.5 kB 6.4 MB/s eta 0:00:00\nRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx==0.23.3-&gt;monailabel) (2024.2.2)\nCollecting httpcore&lt;0.17.0,&gt;=0.15.0 (from httpx==0.23.3-&gt;monailabel)\n  Downloading httpcore-0.16.3-py3-none-any.whl (69 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 69.6/69.6 kB 10.2 MB/s eta 0:00:00\nCollecting rfc3986[idna2008]&lt;2,&gt;=1.3 (from httpx==0.23.3-&gt;monailabel)\n  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\nRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx==0.23.3-&gt;monailabel) (1.3.0)\nCollecting SimpleITK&gt;1.2.4 (from pydicom-seg==0.4.1-&gt;monailabel)\n  Downloading SimpleITK-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (52.7 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 52.7/52.7 MB 11.0 MB/s eta 0:00:00\nCollecting jsonschema&lt;4.0.0,&gt;=3.2.0 (from pydicom-seg==0.4.1-&gt;monailabel)\n  Downloading jsonschema-3.2.0-py2.py3-none-any.whl (56 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.3/56.3 kB 8.1 MB/s eta 0:00:00\nCollecting nptyping (from pynrrd==1.0.0-&gt;monailabel)\n  Downloading nptyping-2.5.0-py3-none-any.whl (37 kB)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from pynrrd==1.0.0-&gt;monailabel) (4.9.0)\nCollecting ecdsa!=0.15 (from python-jose[cryptography]==3.3.0-&gt;monailabel)\n  Downloading ecdsa-0.18.0-py2.py3-none-any.whl (142 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 142.9/142.9 kB 16.7 MB/s eta 0:00:00\nRequirement already satisfied: rsa in /usr/local/lib/python3.10/dist-packages (from python-jose[cryptography]==3.3.0-&gt;monailabel) (4.9)\nRequirement already satisfied: pyasn1 in /usr/local/lib/python3.10/dist-packages (from python-jose[cryptography]==3.3.0-&gt;monailabel) (0.5.1)\nRequirement already satisfied: cryptography&gt;=3.4.0 in /usr/local/lib/python3.10/dist-packages (from python-jose[cryptography]==3.3.0-&gt;monailabel) (42.0.3)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /usr/local/lib/python3.10/dist-packages (from requests==2.28.2-&gt;monailabel) (3.3.2)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.10/dist-packages (from requests==2.28.2-&gt;monailabel) (3.6)\nCollecting urllib3&lt;1.27,&gt;=1.21.1 (from requests==2.28.2-&gt;monailabel)\n  Downloading urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 143.8/143.8 kB 19.0 MB/s eta 0:00:00\nCollecting h11&gt;=0.8 (from uvicorn==0.21.1-&gt;monailabel)\n  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.3/58.3 kB 8.5 MB/s eta 0:00:00\nRequirement already satisfied: torch&gt;=1.9 in /usr/local/lib/python3.10/dist-packages (from monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (2.1.0+cu121)\nCollecting mlflow (from monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel)\n  Downloading mlflow-2.10.2-py3-none-any.whl (19.5 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.5/19.5 MB 45.7 MB/s eta 0:00:00\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (5.9.5)\nCollecting itk&gt;=5.2 (from monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel)\n  Downloading itk-5.3.0-cp310-cp310-manylinux_2_28_x86_64.whl (8.3 kB)\nCollecting pytorch-ignite==0.4.11 (from monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel)\n  Downloading pytorch_ignite-0.4.11-py3-none-any.whl (266 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 266.5/266.5 kB 31.6 MB/s eta 0:00:00\nRequirement already satisfied: scikit-image&gt;=0.14.2 in /usr/local/lib/python3.10/dist-packages (from monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (0.19.3)\nRequirement already satisfied: gdown&gt;=4.4.0 in /usr/local/lib/python3.10/dist-packages (from monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (4.7.3)\nCollecting openslide-python==1.1.2 (from monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel)\n  Downloading openslide-python-1.1.2.tar.gz (316 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 316.6/316.6 kB 36.7 MB/s eta 0:00:00\n  Preparing metadata (setup.py) ... done\nRequirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (2.15.2)\nCollecting lmdb (from monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel)\n  Downloading lmdb-1.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (299 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 299.2/299.2 kB 34.5 MB/s eta 0:00:00\nRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (0.16.0+cu121)\nRequirement already satisfied: tqdm&gt;=4.47.0 in /usr/local/lib/python3.10/dist-packages (from monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (4.66.2)\nRequirement already satisfied: nibabel in /usr/local/lib/python3.10/dist-packages (from monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (4.0.2)\nCollecting fire (from monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel)\n  Downloading fire-0.5.0.tar.gz (88 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 88.3/88.3 kB 11.8 MB/s eta 0:00:00\n  Preparing metadata (setup.py) ... done\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pytorch-ignite==0.4.11-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (23.2)\nRequirement already satisfied: joblib&gt;=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn-&gt;monailabel) (1.3.2)\nRequirement already satisfied: threadpoolctl&gt;=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn-&gt;monailabel) (3.3.0)\nRequirement already satisfied: cffi&gt;=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography&gt;=3.4.0-&gt;python-jose[cryptography]==3.3.0-&gt;monailabel) (1.16.0)\nRequirement already satisfied: six&gt;=1.9.0 in /usr/local/lib/python3.10/dist-packages (from ecdsa!=0.15-&gt;python-jose[cryptography]==3.3.0-&gt;monailabel) (1.16.0)\nRequirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown&gt;=4.4.0-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (2.31.0)\nRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown&gt;=4.4.0-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (4.12.3)\nRequirement already satisfied: anyio&lt;5.0,&gt;=3.0 in /usr/local/lib/python3.10/dist-packages (from httpcore&lt;0.17.0,&gt;=0.15.0-&gt;httpx==0.23.3-&gt;monailabel) (3.7.1)\nCollecting itk-core==5.3.0 (from itk&gt;=5.2-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel)\n  Downloading itk_core-5.3.0-cp310-cp310-manylinux_2_28_x86_64.whl (81.2 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 81.2/81.2 MB 9.4 MB/s eta 0:00:00\nCollecting itk-numerics==5.3.0 (from itk&gt;=5.2-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel)\n  Downloading itk_numerics-5.3.0-cp310-cp310-manylinux_2_28_x86_64.whl (58.8 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.8/58.8 MB 11.1 MB/s eta 0:00:00\nCollecting itk-io==5.3.0 (from itk&gt;=5.2-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel)\n  Downloading itk_io-5.3.0-cp310-cp310-manylinux_2_28_x86_64.whl (25.6 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 25.6/25.6 MB 59.7 MB/s eta 0:00:00\nCollecting itk-filtering==5.3.0 (from itk&gt;=5.2-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel)\n  Downloading itk_filtering-5.3.0-cp310-cp310-manylinux_2_28_x86_64.whl (73.5 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 73.5/73.5 MB 9.0 MB/s eta 0:00:00\nCollecting itk-registration==5.3.0 (from itk&gt;=5.2-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel)\n  Downloading itk_registration-5.3.0-cp310-cp310-manylinux_2_28_x86_64.whl (26.6 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 26.6/26.6 MB 42.1 MB/s eta 0:00:00\nCollecting itk-segmentation==5.3.0 (from itk&gt;=5.2-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel)\n  Downloading itk_segmentation-5.3.0-cp310-cp310-manylinux_2_28_x86_64.whl (16.5 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.5/16.5 MB 52.8 MB/s eta 0:00:00\nRequirement already satisfied: attrs&gt;=17.4.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema&lt;4.0.0,&gt;=3.2.0-&gt;pydicom-seg==0.4.1-&gt;monailabel) (23.2.0)\nCollecting pyrsistent&gt;=0.14.0 (from jsonschema&lt;4.0.0,&gt;=3.2.0-&gt;pydicom-seg==0.4.1-&gt;monailabel)\n  Downloading pyrsistent-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (117 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 117.7/117.7 kB 17.9 MB/s eta 0:00:00\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from jsonschema&lt;4.0.0,&gt;=3.2.0-&gt;pydicom-seg==0.4.1-&gt;monailabel) (67.7.2)\nRequirement already satisfied: networkx&gt;=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image&gt;=0.14.2-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (3.2.1)\nRequirement already satisfied: imageio&gt;=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image&gt;=0.14.2-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (2.31.6)\nRequirement already satisfied: tifffile&gt;=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image&gt;=0.14.2-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (2024.2.12)\nRequirement already satisfied: PyWavelets&gt;=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image&gt;=0.14.2-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (1.5.0)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch&gt;=1.9-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (1.12)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch&gt;=1.9-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (3.1.3)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch&gt;=1.9-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (2023.6.0)\nRequirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch&gt;=1.9-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (2.1.0)\nRequirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (2.4.0)\nRequirement already satisfied: cloudpickle&lt;4 in /usr/local/lib/python3.10/dist-packages (from mlflow-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (2.2.1)\nRequirement already satisfied: entrypoints&lt;1 in /usr/local/lib/python3.10/dist-packages (from mlflow-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (0.4)\nCollecting gitpython&lt;4,&gt;=2.1.0 (from mlflow-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel)\n  Downloading GitPython-3.1.42-py3-none-any.whl (195 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 195.4/195.4 kB 24.2 MB/s eta 0:00:00\nRequirement already satisfied: protobuf&lt;5,&gt;=3.12.0 in /usr/local/lib/python3.10/dist-packages (from mlflow-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (3.20.3)\nRequirement already satisfied: pytz&lt;2024 in /usr/local/lib/python3.10/dist-packages (from mlflow-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (2023.4)\nRequirement already satisfied: importlib-metadata!=4.7.0,&lt;8,&gt;=3.7.0 in /usr/local/lib/python3.10/dist-packages (from mlflow-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (7.0.1)\nRequirement already satisfied: sqlparse&lt;1,&gt;=0.4.0 in /usr/local/lib/python3.10/dist-packages (from mlflow-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (0.4.4)\nCollecting alembic!=1.10.0,&lt;2 (from mlflow-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel)\n  Downloading alembic-1.13.1-py3-none-any.whl (233 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 233.4/233.4 kB 31.5 MB/s eta 0:00:00\nCollecting docker&lt;8,&gt;=4.0.0 (from mlflow-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel)\n  Downloading docker-7.0.0-py3-none-any.whl (147 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 147.6/147.6 kB 21.2 MB/s eta 0:00:00\nRequirement already satisfied: Flask&lt;4 in /usr/local/lib/python3.10/dist-packages (from mlflow-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (2.2.5)\nRequirement already satisfied: pandas&lt;3 in /usr/local/lib/python3.10/dist-packages (from mlflow-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (1.5.3)\nCollecting querystring-parser&lt;2 (from mlflow-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel)\n  Downloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\nRequirement already satisfied: sqlalchemy&lt;3,&gt;=1.4.0 in /usr/local/lib/python3.10/dist-packages (from mlflow-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (2.0.27)\nRequirement already satisfied: pyarrow&lt;16,&gt;=4.0.0 in /usr/local/lib/python3.10/dist-packages (from mlflow-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (14.0.2)\nRequirement already satisfied: markdown&lt;4,&gt;=3.3 in /usr/local/lib/python3.10/dist-packages (from mlflow-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (3.5.2)\nRequirement already satisfied: matplotlib&lt;4 in /usr/local/lib/python3.10/dist-packages (from mlflow-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (3.7.1)\nCollecting gunicorn&lt;22 (from mlflow-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel)\n  Downloading gunicorn-21.2.0-py3-none-any.whl (80 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 80.2/80.2 kB 11.8 MB/s eta 0:00:00\nRequirement already satisfied: absl-py&gt;=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (1.4.0)\nRequirement already satisfied: grpcio&gt;=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (1.60.1)\nRequirement already satisfied: google-auth&lt;3,&gt;=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (2.27.0)\nRequirement already satisfied: google-auth-oauthlib&lt;2,&gt;=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (1.2.0)\nRequirement already satisfied: tensorboard-data-server&lt;0.8.0,&gt;=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (0.7.2)\nRequirement already satisfied: werkzeug&gt;=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (3.0.1)\nCollecting Mako (from alembic!=1.10.0,&lt;2-&gt;mlflow-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel)\n  Downloading Mako-1.3.2-py3-none-any.whl (78 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.7/78.7 kB 11.2 MB/s eta 0:00:00\nRequirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio&lt;5.0,&gt;=3.0-&gt;httpcore&lt;0.17.0,&gt;=0.15.0-&gt;httpx==0.23.3-&gt;monailabel) (1.2.0)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi&gt;=1.12-&gt;cryptography&gt;=3.4.0-&gt;python-jose[cryptography]==3.3.0-&gt;monailabel) (2.21)\nRequirement already satisfied: itsdangerous&gt;=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask&lt;4-&gt;mlflow-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (2.1.2)\nCollecting gitdb&lt;5,&gt;=4.0.1 (from gitpython&lt;4,&gt;=2.1.0-&gt;mlflow-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel)\n  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.7/62.7 kB 8.9 MB/s eta 0:00:00\nRequirement already satisfied: pyasn1-modules&gt;=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth&lt;3,&gt;=1.6.3-&gt;tensorboard-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (0.3.0)\nRequirement already satisfied: requests-oauthlib&gt;=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib&lt;2,&gt;=0.5-&gt;tensorboard-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (1.3.1)\nRequirement already satisfied: zipp&gt;=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata!=4.7.0,&lt;8,&gt;=3.7.0-&gt;mlflow-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (3.17.0)\nRequirement already satisfied: MarkupSafe&gt;=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2-&gt;torch&gt;=1.9-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (2.1.5)\nRequirement already satisfied: contourpy&gt;=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib&lt;4-&gt;mlflow-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (1.2.0)\nRequirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib&lt;4-&gt;mlflow-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (0.12.1)\nRequirement already satisfied: fonttools&gt;=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib&lt;4-&gt;mlflow-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (4.49.0)\nRequirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib&lt;4-&gt;mlflow-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (1.4.5)\nRequirement already satisfied: pyparsing&gt;=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib&lt;4-&gt;mlflow-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (3.1.1)\nRequirement already satisfied: python-dateutil&gt;=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib&lt;4-&gt;mlflow-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (2.8.2)\nRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy&lt;3,&gt;=1.4.0-&gt;mlflow-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (3.0.3)\nRequirement already satisfied: soupsieve&gt;1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4-&gt;gdown&gt;=4.4.0-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (2.5)\nINFO: pip is looking at multiple versions of requests[socks] to determine which version is compatible with other requirements. This could take a while.\nCollecting requests[socks] (from gdown&gt;=4.4.0-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel)\n  Downloading requests-2.30.0-py3-none-any.whl (62 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.5/62.5 kB 8.5 MB/s eta 0:00:00\n  Downloading requests-2.29.0-py3-none-any.whl (62 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.5/62.5 kB 9.2 MB/s eta 0:00:00\nRequirement already satisfied: PySocks!=1.5.7,&gt;=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests==2.28.2-&gt;monailabel) (1.7.1)\nRequirement already satisfied: mpmath&gt;=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy-&gt;torch&gt;=1.9-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (1.3.0)\nCollecting smmap&lt;6,&gt;=3.0.1 (from gitdb&lt;5,&gt;=4.0.1-&gt;gitpython&lt;4,&gt;=2.1.0-&gt;mlflow-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel)\n  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\nRequirement already satisfied: oauthlib&gt;=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib&gt;=0.7.0-&gt;google-auth-oauthlib&lt;2,&gt;=0.5-&gt;tensorboard-&gt;monai[fire,gdown,ignite,itk,lmdb,mlflow,nibabel,openslide,pillow,psutil,skimage,tensorboard,torchvision,tqdm]&gt;=1.3.0-&gt;monailabel) (3.2.2)\nBuilding wheels for collected packages: girder-client, timeloop, openslide-python, fire\n  Building wheel for girder-client (setup.py) ... done\n  Created wheel for girder-client: filename=girder_client-3.1.17-py3-none-any.whl size=21171 sha256=062009bed2af6eedc07541fde059c47b66d0b635608c8140ae1f30c9435d11c0\n  Stored in directory: /root/.cache/pip/wheels/1c/88/79/eb68de788a8c7a6cf3d101905b31a05dc69aa9ac212f5db922\n  Building wheel for timeloop (setup.py) ... done\n  Created wheel for timeloop: filename=timeloop-1.0.2-py3-none-any.whl size=3702 sha256=67f0c94dc264d6286b729b4a635e7a8613a027fd59bf2ba771d04ad29549bdb9\n  Stored in directory: /root/.cache/pip/wheels/8b/df/32/f72b9fd897c185cd70103331f70e4cb66e3df1de24bd476548\n  Building wheel for openslide-python (setup.py) ... done\n  Created wheel for openslide-python: filename=openslide_python-1.1.2-cp310-cp310-linux_x86_64.whl size=22698 sha256=cec9573e1a7f27acc170782361b3f6c726d05bfb6af42859a57b68e5ba31c6d4\n  Stored in directory: /root/.cache/pip/wheels/93/ac/a7/517e63825943228057758b7f42d0c238290fc21e2a53b1043e\n  Building wheel for fire (setup.py) ... done\n  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116934 sha256=5206dc513f9b778aad8796bdbb9353131f26c330a86036f82a534025a7de7467\n  Stored in directory: /root/.cache/pip/wheels/90/d4/f7/9404e5db0116bd4d43e5666eaa3e70ab53723e1e3ea40c9a95\nSuccessfully built girder-client timeloop openslide-python fire\nInstalling collected packages: timeloop, SimpleITK, rfc3986, passlib, ninja, lmdb, expiringdict, watchdog, urllib3, smmap, shapely, schedule, retrying, querystring-parser, pyyaml, python-multipart, python-dotenv, pyrsistent, pydicom, pydantic, openslide-python, opencv-python-headless, numpymaxflow, nptyping, Mako, itk-core, h11, gunicorn, fire, filelock, expiring-dict, einops, ecdsa, diskcache, cachetools, bcrypt, uvicorn, starlette, requests, python-jose, pynrrd, pynetdicom, jsonschema, itk-numerics, itk-io, httpcore, gitdb, alembic, requests-toolbelt, pydicom-seg, itk-filtering, httpx, gitpython, fastapi, docker, dicomweb-client, pytorch-ignite, monai, mlflow, itk-segmentation, itk-registration, girder-client, itk, monailabel\n  Attempting uninstall: urllib3\n    Found existing installation: urllib3 2.0.7\n    Uninstalling urllib3-2.0.7:\n      Successfully uninstalled urllib3-2.0.7\n  Attempting uninstall: shapely\n    Found existing installation: shapely 2.0.3\n    Uninstalling shapely-2.0.3:\n      Successfully uninstalled shapely-2.0.3\n  Attempting uninstall: pyyaml\n    Found existing installation: PyYAML 6.0.1\n    Uninstalling PyYAML-6.0.1:\n      Successfully uninstalled PyYAML-6.0.1\n  Attempting uninstall: pydantic\n    Found existing installation: pydantic 2.6.1\n    Uninstalling pydantic-2.6.1:\n      Successfully uninstalled pydantic-2.6.1\n  Attempting uninstall: opencv-python-headless\n    Found existing installation: opencv-python-headless 4.9.0.80\n    Uninstalling opencv-python-headless-4.9.0.80:\n      Successfully uninstalled opencv-python-headless-4.9.0.80\n  Attempting uninstall: filelock\n    Found existing installation: filelock 3.13.1\n    Uninstalling filelock-3.13.1:\n      Successfully uninstalled filelock-3.13.1\n  Attempting uninstall: cachetools\n    Found existing installation: cachetools 5.3.2\n    Uninstalling cachetools-5.3.2:\n      Successfully uninstalled cachetools-5.3.2\n  Attempting uninstall: requests\n    Found existing installation: requests 2.31.0\n    Uninstalling requests-2.31.0:\n      Successfully uninstalled requests-2.31.0\n  Attempting uninstall: jsonschema\n    Found existing installation: jsonschema 4.19.2\n    Uninstalling jsonschema-4.19.2:\n      Successfully uninstalled jsonschema-4.19.2\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-colab 1.0.0 requires requests==2.31.0, but you have requests 2.28.2 which is incompatible.\nyfinance 0.2.36 requires requests&gt;=2.31, but you have requests 2.28.2 which is incompatible.\nSuccessfully installed Mako-1.3.2 SimpleITK-2.3.1 alembic-1.13.1 bcrypt-4.0.1 cachetools-5.3.0 dicomweb-client-0.59.1 diskcache-5.6.3 docker-7.0.0 ecdsa-0.18.0 einops-0.7.0 expiring-dict-1.1.0 expiringdict-1.2.2 fastapi-0.95.0 filelock-3.11.0 fire-0.5.0 girder-client-3.1.17 gitdb-4.0.11 gitpython-3.1.42 gunicorn-21.2.0 h11-0.14.0 httpcore-0.16.3 httpx-0.23.3 itk-5.3.0 itk-core-5.3.0 itk-filtering-5.3.0 itk-io-5.3.0 itk-numerics-5.3.0 itk-registration-5.3.0 itk-segmentation-5.3.0 jsonschema-3.2.0 lmdb-1.4.1 mlflow-2.10.2 monai-1.3.0 monailabel-0.8.1 ninja-1.11.1 nptyping-2.5.0 numpymaxflow-0.0.6 opencv-python-headless-4.7.0.72 openslide-python-1.1.2 passlib-1.7.4 pydantic-1.10.14 pydicom-2.3.1 pydicom-seg-0.4.1 pynetdicom-2.0.2 pynrrd-1.0.0 pyrsistent-0.20.0 python-dotenv-1.0.0 python-jose-3.3.0 python-multipart-0.0.6 pytorch-ignite-0.4.11 pyyaml-6.0 querystring-parser-1.2.4 requests-2.28.2 requests-toolbelt-0.10.1 retrying-1.3.4 rfc3986-1.5.0 schedule-1.1.0 shapely-2.0.1 smmap-5.0.1 starlette-0.26.1 timeloop-1.0.2 urllib3-1.26.18 uvicorn-0.21.1 watchdog-3.0.0\n\n\nUnable to display output for mime type(s): application/vnd.colab-display-data+json\n\n\n\n# download Radiology sample app to local directory\n!monailabel apps --name radiology --download --output .\n\nUsing PYTHONPATH=/usr:/env/python\n\n2024-02-24 17:28:35.241855: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-02-24 17:28:35.241925: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-02-24 17:28:35.243427: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2024-02-24 17:28:36.519537: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\nradiology is copied at: /content/radiology\n\n\n\n# download Task 2 MSD dataset\n!monailabel datasets --download --name Task09_Spleen --output .\n\nUsing PYTHONPATH=/usr:/env/python\n\n2024-02-24 17:28:54.798816: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-02-24 17:28:54.798878: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-02-24 17:28:54.800232: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2024-02-24 17:28:55.888070: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\nTask09_Spleen.tar: 1.50GB [01:39, 16.2MB/s]                \n2024-02-24 17:30:36,412 - INFO - Downloaded: Task09_Spleen.tar\n2024-02-24 17:30:39,553 - INFO - Verified 'Task09_Spleen.tar', md5: 410d4a301da4e5b2f6f86ec3ddba524e.\n2024-02-24 17:30:39,553 - INFO - Writing into directory: /content.\nTask09_Spleen is downloaded at: ./Task09_Spleen\n\n\n\n# start the Radiology app in MONAI label server\n# and start annotating the downloaded images using deepedit model\n!monailabel start_server --app radiology --studies Task09_Spleen/imagesTr --conf models deepedit\n\nUsing PYTHONPATH=/usr:/env/python\n\n2024-02-24 17:31:04.670173: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-02-24 17:31:04.670226: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-02-24 17:31:04.671731: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2024-02-24 17:31:05.799085: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n[2024-02-24 17:31:06,936] [4401] [MainThread] [INFO] (__main__:285) - USING:: version = False\n[2024-02-24 17:31:06,936] [4401] [MainThread] [INFO] (__main__:285) - USING:: app = /content/radiology\n[2024-02-24 17:31:06,936] [4401] [MainThread] [INFO] (__main__:285) - USING:: studies = /content/Task09_Spleen/imagesTr\n[2024-02-24 17:31:06,936] [4401] [MainThread] [INFO] (__main__:285) - USING:: verbose = INFO\n[2024-02-24 17:31:06,936] [4401] [MainThread] [INFO] (__main__:285) - USING:: conf = [['models', 'deepedit']]\n[2024-02-24 17:31:06,936] [4401] [MainThread] [INFO] (__main__:285) - USING:: host = 0.0.0.0\n[2024-02-24 17:31:06,936] [4401] [MainThread] [INFO] (__main__:285) - USING:: port = 8000\n[2024-02-24 17:31:06,937] [4401] [MainThread] [INFO] (__main__:285) - USING:: uvicorn_app = monailabel.app:app\n[2024-02-24 17:31:06,937] [4401] [MainThread] [INFO] (__main__:285) - USING:: ssl_keyfile = None\n[2024-02-24 17:31:06,937] [4401] [MainThread] [INFO] (__main__:285) - USING:: ssl_certfile = None\n[2024-02-24 17:31:06,937] [4401] [MainThread] [INFO] (__main__:285) - USING:: ssl_keyfile_password = None\n[2024-02-24 17:31:06,937] [4401] [MainThread] [INFO] (__main__:285) - USING:: ssl_ca_certs = None\n[2024-02-24 17:31:06,937] [4401] [MainThread] [INFO] (__main__:285) - USING:: workers = None\n[2024-02-24 17:31:06,937] [4401] [MainThread] [INFO] (__main__:285) - USING:: limit_concurrency = None\n[2024-02-24 17:31:06,937] [4401] [MainThread] [INFO] (__main__:285) - USING:: access_log = False\n[2024-02-24 17:31:06,937] [4401] [MainThread] [INFO] (__main__:285) - USING:: root_path = /\n[2024-02-24 17:31:06,937] [4401] [MainThread] [INFO] (__main__:285) - USING:: log_level = info\n[2024-02-24 17:31:06,937] [4401] [MainThread] [INFO] (__main__:285) - USING:: log_config = None\n[2024-02-24 17:31:06,937] [4401] [MainThread] [INFO] (__main__:285) - USING:: dryrun = False\n[2024-02-24 17:31:06,937] [4401] [MainThread] [INFO] (__main__:285) - USING:: action = start_server\n[2024-02-24 17:31:06,937] [4401] [MainThread] [INFO] (__main__:296) - \nAllow Origins: ['*']\n[2024-02-24 17:31:07,714] [4401] [MainThread] [INFO] (uvicorn.error:74) - Started server process [4401]\n[2024-02-24 17:31:07,715] [4401] [MainThread] [INFO] (uvicorn.error:48) - Waiting for application startup.\n[2024-02-24 17:31:07,715] [4401] [MainThread] [INFO] (monailabel.interfaces.utils.app:37) - Initializing App from: /content/radiology; studies: /content/Task09_Spleen/imagesTr; conf: {'models': 'deepedit'}\n[2024-02-24 17:31:07,885] [4401] [MainThread] [INFO] (monailabel.utils.others.class_utils:57) - Subclass for MONAILabelApp Found: &lt;class 'main.MyApp'&gt;\n[2024-02-24 17:31:07,899] [4401] [MainThread] [INFO] (monailabel.utils.others.class_utils:57) - Subclass for TaskConfig Found: &lt;class 'lib.configs.localization_vertebra.LocalizationVertebra'&gt;\n[2024-02-24 17:31:07,902] [4401] [MainThread] [INFO] (monailabel.utils.others.class_utils:57) - Subclass for TaskConfig Found: &lt;class 'lib.configs.segmentation_spleen.SegmentationSpleen'&gt;\n[2024-02-24 17:31:07,903] [4401] [MainThread] [INFO] (monailabel.utils.others.class_utils:57) - Subclass for TaskConfig Found: &lt;class 'lib.configs.segmentation.Segmentation'&gt;\n[2024-02-24 17:31:07,904] [4401] [MainThread] [INFO] (monailabel.utils.others.class_utils:57) - Subclass for TaskConfig Found: &lt;class 'lib.configs.deepgrow_2d.Deepgrow2D'&gt;\n[2024-02-24 17:31:07,905] [4401] [MainThread] [INFO] (monailabel.utils.others.class_utils:57) - Subclass for TaskConfig Found: &lt;class 'lib.configs.segmentation_vertebra.SegmentationVertebra'&gt;\n[2024-02-24 17:31:07,907] [4401] [MainThread] [INFO] (monailabel.utils.others.class_utils:57) - Subclass for TaskConfig Found: &lt;class 'lib.configs.deepgrow_3d.Deepgrow3D'&gt;\n[2024-02-24 17:31:07,908] [4401] [MainThread] [INFO] (monailabel.utils.others.class_utils:57) - Subclass for TaskConfig Found: &lt;class 'lib.configs.deepedit.DeepEdit'&gt;\n[2024-02-24 17:31:07,910] [4401] [MainThread] [INFO] (monailabel.utils.others.class_utils:57) - Subclass for TaskConfig Found: &lt;class 'lib.configs.localization_spine.LocalizationSpine'&gt;\n[2024-02-24 17:31:07,910] [4401] [MainThread] [INFO] (main:93) - +++ Adding Model: deepedit =&gt; lib.configs.deepedit.DeepEdit\n[2024-02-24 17:31:07,910] [4401] [MainThread] [INFO] (monailabel.utils.others.generic:187) - Downloading resource: /content/radiology/model/pretrained_deepedit_dynunet.pt from https://github.com/Project-MONAI/MONAILabel/releases/download/pretrained/radiology_deepedit_dynunet_multilabel.pt\npretrained_deepedit_dynunet.pt: 118MB [00:09, 13.6MB/s]               \n2024-02-24 17:31:17,041 - INFO - Downloaded: /content/radiology/model/pretrained_deepedit_dynunet.pt\n2024-02-24 17:31:17,042 - INFO - Expected md5 is None, skip md5 check for file /content/radiology/model/pretrained_deepedit_dynunet.pt.\n[2024-02-24 17:31:19,102] [4401] [MainThread] [INFO] (lib.configs.deepedit:141) - EPISTEMIC Enabled: False; Samples: 5\n[2024-02-24 17:31:19,102] [4401] [MainThread] [INFO] (main:96) - +++ Using Models: ['deepedit']\n[2024-02-24 17:31:19,102] [4401] [MainThread] [INFO] (monailabel.interfaces.app:135) - Init Datastore for: /content/Task09_Spleen/imagesTr\n[2024-02-24 17:31:19,103] [4401] [MainThread] [INFO] (monailabel.datastore.local:130) - Auto Reload: True; Extensions: ['*.nii.gz', '*.nii', '*.nrrd', '*.jpg', '*.png', '*.tif', '*.svs', '*.xml']\n[2024-02-24 17:31:19,104] [4401] [MainThread] [INFO] (monailabel.datastore.local:594) - Adding New Image: spleen_31 =&gt; spleen_31.nii.gz\n[2024-02-24 17:31:19,105] [4401] [MainThread] [INFO] (monailabel.datastore.local:594) - Adding New Image: spleen_14 =&gt; spleen_14.nii.gz\n[2024-02-24 17:31:19,105] [4401] [MainThread] [INFO] (monailabel.datastore.local:594) - Adding New Image: spleen_18 =&gt; spleen_18.nii.gz\n[2024-02-24 17:31:19,105] [4401] [MainThread] [INFO] (monailabel.datastore.local:594) - Adding New Image: spleen_16 =&gt; spleen_16.nii.gz\n[2024-02-24 17:31:19,105] [4401] [MainThread] [INFO] (monailabel.datastore.local:594) - Adding New Image: spleen_9 =&gt; spleen_9.nii.gz\n[2024-02-24 17:31:19,105] [4401] [MainThread] [INFO] (monailabel.datastore.local:594) - Adding New Image: spleen_53 =&gt; spleen_53.nii.gz\n[2024-02-24 17:31:19,105] [4401] [MainThread] [INFO] (monailabel.datastore.local:594) - Adding New Image: spleen_44 =&gt; spleen_44.nii.gz\n[2024-02-24 17:31:19,106] [4401] [MainThread] [INFO] (monailabel.datastore.local:594) - Adding New Image: spleen_60 =&gt; spleen_60.nii.gz\n[2024-02-24 17:31:19,106] [4401] [MainThread] [INFO] (monailabel.datastore.local:594) - Adding New Image: spleen_12 =&gt; spleen_12.nii.gz\n[2024-02-24 17:31:19,106] [4401] [MainThread] [INFO] (monailabel.datastore.local:594) - Adding New Image: spleen_32 =&gt; spleen_32.nii.gz\n[2024-02-24 17:31:19,106] [4401] [MainThread] [INFO] (monailabel.datastore.local:594) - Adding New Image: spleen_20 =&gt; spleen_20.nii.gz\n[2024-02-24 17:31:19,106] [4401] [MainThread] [INFO] (monailabel.datastore.local:594) - Adding New Image: spleen_41 =&gt; spleen_41.nii.gz\n[2024-02-24 17:31:19,107] [4401] [MainThread] [INFO] (monailabel.datastore.local:594) - Adding New Image: spleen_40 =&gt; spleen_40.nii.gz\n[2024-02-24 17:31:19,107] [4401] [MainThread] [INFO] (monailabel.datastore.local:594) - Adding New Image: spleen_63 =&gt; spleen_63.nii.gz\n[2024-02-24 17:31:19,107] [4401] [MainThread] [INFO] (monailabel.datastore.local:594) - Adding New Image: spleen_61 =&gt; spleen_61.nii.gz\n[2024-02-24 17:31:19,107] [4401] [MainThread] [INFO] (monailabel.datastore.local:594) - Adding New Image: spleen_56 =&gt; spleen_56.nii.gz\n[2024-02-24 17:31:19,107] [4401] [MainThread] [INFO] (monailabel.datastore.local:594) - Adding New Image: spleen_52 =&gt; spleen_52.nii.gz\n[2024-02-24 17:31:19,107] [4401] [MainThread] [INFO] (monailabel.datastore.local:594) - Adding New Image: spleen_27 =&gt; spleen_27.nii.gz\n[2024-02-24 17:31:19,108] [4401] [MainThread] [INFO] (monailabel.datastore.local:594) - Adding New Image: spleen_45 =&gt; spleen_45.nii.gz\n[2024-02-24 17:31:19,108] [4401] [MainThread] [INFO] (monailabel.datastore.local:594) - Adding New Image: spleen_21 =&gt; spleen_21.nii.gz\n[2024-02-24 17:31:19,108] [4401] [MainThread] [INFO] (monailabel.datastore.local:594) - Adding New Image: spleen_33 =&gt; spleen_33.nii.gz\n[2024-02-24 17:31:19,108] [4401] [MainThread] [INFO] (monailabel.datastore.local:594) - Adding New Image: spleen_10 =&gt; spleen_10.nii.gz\n[2024-02-24 17:31:19,108] [4401] [MainThread] [INFO] (monailabel.datastore.local:594) - Adding New Image: spleen_62 =&gt; spleen_62.nii.gz\n[2024-02-24 17:31:19,108] [4401] [MainThread] [INFO] (monailabel.datastore.local:594) - Adding New Image: spleen_46 =&gt; spleen_46.nii.gz\n[2024-02-24 17:31:19,109] [4401] [MainThread] [INFO] (monailabel.datastore.local:594) - Adding New Image: spleen_19 =&gt; spleen_19.nii.gz\n[2024-02-24 17:31:19,109] [4401] [MainThread] [INFO] (monailabel.datastore.local:594) - Adding New Image: spleen_25 =&gt; spleen_25.nii.gz\n[2024-02-24 17:31:19,109] [4401] [MainThread] [INFO] (monailabel.datastore.local:594) - Adding New Image: spleen_2 =&gt; spleen_2.nii.gz\n[2024-02-24 17:31:19,109] [4401] [MainThread] [INFO] (monailabel.datastore.local:594) - Adding New Image: spleen_26 =&gt; spleen_26.nii.gz\n[2024-02-24 17:31:19,109] [4401] [MainThread] [INFO] (monailabel.datastore.local:594) - Adding New Image: spleen_6 =&gt; spleen_6.nii.gz\n[2024-02-24 17:31:19,109] [4401] [MainThread] [INFO] (monailabel.datastore.local:594) - Adding New Image: spleen_22 =&gt; spleen_22.nii.gz\n[2024-02-24 17:31:19,110] [4401] [MainThread] [INFO] (monailabel.datastore.local:594) - Adding New Image: spleen_49 =&gt; spleen_49.nii.gz\n[2024-02-24 17:31:19,110] [4401] [MainThread] [INFO] (monailabel.datastore.local:594) - Adding New Image: spleen_28 =&gt; spleen_28.nii.gz\n[2024-02-24 17:31:19,110] [4401] [MainThread] [INFO] (monailabel.datastore.local:594) - Adding New Image: spleen_47 =&gt; spleen_47.nii.gz\n[2024-02-24 17:31:19,110] [4401] [MainThread] [INFO] (monailabel.datastore.local:594) - Adding New Image: spleen_29 =&gt; spleen_29.nii.gz\n[2024-02-24 17:31:19,110] [4401] [MainThread] [INFO] (monailabel.datastore.local:594) - Adding New Image: spleen_8 =&gt; spleen_8.nii.gz\n[2024-02-24 17:31:19,110] [4401] [MainThread] [INFO] (monailabel.datastore.local:594) - Adding New Image: spleen_24 =&gt; spleen_24.nii.gz\n[2024-02-24 17:31:19,111] [4401] [MainThread] [INFO] (monailabel.datastore.local:594) - Adding New Image: spleen_59 =&gt; spleen_59.nii.gz\n[2024-02-24 17:31:19,111] [4401] [MainThread] [INFO] (monailabel.datastore.local:594) - Adding New Image: spleen_17 =&gt; spleen_17.nii.gz\n[2024-02-24 17:31:19,111] [4401] [MainThread] [INFO] (monailabel.datastore.local:594) - Adding New Image: spleen_3 =&gt; spleen_3.nii.gz\n[2024-02-24 17:31:19,111] [4401] [MainThread] [INFO] (monailabel.datastore.local:594) - Adding New Image: spleen_38 =&gt; spleen_38.nii.gz\n[2024-02-24 17:31:19,111] [4401] [MainThread] [INFO] (monailabel.datastore.local:594) - Adding New Image: spleen_13 =&gt; spleen_13.nii.gz\n[2024-02-24 17:31:19,113] [4401] [MainThread] [INFO] (monailabel.datastore.local:577) - Invalidate count: 41\n[2024-02-24 17:31:19,116] [4401] [MainThread] [INFO] (monailabel.datastore.local:151) - Start observing external modifications on datastore (AUTO RELOAD)\n[2024-02-24 17:31:19,143] [4401] [MainThread] [INFO] (main:126) - +++ Adding Inferer:: deepedit =&gt; &lt;lib.infers.deepedit.DeepEdit object at 0x79df9887a830&gt;\n[2024-02-24 17:31:19,143] [4401] [MainThread] [INFO] (main:126) - +++ Adding Inferer:: deepedit_seg =&gt; &lt;lib.infers.deepedit.DeepEdit object at 0x79df988613c0&gt;\n[2024-02-24 17:31:19,143] [4401] [MainThread] [INFO] (main:191) - {'deepedit': &lt;lib.infers.deepedit.DeepEdit object at 0x79df9887a830&gt;, 'deepedit_seg': &lt;lib.infers.deepedit.DeepEdit object at 0x79df988613c0&gt;, 'Histogram+GraphCut': &lt;monailabel.scribbles.infer.HistogramBasedGraphCut object at 0x79df98860700&gt;, 'GMM+GraphCut': &lt;monailabel.scribbles.infer.GMMBasedGraphCut object at 0x79df98862dd0&gt;}\n[2024-02-24 17:31:19,143] [4401] [MainThread] [INFO] (main:206) - +++ Adding Trainer:: deepedit =&gt; &lt;lib.trainers.deepedit.DeepEdit object at 0x79df98860580&gt;\n[2024-02-24 17:31:19,144] [4401] [MainThread] [INFO] (monailabel.utils.sessions:51) - Session Path: /root/.cache/monailabel/sessions\n[2024-02-24 17:31:19,144] [4401] [MainThread] [INFO] (monailabel.utils.sessions:52) - Session Expiry (max): 3600\n[2024-02-24 17:31:19,144] [4401] [MainThread] [INFO] (monailabel.interfaces.app:469) - App Init - completed\n[2024-02-24 17:31:19,145] [timeloop] [INFO] Starting Timeloop..\n[2024-02-24 17:31:19,145] [4401] [MainThread] [INFO] (timeloop:60) - Starting Timeloop..\n[2024-02-24 17:31:19,145] [timeloop] [INFO] Registered job &lt;function MONAILabelApp.on_init_complete.&lt;locals&gt;.run_scheduler at 0x79df983080d0&gt;\n[2024-02-24 17:31:19,145] [4401] [MainThread] [INFO] (timeloop:42) - Registered job &lt;function MONAILabelApp.on_init_complete.&lt;locals&gt;.run_scheduler at 0x79df983080d0&gt;\n[2024-02-24 17:31:19,145] [timeloop] [INFO] Timeloop now started. Jobs will run based on the interval set\n[2024-02-24 17:31:19,145] [4401] [MainThread] [INFO] (timeloop:63) - Timeloop now started. Jobs will run based on the interval set\n[2024-02-24 17:31:19,145] [4401] [MainThread] [INFO] (uvicorn.error:62) - Application startup complete.\n[2024-02-24 17:31:19,146] [4401] [MainThread] [INFO] (uvicorn.error:217) - Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Intel·ligència artificial en medicina",
    "section": "",
    "text": "Clasificador de Neumonia (ViT)\n\n\n\n\n\n\n\nColab\n\n\n\n\n\n\n\n\n\n\n\n26 feb 2024\n\n\n\n\n\n\n  \n\n\n\n\nServidor Monai\n\n\n\n\n\n\n\nColab\n\n\n\n\n\n\n\n\n\n\n\n26 feb 2024\n\n\n\n\n\n\n  \n\n\n\n\nU-NET\n\n\n\n\n\n\n\nColab\n\n\n\n\n\n\n\n\n\n\n\n26 feb 2024\n\n\n\n\n\n\n  \n\n\n\n\nCNN_Neumonía\n\n\n\n\n\n\n\nColab\n\n\n\n\n\n\n\n\n\n\n\n26 feb 2024\n\n\n\n\n\n\n  \n\n\n\n\nInteligencia artificial en imagen médica\n\n\n\n\n\n\n\nPresentación\n\n\n\n\n\n\n\n\n\n\n\n26 feb 2024\n\n\nAntonio Otal Palacín\n\n\n\n\n\n\n  \n\n\n\n\nIntroducción a la imagen médica\n\n\n\n\n\n\n\nPresentación\n\n\n\n\n\n\n\n\n\n\n\n22 feb 2024\n\n\nAntonio Otal Palacín\n\n\n\n\n\n\n  \n\n\n\n\nEmbeddings\n\n\n\n\n\n\n\nColab\n\n\n\n\n\n\n\n\n\n\n\n19 feb 2024\n\n\n\n\n\n\n  \n\n\n\n\nDatos en el ámbito de la salud\n\n\n\n\n\n\n\nPresentación\n\n\n\n\n\n\n\n\n\n\n\n19 feb 2024\n\n\nAntonio Otal Palacín\n\n\n\n\n\n\nNo hay resultados"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Acerca de el autor",
    "section": "",
    "text": "Material del curso"
  },
  {
    "objectID": "posts/code/Neumonia.html",
    "href": "posts/code/Neumonia.html",
    "title": "Clasificador de Neumonia (ViT)",
    "section": "",
    "text": "Colab\n\nDetector de neumonía\n\n\n\nEsquema del Vision Transformer Model\n\n\n\nfrom google.colab import userdata\naccess_token = userdata.get('tokenHF')\n\n\nimport os\n\ndef descargar_imagen_desde_url(url, nombre_archivo):\n    try:\n        # Utilizar la instrucción curl para descargar la imagen\n        os.system(f\"curl -o {nombre_archivo} -L -H 'Accept: image/jpeg' {url}\")\n        print(f\"La imagen ha sido descargada y guardada como '{nombre_archivo}'.\")\n    except Exception as e:\n        print(f\"Error: {e}\")\n\n\n# Use a pipeline as a high-level helper\nfrom transformers import pipeline\n\npipe = pipeline(\"image-classification\", model=\"nickmuchi/vit-finetuned-chest-xray-pneumonia\",token=access_token)\n\n\n\n\n\n\n\n/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nCould not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n\n\n\n\n\n\nn=1\np=3\nN=\"https://raw.githubusercontent.com/aotal/ImagenMedica/master/posts/code/data/clasificatorio/normal/N\"+str(n)+\".jpeg\"\nP=\"https://raw.githubusercontent.com/aotal/ImagenMedica/master/posts/code/data/clasificatorio/neumo/P\"+str(p)+\".jpeg\"\n\n\ndescargar_imagen_desde_url(N, \"imagen1.jpg\")\ndescargar_imagen_desde_url(P, \"imagen2.jpg\")\n\nLa imagen ha sido descargada y guardada como 'imagen1.jpg'.\nLa imagen ha sido descargada y guardada como 'imagen2.jpg'.\n\n\n\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\n# Cargar las imágenes\nimagen1 = Image.open(\"imagen1.jpg\")\nimagen2 = Image.open(\"imagen2.jpg\")\n\n# Crear una figura y ejes de subtramas\nfig, axs = plt.subplots(1, 2, figsize=(10, 5))\n\n# Mostrar la primera imagen en el primer eje de subtrama\naxs[0].imshow(imagen1,cmap=\"gray\")\naxs[0].axis('off')  # Desactivar los ejes\n\n# Mostrar la segunda imagen en el segundo eje de subtrama\naxs[1].imshow(imagen2,cmap=\"gray\")\naxs[1].axis('off')  # Desactivar los ejes\n\n# Ajustar el espacio entre las subtramas\nplt.subplots_adjust(wspace=0.2)\n\n# Mostrar la figura\nplt.show()\n\n\n\n\n\npipe('imagen1.jpg')\n\n[{'label': 'NORMAL', 'score': 0.9813147187232971},\n {'label': 'PNEUMONIA', 'score': 0.01868528686463833}]\n\n\n\npipe('imagen2.jpg')\n\n[{'label': 'PNEUMONIA', 'score': 0.9920761585235596},\n {'label': 'NORMAL', 'score': 0.007923857308924198}]"
  },
  {
    "objectID": "posts/code/unet_segmentation.html",
    "href": "posts/code/unet_segmentation.html",
    "title": "U-NET",
    "section": "",
    "text": "Colab"
  },
  {
    "objectID": "posts/code/unet_segmentation.html#original-article-of-nikhil-tomar-on-github.",
    "href": "posts/code/unet_segmentation.html#original-article-of-nikhil-tomar-on-github.",
    "title": "U-NET",
    "section": "Original article of Nikhil Tomar on GitHub.",
    "text": "Original article of Nikhil Tomar on GitHub.\n\n#@title Unet segmentation in Keras TensorFlow\n#@markdown Video explanation:\nfrom IPython.display import YouTubeVideo\nYouTubeVideo('M3EZS__Z_XE', width=600, height=400)\n\n\n        \n        \n\n\nPaper Arxiv Link: U-Net: Convolutional Networks for Biomedical Image Segmentation\n\n\nUNet is a fully convolutional network(FCN) that does image segmentation. Its goal is to predict each pixel’s class.\n\n\nUNet is built upon the FCN and modified in a way that it yields better segmentation in medical imaging."
  },
  {
    "objectID": "posts/code/unet_segmentation.html#architecture",
    "href": "posts/code/unet_segmentation.html#architecture",
    "title": "U-NET",
    "section": "1.1 Architecture",
    "text": "1.1 Architecture\n\n\nUNet Architecture has 3 parts:\n\n\n\nThe Contracting/Downsampling Path\n\n\nBottleneck\n\n\nThe Expanding/Upsampling Path\n\n\n\nDownsampling Path:\n\n\n\nIt consists of two 3x3 convolutions (unpadded convolutions), each followed by a rectified linear unit (ReLU) and a 2x2 max pooling operation with stride 2 for downsampling.\n\n\nAt each downsampling step we double the number of feature channels.\n\n\n\nUpsampling Path:\n\n\n\nEvery step in the expansive path consists of an upsampling of the feature map followed by a 2x2 convolution (“up-convolution”), a concatenation with the correspondingly feature map from the downsampling path, and two 3x3 convolutions, each followed by a ReLU.\n\n\n\nSkip Connection:\n\nThe skip connection from the downsampling path are concatenated with feature map during upsampling path. These skip connection provide local information to global information while upsampling.\n\nFinal Layer:\n\nAt the final layer a 1x1 convolution is used to map each feature vector to the desired number of classes."
  },
  {
    "objectID": "posts/code/unet_segmentation.html#advantages",
    "href": "posts/code/unet_segmentation.html#advantages",
    "title": "U-NET",
    "section": "1.2 Advantages",
    "text": "1.2 Advantages\n\nAdvantages:\n\n\n\nThe UNet combines the location information from the downsampling path to finally obtain a general information combining localisation and context, which is necessary to predict a good segmentation map.\n\n\nNo Dense layer is used, so image sizes can be used."
  },
  {
    "objectID": "posts/code/unet_segmentation.html#dataset",
    "href": "posts/code/unet_segmentation.html#dataset",
    "title": "U-NET",
    "section": "1.3 Dataset",
    "text": "1.3 Dataset\nLink: Data Science Bowl 2018 Find the nuclei in divergent images to advance medical discovery\n\n# Get file data-science-bowl-2018/stage1_train.zip\n# Download it from here: https://www.kaggle.com/c/8089/download-all\n# And upload stage1_train.zip to the Google Colab\nfrom google.colab import files\nuploaded = files.upload()\n\n\n     \n     \n      Upload widget is only available when the cell has been executed in the\n      current browser session. Please rerun this cell to enable.\n      \n       \n\n\nSaving stage1_train.zip to stage1_train.zip\n\n\n\n# Unzip it\n!mkdir stage1_train  # create directory\n!unzip stage1_train.zip -d stage1_train  # unzip into 'stage1_train' dir\n!rm stage1_train.zip  # delete 'stage1_train.zip' file"
  },
  {
    "objectID": "posts/code/unet_segmentation.html#code",
    "href": "posts/code/unet_segmentation.html#code",
    "title": "U-NET",
    "section": "1.4 Code",
    "text": "1.4 Code\n\n## Imports\nimport os\nimport sys\nimport random\n\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nfrom tensorflow import keras\n\n## Seeding\nseed = 42\nrandom.seed = seed\nnp.random.seed = seed\ntf.seed = seed"
  },
  {
    "objectID": "posts/code/unet_segmentation.html#data-generator",
    "href": "posts/code/unet_segmentation.html#data-generator",
    "title": "U-NET",
    "section": "Data Generator",
    "text": "Data Generator\n\nclass DataGen(keras.utils.Sequence):\n    def __init__(self, ids, path, batch_size=8, image_size=128):\n        self.ids = ids\n        self.path = path\n        self.batch_size = batch_size\n        self.image_size = image_size\n        self.on_epoch_end()\n\n    def __load__(self, id_name):\n        ## Path\n        image_path = os.path.join(self.path, id_name, \"images\", id_name) + \".png\"\n        mask_path = os.path.join(self.path, id_name, \"masks/\")\n        all_masks = os.listdir(mask_path)\n\n        ## Reading Image\n        image = cv2.imread(image_path, 1)\n        image = cv2.resize(image, (self.image_size, self.image_size))\n\n        mask = np.zeros((self.image_size, self.image_size, 1))\n\n        ## Reading Masks\n        for name in all_masks:\n            _mask_path = mask_path + name\n            _mask_image = cv2.imread(_mask_path, -1)\n            _mask_image = cv2.resize(_mask_image, (self.image_size, self.image_size)) #128x128\n            _mask_image = np.expand_dims(_mask_image, axis=-1)\n            mask = np.maximum(mask, _mask_image)\n\n        ## Normalizing\n        image = image/255.0\n        mask = mask/255.0\n\n        return image, mask\n\n    def __getitem__(self, index):\n        if(index+1)*self.batch_size &gt; len(self.ids):\n            self.batch_size = len(self.ids) - index*self.batch_size\n\n        files_batch = self.ids[index*self.batch_size : (index+1)*self.batch_size]\n\n        image = []\n        mask  = []\n\n        for id_name in files_batch:\n            _img, _mask = self.__load__(id_name)\n            image.append(_img)\n            mask.append(_mask)\n\n        image = np.array(image)\n        mask  = np.array(mask)\n\n        return image, mask\n\n    def on_epoch_end(self):\n        pass\n\n    def __len__(self):\n        return int(np.ceil(len(self.ids)/float(self.batch_size)))"
  },
  {
    "objectID": "posts/code/unet_segmentation.html#hyperparameters",
    "href": "posts/code/unet_segmentation.html#hyperparameters",
    "title": "U-NET",
    "section": "Hyperparameters",
    "text": "Hyperparameters\n\nimage_size = 128\ntrain_path = \"stage1_train/\"\nepochs = 10\nbatch_size = 8\n\n## Training Ids\ntrain_ids = next(os.walk(train_path))[1]\n\n## Validation Data Size\nval_data_size = 10\n\nvalid_ids = train_ids[:val_data_size]\ntrain_ids = train_ids[val_data_size:]\n\n\ngen = DataGen(train_ids, train_path, batch_size=batch_size, image_size=image_size)\nx, y = gen.__getitem__(0)\nprint(x.shape, y.shape)\n\n(8, 128, 128, 3) (8, 128, 128, 1)\n\n\n\nr = random.randint(0, len(x)-1)\n\nfig = plt.figure()\nfig.subplots_adjust(hspace=0.4, wspace=0.4)\nax = fig.add_subplot(1, 2, 1)\nax.imshow(x[r])\nax = fig.add_subplot(1, 2, 2)\nax.imshow(np.reshape(y[r], (image_size, image_size)), cmap=\"gray\")\n\n&lt;matplotlib.image.AxesImage at 0x7f7f12a48e80&gt;"
  },
  {
    "objectID": "posts/code/unet_segmentation.html#different-convolutional-blocks",
    "href": "posts/code/unet_segmentation.html#different-convolutional-blocks",
    "title": "U-NET",
    "section": "Different Convolutional Blocks",
    "text": "Different Convolutional Blocks\n\ndef down_block(x, filters, kernel_size=(3, 3), padding=\"same\", strides=1):\n    c = keras.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(x)\n    c = keras.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(c)\n    p = keras.layers.MaxPool2D((2, 2), (2, 2))(c)\n    return c, p\n\ndef up_block(x, skip, filters, kernel_size=(3, 3), padding=\"same\", strides=1):\n    us = keras.layers.UpSampling2D((2, 2))(x)\n    concat = keras.layers.Concatenate()([us, skip])\n    c = keras.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(concat)\n    c = keras.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(c)\n    return c\n\ndef bottleneck(x, filters, kernel_size=(3, 3), padding=\"same\", strides=1):\n    c = keras.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(x)\n    c = keras.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(c)\n    return c"
  },
  {
    "objectID": "posts/code/unet_segmentation.html#unet-model",
    "href": "posts/code/unet_segmentation.html#unet-model",
    "title": "U-NET",
    "section": "UNet Model",
    "text": "UNet Model\n\ndef UNet():\n    f = [16, 32, 64, 128, 256]\n    inputs = keras.layers.Input((image_size, image_size, 3))\n\n    p0 = inputs\n    c1, p1 = down_block(p0, f[0])  # 128 --&gt; 64\n    c2, p2 = down_block(p1, f[1])  # 64  --&gt; 32\n    c3, p3 = down_block(p2, f[2])  # 32  --&gt; 16\n    c4, p4 = down_block(p3, f[3])  # 16  --&gt; 8\n\n    bn = bottleneck(p4, f[4])\n\n    u1 = up_block(bn, c4, f[3])  # 8  --&gt; 16\n    u2 = up_block(u1, c3, f[2])  # 16 --&gt; 32\n    u3 = up_block(u2, c2, f[1])  # 32 --&gt; 64\n    u4 = up_block(u3, c1, f[0])  # 64 --&gt; 128\n\n    outputs = keras.layers.Conv2D(1, (1, 1), padding=\"same\", activation=\"sigmoid\")(u4)\n    model = keras.models.Model(inputs, outputs)\n    return model\n\n\nmodel = UNet()\nmodel.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\nmodel.summary()\n\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_1 (InputLayer)            (None, 128, 128, 3)  0                                            \n__________________________________________________________________________________________________\nconv2d (Conv2D)                 (None, 128, 128, 16) 448         input_1[0][0]                    \n__________________________________________________________________________________________________\nconv2d_1 (Conv2D)               (None, 128, 128, 16) 2320        conv2d[0][0]                     \n__________________________________________________________________________________________________\nmax_pooling2d (MaxPooling2D)    (None, 64, 64, 16)   0           conv2d_1[0][0]                   \n__________________________________________________________________________________________________\nconv2d_2 (Conv2D)               (None, 64, 64, 32)   4640        max_pooling2d[0][0]              \n__________________________________________________________________________________________________\nconv2d_3 (Conv2D)               (None, 64, 64, 32)   9248        conv2d_2[0][0]                   \n__________________________________________________________________________________________________\nmax_pooling2d_1 (MaxPooling2D)  (None, 32, 32, 32)   0           conv2d_3[0][0]                   \n__________________________________________________________________________________________________\nconv2d_4 (Conv2D)               (None, 32, 32, 64)   18496       max_pooling2d_1[0][0]            \n__________________________________________________________________________________________________\nconv2d_5 (Conv2D)               (None, 32, 32, 64)   36928       conv2d_4[0][0]                   \n__________________________________________________________________________________________________\nmax_pooling2d_2 (MaxPooling2D)  (None, 16, 16, 64)   0           conv2d_5[0][0]                   \n__________________________________________________________________________________________________\nconv2d_6 (Conv2D)               (None, 16, 16, 128)  73856       max_pooling2d_2[0][0]            \n__________________________________________________________________________________________________\nconv2d_7 (Conv2D)               (None, 16, 16, 128)  147584      conv2d_6[0][0]                   \n__________________________________________________________________________________________________\nmax_pooling2d_3 (MaxPooling2D)  (None, 8, 8, 128)    0           conv2d_7[0][0]                   \n__________________________________________________________________________________________________\nconv2d_8 (Conv2D)               (None, 8, 8, 256)    295168      max_pooling2d_3[0][0]            \n__________________________________________________________________________________________________\nconv2d_9 (Conv2D)               (None, 8, 8, 256)    590080      conv2d_8[0][0]                   \n__________________________________________________________________________________________________\nup_sampling2d (UpSampling2D)    (None, 16, 16, 256)  0           conv2d_9[0][0]                   \n__________________________________________________________________________________________________\nconcatenate (Concatenate)       (None, 16, 16, 384)  0           up_sampling2d[0][0]              \n                                                                 conv2d_7[0][0]                   \n__________________________________________________________________________________________________\nconv2d_10 (Conv2D)              (None, 16, 16, 128)  442496      concatenate[0][0]                \n__________________________________________________________________________________________________\nconv2d_11 (Conv2D)              (None, 16, 16, 128)  147584      conv2d_10[0][0]                  \n__________________________________________________________________________________________________\nup_sampling2d_1 (UpSampling2D)  (None, 32, 32, 128)  0           conv2d_11[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_1 (Concatenate)     (None, 32, 32, 192)  0           up_sampling2d_1[0][0]            \n                                                                 conv2d_5[0][0]                   \n__________________________________________________________________________________________________\nconv2d_12 (Conv2D)              (None, 32, 32, 64)   110656      concatenate_1[0][0]              \n__________________________________________________________________________________________________\nconv2d_13 (Conv2D)              (None, 32, 32, 64)   36928       conv2d_12[0][0]                  \n__________________________________________________________________________________________________\nup_sampling2d_2 (UpSampling2D)  (None, 64, 64, 64)   0           conv2d_13[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_2 (Concatenate)     (None, 64, 64, 96)   0           up_sampling2d_2[0][0]            \n                                                                 conv2d_3[0][0]                   \n__________________________________________________________________________________________________\nconv2d_14 (Conv2D)              (None, 64, 64, 32)   27680       concatenate_2[0][0]              \n__________________________________________________________________________________________________\nconv2d_15 (Conv2D)              (None, 64, 64, 32)   9248        conv2d_14[0][0]                  \n__________________________________________________________________________________________________\nup_sampling2d_3 (UpSampling2D)  (None, 128, 128, 32) 0           conv2d_15[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_3 (Concatenate)     (None, 128, 128, 48) 0           up_sampling2d_3[0][0]            \n                                                                 conv2d_1[0][0]                   \n__________________________________________________________________________________________________\nconv2d_16 (Conv2D)              (None, 128, 128, 16) 6928        concatenate_3[0][0]              \n__________________________________________________________________________________________________\nconv2d_17 (Conv2D)              (None, 128, 128, 16) 2320        conv2d_16[0][0]                  \n__________________________________________________________________________________________________\nconv2d_18 (Conv2D)              (None, 128, 128, 1)  17          conv2d_17[0][0]                  \n==================================================================================================\nTotal params: 1,962,625\nTrainable params: 1,962,625\nNon-trainable params: 0\n__________________________________________________________________________________________________"
  },
  {
    "objectID": "posts/code/unet_segmentation.html#training-the-model",
    "href": "posts/code/unet_segmentation.html#training-the-model",
    "title": "U-NET",
    "section": "Training the model",
    "text": "Training the model\n\ntrain_gen = DataGen(train_ids, train_path, image_size=image_size, batch_size=batch_size)\nvalid_gen = DataGen(valid_ids, train_path, image_size=image_size, batch_size=batch_size)\n\ntrain_steps = len(train_ids)//batch_size\nvalid_steps = len(valid_ids)//batch_size\n\nmodel.fit_generator(train_gen, validation_data=valid_gen, steps_per_epoch=train_steps,\n                    validation_steps=valid_steps, epochs=epochs)\n\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.cast instead.\nEpoch 1/10\n2/2 [==============================] - 1s 462ms/step - loss: 0.8926 - acc: 0.9346\n83/83 [==============================] - 24s 291ms/step - loss: 2.6685 - acc: 0.8320 - val_loss: 0.8926 - val_acc: 0.9346\nEpoch 2/10\n5/5 [==============================] - 0s 76ms/step - loss: 0.2497 - acc: 0.9107\n83/83 [==============================] - 14s 170ms/step - loss: 0.8620 - acc: 0.8629 - val_loss: 0.2497 - val_acc: 0.9107\nEpoch 3/10\n5/5 [==============================] - 0s 85ms/step - loss: 0.1496 - acc: 0.9668\n83/83 [==============================] - 15s 177ms/step - loss: 0.6539 - acc: 0.9156 - val_loss: 0.1496 - val_acc: 0.9668\nEpoch 4/10\n5/5 [==============================] - 0s 79ms/step - loss: 0.1137 - acc: 0.9709\n83/83 [==============================] - 15s 179ms/step - loss: 0.4919 - acc: 0.9382 - val_loss: 0.1137 - val_acc: 0.9709\nEpoch 5/10\n5/5 [==============================] - 0s 78ms/step - loss: 0.1407 - acc: 0.9667\n83/83 [==============================] - 14s 172ms/step - loss: 0.5270 - acc: 0.9361 - val_loss: 0.1407 - val_acc: 0.9667\nEpoch 6/10\n5/5 [==============================] - 0s 83ms/step - loss: 0.1272 - acc: 0.9680\n83/83 [==============================] - 14s 172ms/step - loss: 0.4950 - acc: 0.9378 - val_loss: 0.1272 - val_acc: 0.9680\nEpoch 7/10\n5/5 [==============================] - 0s 79ms/step - loss: 0.1142 - acc: 0.9684\n83/83 [==============================] - 14s 172ms/step - loss: 0.5161 - acc: 0.9344 - val_loss: 0.1142 - val_acc: 0.9684\nEpoch 8/10\n5/5 [==============================] - 0s 80ms/step - loss: 0.0952 - acc: 0.9717\n83/83 [==============================] - 15s 175ms/step - loss: 0.4100 - acc: 0.9449 - val_loss: 0.0952 - val_acc: 0.9717\nEpoch 9/10\n5/5 [==============================] - 0s 73ms/step - loss: 0.0981 - acc: 0.9712\n83/83 [==============================] - 16s 188ms/step - loss: 0.3795 - acc: 0.9478 - val_loss: 0.0981 - val_acc: 0.9712\nEpoch 10/10\n5/5 [==============================] - 0s 82ms/step - loss: 0.0883 - acc: 0.9726\n83/83 [==============================] - 14s 172ms/step - loss: 0.3781 - acc: 0.9481 - val_loss: 0.0883 - val_acc: 0.9726\n\n\n&lt;tensorflow.python.keras.callbacks.History at 0x7f7f0e253ba8&gt;"
  },
  {
    "objectID": "posts/code/unet_segmentation.html#testing-the-model",
    "href": "posts/code/unet_segmentation.html#testing-the-model",
    "title": "U-NET",
    "section": "Testing the model",
    "text": "Testing the model\n\n## Save the Weights\nmodel.save_weights(\"UNetW.h5\")\n\n## Dataset for prediction\nx, y = valid_gen.__getitem__(2)\nresult = model.predict(x)\n\nresult = result &gt; 0.5\n\n\nfig = plt.figure()\nfig.subplots_adjust(hspace=0.4, wspace=0.4)\n\nax = fig.add_subplot(1, 2, 1)\nax.imshow(np.reshape(y[0]*255, (image_size, image_size)), cmap=\"gray\")\n\nax = fig.add_subplot(1, 2, 2)\nax.imshow(np.reshape(result[0]*255, (image_size, image_size)), cmap=\"gray\")\n\n&lt;matplotlib.image.AxesImage at 0x7f7f0ac86940&gt;\n\n\n\n\n\n\nfig = plt.figure()\nfig.subplots_adjust(hspace=0.4, wspace=0.4)\n\nax = fig.add_subplot(1, 2, 1)\nax.imshow(np.reshape(y[1]*255, (image_size, image_size)), cmap=\"gray\")\n\nax = fig.add_subplot(1, 2, 2)\nax.imshow(np.reshape(result[1]*255, (image_size, image_size)), cmap=\"gray\")\n\n&lt;matplotlib.image.AxesImage at 0x7f7f0ac273c8&gt;"
  },
  {
    "objectID": "posts/code/2_ConvolutionalNN_PNEUMONIA_X_ray.html",
    "href": "posts/code/2_ConvolutionalNN_PNEUMONIA_X_ray.html",
    "title": "CNN_Neumonía",
    "section": "",
    "text": "Colab"
  },
  {
    "objectID": "posts/code/2_ConvolutionalNN_PNEUMONIA_X_ray.html#notebook-de-preprocesamiento",
    "href": "posts/code/2_ConvolutionalNN_PNEUMONIA_X_ray.html#notebook-de-preprocesamiento",
    "title": "CNN_Neumonía",
    "section": "Notebook de preprocesamiento",
    "text": "Notebook de preprocesamiento\nJoel Ricci López, 2021.\n\n\n\nOpen In Colab"
  },
  {
    "objectID": "posts/code/2_ConvolutionalNN_PNEUMONIA_X_ray.html#planteamiento",
    "href": "posts/code/2_ConvolutionalNN_PNEUMONIA_X_ray.html#planteamiento",
    "title": "CNN_Neumonía",
    "section": "Planteamiento",
    "text": "Planteamiento\nVamos a implementar un modelo de clasificación mediante una red neuronal convolucional para la identificación de casos de neumonía presentado en Kaggle. Usaremos Keras y su API Sequential para crear una red capa por capa. Además compararemos el desempeño de nuestra red contra un modelo creado a partir de Transfer Learning, utilizando la red preentrenada (VGG16)[https://arxiv.org/abs/1409.1556]."
  },
  {
    "objectID": "posts/code/2_ConvolutionalNN_PNEUMONIA_X_ray.html#estrategia",
    "href": "posts/code/2_ConvolutionalNN_PNEUMONIA_X_ray.html#estrategia",
    "title": "CNN_Neumonía",
    "section": "Estrategia",
    "text": "Estrategia\nLa estrategia a seguir es la siguiente: 1. Descargar los datos de Kaggle: Chest X-ray Images (Pneumonia). 2. Los datos fueron descargados en Colab y los conjuntos fueron modificados para que se cumpliera lo siguiente:\n- Conjunto de Entrenamiento: 80% de las imágenes. - Conjuntos de Validación y Prueba: 10% de las imágenes cada uno. - Se aseguró que la proporción de imágenes de las clases PNEUMONIA y NORMAL fuera la misma en cada conjunto. 3. Esta fase preliminar se realizó en el siguiente notebook:  4. Se realizó una fase de Análisis Exploratorio con las imágenes. 5. Se implementó una red neuronal convolucional (CNN) con Keras para la clasificación de las imágenes.\na) Se evaluó el desempeño de la CNN con el conjunto de prueba. 6. Se implementó una red neuronal convolucional basada en transferencia de conocimiento.\na) Se tomó como base la red preentrenada VGG16 con los pesos de imagenet.\nb) Se evaluó el desempeño de la red VGG16-base con el conjunto de prueba."
  },
  {
    "objectID": "posts/code/2_ConvolutionalNN_PNEUMONIA_X_ray.html#preliminares-y-carga-de-datos",
    "href": "posts/code/2_ConvolutionalNN_PNEUMONIA_X_ray.html#preliminares-y-carga-de-datos",
    "title": "CNN_Neumonía",
    "section": "Preliminares y carga de datos",
    "text": "Preliminares y carga de datos\n\nMontamos el acceso a Drive para cargar los datos\n\nfrom google.colab import drive\n\ndrive.mount('/content/gdrive/')\nroot_path = '/content/gdrive/My Drive/kaggle/ap-jrl-neumonia/chest_xray'\n%cd $root_path/\n%ls\n\nDrive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n/content/gdrive/My Drive/kaggle/ap-jrl-neumonia/chest_xray\nchest_xray/        modelo_neu_jrl_TRAINED.h5              test/\nhistory_cnn.pkl    modelo_vgg16_jrl.h5                    train/\n__MACOSX/          modelo_VGG16_jrl_trained_20_epocas.h5  val/\nmodelo_neu_jrl.h5  output.png\n\n\n\n\nCarga de las ibrerías a utilizar\n\nimport pandas as pd\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style='ticks', context='talk', palette='Spectral', font_scale=0.9)\n\n\n# Variables con los paths a los directorios de cada conjunto\ntest_dir = os.path.join(root_path, 'test')\ntrain_dir = os.path.join(root_path, 'train')\nval_dir = os.path.join(root_path, 'val')"
  },
  {
    "objectID": "posts/code/2_ConvolutionalNN_PNEUMONIA_X_ray.html#evaluación-del-modelo",
    "href": "posts/code/2_ConvolutionalNN_PNEUMONIA_X_ray.html#evaluación-del-modelo",
    "title": "CNN_Neumonía",
    "section": "Evaluación del modelo",
    "text": "Evaluación del modelo\n\n#@title Desempeño de la red durante el entrenamiento\n#@markdown (desplegar para ver el código)\n\n# Calculamos el F1 score a partir de los resultados de Precision y Recall\ndef get_f1_score(precision_arr, recall_arr):\n  epsilon       = 1e-7\n  precision_arr = np.array(precision_arr)\n  recall_arr    = np.array(recall_arr)\n  f1 = (2) * (precision_arr*recall_arr) / \\\n      (precision_arr + recall_arr + epsilon)\n  return f1\n\ndef plot_history(train_hist, suptitle=''):\n  # Agregar F1 score\n  train_hist['F1_score']     = get_f1_score(train_hist['precision'],\n                                    train_hist['recall'])\n  train_hist['val_F1_score'] = get_f1_score(train_hist['val_precision'],\n                                    train_hist['val_recall'])\n\n  # Gráficas\n  metrics = [key for key in train_hist.keys()\n              if not key.startswith('val')]\n\n  fig, ax = plt.subplots(2, int(len(metrics)/2),\n                        figsize = (16, 8))\n  for i, metric in enumerate(metrics):\n    if i &lt; 3:\n      j = 0\n    else:\n      i, j = i - 3, 1\n    ax[j, i].plot(train_hist[metric])\n    ax[j, i].plot(train_hist[f\"val_{metric}\"])\n    ax[j, i].set_title(metric.replace('_', ' ').capitalize())\n    ax[j, i].set_xlabel('Épocas')\n    ax[j, i].grid()\n    if metric != 'loss':\n      ax[j, i].set_ylim(0.7, 1.05)\n    else:\n      ax[j, i].legend(['Train', 'Val'])\n  plt.tight_layout()\n  plt.suptitle(suptitle, y = 1.02)\n  plt.show()\n\n\n# Ejecutamos la función\ntrain_hist = history #.history.copy()\nplot_history(train_hist,\n      suptitle='CNN: Métricas de evaluación en ' +\n      'los conjuntos de Entrenamiento y Validación')\n\n\n\n\n\nEvaluación con el conjunto de prueba\n\nscore_test = model.evaluate(test_generator, batch_size=128)\n\nfor i, j in zip(history.keys(), score_test):\n  print(f'{i.upper()}: {j:.3f}')\n\n19/19 [==============================] - 5s 226ms/step - loss: 0.4822 - accuracy: 0.8904 - roc_auc: 0.9339 - precision: 0.8740 - recall: 0.9930\nLOSS: 0.482\nACCURACY: 0.890\nROC_AUC: 0.934\nPRECISION: 0.874\nRECALL: 0.993\n\n\nOtenemos las etiquetas reales y las etiquetas predichas del conjunto de prueba.\n\n# Etiquetas reales\ny_test = test_generator.classes\n# Predicciones\ny_pred = model.predict(test_generator)\ny_pred_classes = (y_pred &gt;= 0.5).astype('int32')\n\n\nMatriz de confusión\n\n#@markdown (desplegar para ver el código)\n\nfrom sklearn.metrics import confusion_matrix\n\ndef plot_cfn_matrix(y_test, y_pred_classes, suptitle=''):\n  cf_matrix = confusion_matrix(y_test, y_pred_classes)\n\n  labels_a = np.array(\n      [f'{i}\\n{j}'\n      for i, j in\n        zip(['TN', 'FP', 'FN', 'TP'],\n              cf_matrix.flatten()\n          )\n      ]).reshape(2,2)\n\n  fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n  # Valores crudos\n  sns.heatmap(cf_matrix,\n              cmap = 'Reds', ax = ax[0],\n              fmt = '', annot = labels_a,\n              cbar_kws={'label': 'Imágenes'})\n  # Porcentajes\n  sns.heatmap(cf_matrix/np.sum(cf_matrix),\n              cmap = 'Blues', ax = ax[1],\n              fmt = '.2%', annot = True,\n              cbar_kws={'label': 'Porcentaje'})\n  for i in range(2):\n    ax[i].set(aspect = \"equal\",\n              xlabel = 'Predichos', ylabel = 'Reales')\n    for _, spine in ax[i].spines.items():\n      spine.set_visible(True)\n  plt.suptitle(suptitle, y = 1.05)\n  plt.tight_layout()\n\n# Genera la gráfica\nplot_cfn_matrix(y_test, y_pred_classes,\n    suptitle='Matriz de confusión CNN (conjunto de prueba)')\n\n\n\n\n\n\nCurva ROC y Curva de Precisión Sensibilidad\nPara esta evaluación usamos las métricas: - Área bajo la curva ROC. - Promedio de Precisión-Sensibilidad (avgPR): &gt; \\(\\text{AP} = \\sum_n (R_n - R_{n-1}) P_n\\)\n\n#@markdown (desplegar para ver el código)\n\nfrom sklearn.metrics import roc_auc_score, roc_curve, \\\n          f1_score, precision_recall_curve, average_precision_score\n\ndef plot_roc_and_pr_curves(y_test, y_pred,\n                           suptitle='', name = ''):\n  auc_ = roc_auc_score(y_test, y_pred)\n  f1_score_ = roc_auc_score(y_test, y_pred)\n  vg_pr_score = average_precision_score(y_test, y_pred)\n\n  fpr, tpr, _ = roc_curve(y_test, y_pred)\n  prec_, recall_, _ = precision_recall_curve(y_test, y_pred)\n  pr_baseline = y_test.sum() / len(y_test)\n\n  fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n\n  # Curva ROC\n  ax[0].plot(fpr, tpr, c = '#00A3A4',\n            linewidth = 4, label = f'{name}: {auc_:.3f} AUC')\n  ax[0].plot([0, 1], [0, 1], c = 'gray',\n            linestyle = '--', label = 'Aleatorio')\n  ax[0].legend()\n  ax[0].set(xlabel = '1 - Especificidad (FPR)',\n            ylabel = 'Sensibilidad (TPR)')\n\n  # Curva de Precision-Sensibilidad\n  ax[1].plot(recall_, prec_, c = '#F48000',\n            linewidth = 4, label = f'{name}: {vg_pr_score:.3f} avgPR')\n  ax[1].plot([0, 1], [pr_baseline, pr_baseline], c = 'gray',\n            linestyle = '--', label = 'Aleatorio')\n  ax[1].legend()\n  ax[1].set(xlabel = 'Sensibilidad',\n            ylabel = 'Precisión')\n  for i, title in enumerate(['Curva ROC',\n                            'Curva Precisión-Sensibilidad']):\n    ax[i].set(aspect='equal', ylim=(0,1.02), title=title)\n    ax[i].grid(True)\n  plt.tight_layout()\n  plt.suptitle(suptitle, y = 1.02)\n  plt.show()\n\n# Genera la gráfica\nplot_roc_and_pr_curves(y_test, y_pred,\n          suptitle='Desempeño CNN', name = 'CNN')\n\n\n\n\n\n\n\nInterpretación de Resultados\nVemos que la CNN obtiene buenos valores de desempeño en todas las métricas. - En particular nos interesa que la sensibilidad del modelo sea alta (0.98) y por consecuencia que el número de Falsos Negativos sea bajo (9 FN). - Se observa que el modelo sólo identificó 9 Falsos Negativos, lo cual es un buen resultado pues nos interesa que casos verdaderos de neumonía NO sean identificados como casos NORMALES. - El modelo tiene una mayor sensibilidad (0.98) que precisión (0.89). Es un buen resultado, pero hay que considerar que esto también es consecuencia de que en el conjunto de datos haya considerablemente más ejemplos positivos que negativos."
  },
  {
    "objectID": "posts/code/2_ConvolutionalNN_PNEUMONIA_X_ray.html#uso-de-la-red-vgg16",
    "href": "posts/code/2_ConvolutionalNN_PNEUMONIA_X_ray.html#uso-de-la-red-vgg16",
    "title": "CNN_Neumonía",
    "section": "Uso de la red VGG16",
    "text": "Uso de la red VGG16\nPara esta fase vamos a implementar el método de transferencia de conocimiento (Transfer learning) usando la red VGG16 (Simonyan and Zisserman, 2015).\n\nfrom tensorflow.keras.applications import VGG16\n\nvgg16 = VGG16(weights     = 'imagenet',\n              include_top = False,\n              # Definimos el tamaño de la capa de entrada\n              # igual a las dimensiones de las imágenes\n              input_shape = (IMG_HEIGHT, IMG_WIDTH, 3))\n\n\nVisualizamos la arquitectura de la VGG16\n\nvisualkeras.layered_view(vgg16, scale_xy=1)\n\n\n\n\n\nfrom tensorflow.keras.layers import Input\n# Creamos nuestro nuevo modelo\n\n# Primero congelamos los pesos de las primeras capas de VGG16\nvgg16.trainable = False\n\nmodel_vgg16 = Sequential()\n# Añadimos a VGG16 como si fuese una capa\nmodel_vgg16.add(Input(shape = (IMG_HEIGHT, IMG_WIDTH, 3)))\nmodel_vgg16.add(vgg16)\nmodel_vgg16.add(Flatten())\nmodel_vgg16.add(Dense(128,\n                  activation='relu'))\nmodel_vgg16.add(Dropout(0.4))\nmodel_vgg16.add(Dense(1,\n                      activation='sigmoid',\n                      bias_initializer = output_bias))\nmodel_vgg16.summary()\n\nModel: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nvgg16 (Functional)           (None, 4, 4, 512)         14714688  \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 8192)              0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 128)               1048704   \n_________________________________________________________________\ndropout_3 (Dropout)          (None, 128)               0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 1)                 129       \n=================================================================\nTotal params: 15,763,521\nTrainable params: 1,048,833\nNon-trainable params: 14,714,688\n_________________________________________________________________\n\n\nListo, el modelo tiene 524,417 de parámetros entrenables.\n\n\nCompilamos el modelo basado en VGG16\n\n# Usamos las mismas métricas de evaluación\nmetrics = [BinaryAccuracy(name = 'accuracy'),\n           AUC(name = 'roc_auc'),\n           Precision(name ='precision'),\n           Recall(name = 'recall')\n          ]\n\n# Compilamos el modelo\nmodel_vgg16.compile(\n    optimizer = optimizer,\n    loss      = 'binary_crossentropy',\n    metrics   = metrics\n)\n\n\n\nEntrenamiento del moelo basado en VGG16\n\nCallbacks\n\nBATCH_SIZE = 64\nEPOCHS = 50\n\n# ModelCheckpoint\nmodel_filepath = f\"{root_path}/modelo_vgg16_jrl.h5\"\ncheckpoint_vgg16 = ModelCheckpoint(model_filepath,\n                monitor        = 'val_loss',\n                verbose        = 1,\n                save_best_only = True,\n                mode           = 'min'\n)\n\n# Lista de Callbacks\ncallbacks_list_vgg16 = [checkpoint_vgg16, early_stop]\n\n\n\n\nEntrenamiento de la red\n\n%%time\n# Entrenamiento\nhistory_vgg16 = model_vgg16.fit(\n          train_generator,\n          validation_data = val_generator,\n          epochs          = EPOCHS,\n          batch_size      = BATCH_SIZE,\n          callbacks       = callbacks_list_vgg16\n)\n\nEpoch 1/50\n147/147 [==============================] - 86s 555ms/step - loss: 1.1479 - accuracy: 0.8489 - roc_auc: 0.8704 - precision: 0.8874 - recall: 0.9027 - val_loss: 0.2040 - val_accuracy: 0.9195 - val_roc_auc: 0.9656 - val_precision: 0.9397 - val_recall: 0.9507\n\nEpoch 00001: val_loss improved from inf to 0.20395, saving model to /content/gdrive/My Drive/kaggle/ap-jrl-neumonia/chest_xray/modelo_vgg16_jrl.h5\nEpoch 2/50\n147/147 [==============================] - 78s 532ms/step - loss: 0.1513 - accuracy: 0.9360 - roc_auc: 0.9830 - precision: 0.9473 - recall: 0.9647 - val_loss: 0.2019 - val_accuracy: 0.9195 - val_roc_auc: 0.9677 - val_precision: 0.9566 - val_recall: 0.9319\n\nEpoch 00002: val_loss improved from 0.20395 to 0.20186, saving model to /content/gdrive/My Drive/kaggle/ap-jrl-neumonia/chest_xray/modelo_vgg16_jrl.h5\nEpoch 3/50\n147/147 [==============================] - 79s 534ms/step - loss: 0.1478 - accuracy: 0.9409 - roc_auc: 0.9826 - precision: 0.9522 - recall: 0.9677 - val_loss: 0.1855 - val_accuracy: 0.9315 - val_roc_auc: 0.9702 - val_precision: 0.9447 - val_recall: 0.9624\n\nEpoch 00003: val_loss improved from 0.20186 to 0.18551, saving model to /content/gdrive/My Drive/kaggle/ap-jrl-neumonia/chest_xray/modelo_vgg16_jrl.h5\nEpoch 4/50\n147/147 [==============================] - 79s 536ms/step - loss: 0.1204 - accuracy: 0.9525 - roc_auc: 0.9894 - precision: 0.9611 - recall: 0.9739 - val_loss: 0.2082 - val_accuracy: 0.9161 - val_roc_auc: 0.9646 - val_precision: 0.9255 - val_recall: 0.9624\n\nEpoch 00004: val_loss did not improve from 0.18551\nEpoch 5/50\n147/147 [==============================] - 79s 535ms/step - loss: 0.1245 - accuracy: 0.9509 - roc_auc: 0.9870 - precision: 0.9662 - recall: 0.9675 - val_loss: 0.2068 - val_accuracy: 0.9281 - val_roc_auc: 0.9661 - val_precision: 0.9507 - val_recall: 0.9507\n\nEpoch 00005: val_loss did not improve from 0.18551\nEpoch 6/50\n147/147 [==============================] - 78s 529ms/step - loss: 0.1235 - accuracy: 0.9502 - roc_auc: 0.9887 - precision: 0.9623 - recall: 0.9682 - val_loss: 0.2046 - val_accuracy: 0.9264 - val_roc_auc: 0.9652 - val_precision: 0.9423 - val_recall: 0.9577\n\nEpoch 00006: val_loss did not improve from 0.18551\nEpoch 7/50\n147/147 [==============================] - 78s 529ms/step - loss: 0.1050 - accuracy: 0.9607 - roc_auc: 0.9913 - precision: 0.9653 - recall: 0.9817 - val_loss: 0.2287 - val_accuracy: 0.9264 - val_roc_auc: 0.9627 - val_precision: 0.9362 - val_recall: 0.9648\n\nEpoch 00007: val_loss did not improve from 0.18551\nEpoch 8/50\n147/147 [==============================] - 77s 526ms/step - loss: 0.1014 - accuracy: 0.9596 - roc_auc: 0.9922 - precision: 0.9672 - recall: 0.9774 - val_loss: 0.1935 - val_accuracy: 0.9332 - val_roc_auc: 0.9651 - val_precision: 0.9510 - val_recall: 0.9577\n\nEpoch 00008: val_loss did not improve from 0.18551\nEpoch 9/50\n147/147 [==============================] - 78s 528ms/step - loss: 0.1184 - accuracy: 0.9494 - roc_auc: 0.9893 - precision: 0.9589 - recall: 0.9726 - val_loss: 0.1990 - val_accuracy: 0.9298 - val_roc_auc: 0.9657 - val_precision: 0.9385 - val_recall: 0.9671\n\nEpoch 00009: val_loss did not improve from 0.18551\nEpoch 10/50\n147/147 [==============================] - 77s 527ms/step - loss: 0.1041 - accuracy: 0.9570 - roc_auc: 0.9919 - precision: 0.9625 - recall: 0.9785 - val_loss: 0.2304 - val_accuracy: 0.9195 - val_roc_auc: 0.9627 - val_precision: 0.9165 - val_recall: 0.9789\n\nEpoch 00010: val_loss did not improve from 0.18551\nEpoch 11/50\n147/147 [==============================] - 77s 526ms/step - loss: 0.1035 - accuracy: 0.9583 - roc_auc: 0.9921 - precision: 0.9638 - recall: 0.9792 - val_loss: 0.2082 - val_accuracy: 0.9229 - val_roc_auc: 0.9659 - val_precision: 0.9281 - val_recall: 0.9695\n\nEpoch 00011: val_loss did not improve from 0.18551\nEpoch 12/50\n147/147 [==============================] - 77s 526ms/step - loss: 0.0845 - accuracy: 0.9629 - roc_auc: 0.9940 - precision: 0.9687 - recall: 0.9814 - val_loss: 0.1949 - val_accuracy: 0.9315 - val_roc_auc: 0.9654 - val_precision: 0.9488 - val_recall: 0.9577\n\nEpoch 00012: val_loss did not improve from 0.18551\nEpoch 13/50\n147/147 [==============================] - 78s 527ms/step - loss: 0.1085 - accuracy: 0.9587 - roc_auc: 0.9905 - precision: 0.9679 - recall: 0.9766 - val_loss: 0.1864 - val_accuracy: 0.9298 - val_roc_auc: 0.9678 - val_precision: 0.9385 - val_recall: 0.9671\n\nEpoch 00013: val_loss did not improve from 0.18551\n\n\n\nmodel_vgg16.save(f\"{root_path}/modelo_vgg16_jrl_TRAINED.h5\")\n\n\nhistory_vgg16 = history_vgg16.history\nwith open(f\"{root_path}/history_cnn_vgg16.pkl\", 'wb') as file:\n  pickle.dump(history_vgg16, file)\n\n\n# Ejecutamos la función\ntrain_hist_v16 = history_vgg16.copy()\nplot_history(train_hist_v16,\n      suptitle='VGG16-base: Métricas de evaluación en ' +\n      'los conjuntos de Entrenamiento y Validación')\n\n\n\n\n\nscore_test_vgg16 = model_vgg16.evaluate(test_generator, batch_size=64)\n\nfor i, j in zip(train_hist_v16.keys(), score_test_vgg16):\n  print(f'{i.upper()}: {j:.2f}')\n\n19/19 [==============================] - 5s 251ms/step - loss: 0.2621 - accuracy: 0.9092 - roc_auc: 0.9625 - precision: 0.8943 - recall: 0.9930\nLOSS: 0.26\nACCURACY: 0.91\nROC_AUC: 0.96\nPRECISION: 0.89\nRECALL: 0.99\n\n\n\n# Etiquetas reales\n# y_test = test_generator.classes\n# Predicciones\ny_pred_v16 = model_vgg16.predict(test_generator)\ny_pred_classes_v16 = (y_pred_v16 &gt;= 0.5).astype('int32')\n\n\n#@markdown (desplegar para ver el código)\n\n# Genera la gráfica\nplot_cfn_matrix(y_test, y_pred_classes_v16,\n    suptitle='Matriz de confusión VGG16-base (conjunto de prueba)')\n\n\n\n\n\n# Genera la gráfica\nplot_roc_and_pr_curves(y_test, y_pred_v16,\n          suptitle = 'Desempeño VGG16-base',\n          name = 'VGG16')"
  },
  {
    "objectID": "posts/code/2_ConvolutionalNN_PNEUMONIA_X_ray.html#interpretación-de-resultados-1",
    "href": "posts/code/2_ConvolutionalNN_PNEUMONIA_X_ray.html#interpretación-de-resultados-1",
    "title": "CNN_Neumonía",
    "section": "Interpretación de Resultados",
    "text": "Interpretación de Resultados\nLa red VGG16 mostró buenos resultados, teniendo un desempeño similar al de la primer red (CNN) en términos de Sensibilidad. Sin embargo, mostró una mejora en la precisión y por consiguiente en el Área bajo la curva ROC, así como un mayor valor de avgPR."
  },
  {
    "objectID": "posts/Presentaciones/ImagenAI.html#índice",
    "href": "posts/Presentaciones/ImagenAI.html#índice",
    "title": "Inteligencia artificial en imagen médica",
    "section": "Índice",
    "text": "Índice\n\nDiagnóstico asistido por IA\nSegmentación de órganos y tejidos\nGeneración de imágenes sintéticas\nMejora de la calidad de la imagen\nIntegración de datos multimodales"
  },
  {
    "objectID": "posts/Presentaciones/ImagenAI.html#knowledge-map",
    "href": "posts/Presentaciones/ImagenAI.html#knowledge-map",
    "title": "Inteligencia artificial en imagen médica",
    "section": "Knowledge map",
    "text": "Knowledge map\n\n\n\nNotes"
  },
  {
    "objectID": "posts/Presentaciones/ImagenAI.html#casos-de-uso",
    "href": "posts/Presentaciones/ImagenAI.html#casos-de-uso",
    "title": "Inteligencia artificial en imagen médica",
    "section": "Casos de uso",
    "text": "Casos de uso\n\nDetección y clasificación de anomalías\nDetección temprana (screening)\nReducción de errores"
  },
  {
    "objectID": "posts/Presentaciones/ImagenAI.html#convolutional-neural-network",
    "href": "posts/Presentaciones/ImagenAI.html#convolutional-neural-network",
    "title": "Inteligencia artificial en imagen médica",
    "section": "Convolutional Neural Network",
    "text": "Convolutional Neural Network"
  },
  {
    "objectID": "posts/Presentaciones/ImagenAI.html#vision-transformer",
    "href": "posts/Presentaciones/ImagenAI.html#vision-transformer",
    "title": "Inteligencia artificial en imagen médica",
    "section": "Vision Transformer",
    "text": "Vision Transformer\n\n\n\n\n\n\n\n(Dosovitskiy et al. 2020)"
  },
  {
    "objectID": "posts/Presentaciones/ImagenAI.html#knowledge-map-1",
    "href": "posts/Presentaciones/ImagenAI.html#knowledge-map-1",
    "title": "Inteligencia artificial en imagen médica",
    "section": "Knowledge map",
    "text": "Knowledge map"
  },
  {
    "objectID": "posts/Presentaciones/ImagenAI.html#u-net",
    "href": "posts/Presentaciones/ImagenAI.html#u-net",
    "title": "Inteligencia artificial en imagen médica",
    "section": "U-Net",
    "text": "U-Net\n\n(Ronneberger, Fischer, y Brox 2015)"
  },
  {
    "objectID": "posts/Presentaciones/ImagenAI.html#u-net-1",
    "href": "posts/Presentaciones/ImagenAI.html#u-net-1",
    "title": "Inteligencia artificial en imagen médica",
    "section": "U-Net",
    "text": "U-Net"
  },
  {
    "objectID": "posts/Presentaciones/ImagenAI.html#proyecto-monai",
    "href": "posts/Presentaciones/ImagenAI.html#proyecto-monai",
    "title": "Inteligencia artificial en imagen médica",
    "section": "Proyecto MONAI",
    "text": "Proyecto MONAI"
  },
  {
    "objectID": "posts/Presentaciones/ImagenAI.html#knowledge-map-2",
    "href": "posts/Presentaciones/ImagenAI.html#knowledge-map-2",
    "title": "Inteligencia artificial en imagen médica",
    "section": "Knowledge map",
    "text": "Knowledge map"
  },
  {
    "objectID": "posts/Presentaciones/ImagenAI.html#ejemplo-creación-de-cts-sintéticos-de-mri",
    "href": "posts/Presentaciones/ImagenAI.html#ejemplo-creación-de-cts-sintéticos-de-mri",
    "title": "Inteligencia artificial en imagen médica",
    "section": "Ejemplo: Creación de CTs sintéticos de MRI",
    "text": "Ejemplo: Creación de CTs sintéticos de MRI"
  },
  {
    "objectID": "posts/Presentaciones/ImagenAI.html#knowledge-map-3",
    "href": "posts/Presentaciones/ImagenAI.html#knowledge-map-3",
    "title": "Inteligencia artificial en imagen médica",
    "section": "Knowledge map",
    "text": "Knowledge map"
  },
  {
    "objectID": "posts/Presentaciones/ImagenAI.html#knowledge-map-4",
    "href": "posts/Presentaciones/ImagenAI.html#knowledge-map-4",
    "title": "Inteligencia artificial en imagen médica",
    "section": "Knowledge map",
    "text": "Knowledge map\n\n\n\n\n\nIAMED\n\n\n\n\n\n\n \n\n\n \n\n\n \n\n\n\n\nDosovitskiy, Alexey, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, et al. 2020. «An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale». https://doi.org/10.48550/ARXIV.2010.11929.\n\n\nRonneberger, Olaf, Philipp Fischer, y Thomas Brox. 2015. «U-Net: Convolutional Networks for Biomedical Image Segmentation». En, 234-41. Springer International Publishing. https://doi.org/10.1007/978-3-319-24574-4_28."
  }
]